{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "import re\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "import openai\n",
    "from openai import RateLimitError, OpenAIError\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from openai import OpenAI\n",
    "from fpdf import FPDF\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import logging\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave de API\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"API key is not set\")\n",
    "\n",
    "# Inicializar la API de OpenAI\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "def listar_notebooks(directory):\n",
    "    \"\"\"\n",
    "    Devuelve una lista con los nombres de todos los archivos con extensión '.ipynb' en un directorio dado.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): La ruta al directorio donde buscar los archivos.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos listas, la primera con los nombres de los alumnos (nombres de archivos sin extensión),\n",
    "               y la segunda con los nombres de los archivos completos que cumplen los criterios.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico al intentar listar los archivos.\n",
    "    \"\"\"\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger('listar_notebooks')\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    archivos = []\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.ipynb'):\n",
    "                archivos.append(filename)\n",
    "        \n",
    "        alumnos = [archivo.replace('.ipynb', '') for archivo in archivos]\n",
    "        \n",
    "        if not archivos:\n",
    "            warning_msg = f\"No se encontraron archivos .ipynb en el directorio '{directory}'.\"\n",
    "            logger.warning(warning_msg)\n",
    "            print(warning_msg)\n",
    "        \n",
    "        logger.info(f\"Se encontraron {len(archivos)} archivos .ipynb en el directorio {directory}.\")\n",
    "        return alumnos, archivos\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al listar archivos en el directorio {directory}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def cargar_criterios(criterios_file):\n",
    "    \"\"\"\n",
    "    Carga los criterios desde un archivo de texto y los almacena en un diccionario.\n",
    "\n",
    "    Parameters:\n",
    "        criterios_file (str): Ruta del archivo de texto que contiene los criterios.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario donde las claves son los nombres de los criterios y los valores\n",
    "              son diccionarios con 'descripcion' y 'ejemplo'.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la carga de criterios.\n",
    "    \"\"\"\n",
    "    criterios = {}\n",
    "\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger('cargar_criterios')\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    try:\n",
    "        # Intentar abrir el archivo de criterios y leer su contenido\n",
    "        with open(criterios_file, 'r', encoding='utf-8') as file:\n",
    "            contenido = file.read()\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"El archivo {criterios_file} no se encontró.\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except IOError as e:\n",
    "        error_msg = f\"Error al leer el archivo {criterios_file}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Dividir el contenido en secciones usando '@@' como delimitador\n",
    "        secciones = contenido.split('@@')\n",
    "\n",
    "        # Validar que el contenido esté correctamente estructurado\n",
    "        if len(secciones) < 3 or len(secciones) % 2 == 0:\n",
    "            error_msg = f\"Formato incorrecto en el archivo {criterios_file}. Verifique que cada criterio tenga nombre y detalles asociados.\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        # Iterar sobre las secciones para extraer los criterios y sus detalles\n",
    "        for i in range(1, len(secciones), 2):\n",
    "            nombre_criterio = secciones[i].strip()  # Extraer y limpiar el nombre del criterio\n",
    "            detalles = secciones[i + 1].strip()  # Extraer y limpiar los detalles del criterio\n",
    "            partes = detalles.split(\"Ejemplo:\")  # Dividir detalles en descripción y ejemplo\n",
    "            descripcion = partes[0].replace(\"Descripción:\", \"\").strip()  # Limpiar la descripción\n",
    "            ejemplo = partes[1].strip() if len(partes) > 1 else \"\"  # Limpiar el ejemplo si existe\n",
    "\n",
    "            # Almacenar el criterio en el diccionario\n",
    "            criterios[nombre_criterio] = {\"descripcion\": descripcion, \"ejemplo\": ejemplo}\n",
    "\n",
    "        # Si no se encontraron criterios válidos, lanzar una excepción\n",
    "        if not criterios:\n",
    "            error_msg = f\"No se encontraron criterios válidos en el archivo {criterios_file}.\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    except IndexError:\n",
    "        error_msg = \"Formato incorrecto en el archivo de criterios. Verifique la estructura del archivo.\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Error en cargar_criterios: {e}\")\n",
    "        raise RuntimeError(f\"Error en cargar_criterios: {e}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado al procesar el archivo {criterios_file}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    logger.info(\"Criterios cargados correctamente. No se encontraron errores.\")\n",
    "    \n",
    "    return criterios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "import logging\n",
    "\n",
    "def verifica_estructura_examen(examen_file):\n",
    "    \"\"\"\n",
    "    Verifica que un notebook Jupyter sigue la estructura esperada para un examen.\n",
    "\n",
    "    Parameters:\n",
    "        examen_file (str): Ruta del archivo del notebook de examen.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si se detecta un error crítico en la estructura.\n",
    "    \"\"\"\n",
    "    errores = []\n",
    "    contexto_detectado = False\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    codigo_encontrado = False\n",
    "\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger(f'verifica_estructura_examen')\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(examen_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "        logger.info(f\"Archivo del examen {examen_file} leído correctamente.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error: El archivo {examen_file} no se encontró.\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al leer el archivo {examen_file}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                # Si se esperaba una solución y no se encontró código, generar un error\n",
    "                if se_espera_solucion and not codigo_encontrado:\n",
    "                    error_msg = f\"La Solución del Ejercicio {ejercicio_num} no contiene código.\"\n",
    "                    errores.append(error_msg)\n",
    "                    logger.error(error_msg)\n",
    "                se_espera_solucion = False\n",
    "                codigo_encontrado = False\n",
    "\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Verificar Contexto\n",
    "                if cell_content.startswith(\"## Contexto\"):\n",
    "                    if contexto_detectado:\n",
    "                        error_msg = \"Más de un '## Contexto' detectado.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logger.error(error_msg)\n",
    "                    contexto_detectado = True\n",
    "\n",
    "                # Verificar Ejercicios y Criterios\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "\n",
    "                    if \"Criterios:\" not in cell_content:\n",
    "                        error_msg = f\"El Ejercicio {ejercicio_num} no contiene la sección 'Criterios'.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logger.error(error_msg)\n",
    "                    else:\n",
    "                        criterios = cell_content.split(\"Criterios:\")[-1].strip()\n",
    "                        criterios_list = criterios.split(\",\")\n",
    "\n",
    "                        # Verificar si cada criterio está delimitado correctamente y no está vacío\n",
    "                        for criterio in criterios_list:\n",
    "                            criterio = criterio.strip()\n",
    "                            if not (criterio.startswith(\"@@\") and criterio.endswith(\"@@\")) or len(criterio) <= 4:\n",
    "                                error_msg = f\"Criterios mal formateados o vacíos en el Ejercicio {ejercicio_num}\"\n",
    "                                errores.append(error_msg)\n",
    "                                logger.error(error_msg)\n",
    "                                break\n",
    "\n",
    "            elif cell.cell_type == 'code':\n",
    "                if se_espera_solucion:\n",
    "                    cell_content = cell['source'].strip()\n",
    "\n",
    "                    # Eliminar saltos de línea y espacios adicionales para una comparación más robusta\n",
    "                    normalized_content = \" \".join(cell_content.split())\n",
    "\n",
    "                    # Verificar si la celda comienza con \"## Solución ejercicio\" o \"## Solucion ejercicio\"\n",
    "                    if normalized_content.startswith(\"## Solución ejercicio\") or normalized_content.startswith(\"## Solucion ejercicio\"):\n",
    "                        try:\n",
    "                            sol_num = int(normalized_content.split(\"## Solución ejercicio\" if \"## Solución ejercicio\" in normalized_content else \"## Solucion ejercicio\")[1].strip().split()[0])\n",
    "                            if sol_num != ejercicio_num:\n",
    "                                error_msg = f\"La Solución ejercicio {sol_num} no corresponde al Ejercicio {ejercicio_num}.\"\n",
    "                                errores.append(error_msg)\n",
    "                                logger.error(error_msg)\n",
    "                            codigo_encontrado = False  # Reiniciar el indicador de código encontrado\n",
    "                        except (ValueError, IndexError):\n",
    "                            error_msg = f\"Formato de número incorrecto en la Solución del Ejercicio {ejercicio_num}.\"\n",
    "                            errores.append(error_msg)\n",
    "                            logger.error(error_msg)\n",
    "                    else:\n",
    "                        error_msg = f\"El Ejercicio {ejercicio_num} tiene una celda de código que no comienza con '## Solución ejercicio {ejercicio_num}' o '## Solucion ejercicio {ejercicio_num}'.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logger.error(error_msg)\n",
    "                        se_espera_solucion = False\n",
    "\n",
    "                    # Verificar si la celda contiene código más allá de comentarios o está vacía\n",
    "                    lines = cell_content.split(\"\\n\")\n",
    "                    for line in lines:\n",
    "                        stripped_line = line.strip()\n",
    "                        if stripped_line and not stripped_line.startswith(\"#\"):\n",
    "                            codigo_encontrado = True\n",
    "                            break\n",
    "\n",
    "        # Verificar si la última solución esperada fue proporcionada y si tenía código\n",
    "        if se_espera_solucion and not codigo_encontrado:\n",
    "            error_msg = f\"La Solución del Ejercicio {ejercicio_num} no contiene código.\"\n",
    "            errores.append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "\n",
    "        # Verificaciones finales\n",
    "        if not contexto_detectado:\n",
    "            error_msg = \"No se detectó un '## Contexto' en el notebook.\"\n",
    "            errores.append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado al procesar el archivo {examen_file}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    # Log de resultado final\n",
    "    if errores:\n",
    "        logger.error(\"EXAMEN Estructura Incorrecta: Se encontraron los siguientes errores:\")\n",
    "        for error in errores:\n",
    "            logger.error(error)\n",
    "        raise RuntimeError(\"EXAMEN Se encontraron errores críticos en la estructura del notebook.\")\n",
    "    else:\n",
    "        logger.info(\"EXAMEN Estructura Correcta: El notebook sigue la estructura esperada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import logging\n",
    "\n",
    "def extrae_informacion_examen(examen_file, criterios_validos):\n",
    "    \"\"\"\n",
    "    Extrae la información del examen desde un notebook Jupyter y verifica que los criterios estén en la lista de criterios válidos.\n",
    "\n",
    "    Parameters:\n",
    "        examen_file (str): Ruta del archivo del notebook de examen.\n",
    "        criterios_validos (dict_keys): Las claves de un diccionario de criterios válidos.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con el 'contexto' del examen y una lista de 'ejercicios', \n",
    "              donde cada ejercicio tiene 'enunciado', 'criterios', y 'solucion'.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la extracción de información,\n",
    "                      o si un criterio no está en la lista de criterios válidos.\n",
    "    \"\"\"\n",
    "    examen_info = {\n",
    "        \"contexto\": \"\",\n",
    "        \"ejercicios\": []\n",
    "    }\n",
    "\n",
    "    contexto_detectado = False\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    solucion_detectada = False\n",
    "    ejercicio_info = {}\n",
    "\n",
    "    # Logger específico para esta función\n",
    "    logger = logging.getLogger('extrae_informacion_examen')\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(examen_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"El archivo {examen_file} no se encontró.\"\n",
    "        logger.critical(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al leer el archivo {examen_file}: {e}\"\n",
    "        logger.critical(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Extraer Contexto\n",
    "                if cell_content.startswith(\"## Contexto\"):\n",
    "                    examen_info[\"contexto\"] = cell_content.split(\"## Contexto:\")[-1].strip()\n",
    "                    contexto_detectado = True\n",
    "    \n",
    "                # Extraer Enunciado y Criterios del Ejercicio\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    if ejercicio_num > 0:\n",
    "                        examen_info[\"ejercicios\"].append(ejercicio_info)\n",
    "\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "                    solucion_detectada = False\n",
    "                    ejercicio_info = {\n",
    "                        \"enunciado\": cell_content.split(\"## Ejercicio\")[1].split(\"Criterios:\")[0].strip(),\n",
    "                        \"criterios\": [],\n",
    "                        \"solucion\": []\n",
    "                    }\n",
    "                    criterios_text = cell_content.split(\"Criterios:\")[-1].strip()\n",
    "                    criterios_list = [criterio.strip() for criterio in criterios_text.split(\",\")]\n",
    "                    \n",
    "                    # Verificar que los criterios estén en la lista de criterios válidos\n",
    "                    for criterio in criterios_list:\n",
    "                        criterio_sin_arrobas = criterio.replace(\"@@\", \"\")\n",
    "                        if criterio_sin_arrobas not in criterios_validos:\n",
    "                            error_msg = f\"Criterio no válido encontrado: {criterio} en el ejercicio {ejercicio_num}.\"\n",
    "                            logger.critical(error_msg)\n",
    "                            raise RuntimeError(error_msg)\n",
    "                    \n",
    "                    ejercicio_info[\"criterios\"] = criterios_list\n",
    "\n",
    "            elif cell.cell_type == 'code' and se_espera_solucion:\n",
    "                # Extraer solución de las celdas de código\n",
    "                ejercicio_info[\"solucion\"].append(cell['source'].strip())\n",
    "                solucion_detectada = True\n",
    "\n",
    "        # Agregar la información del último ejercicio si no ha sido añadido\n",
    "        if ejercicio_num > 0 and ejercicio_info:\n",
    "            examen_info[\"ejercicios\"].append(ejercicio_info)\n",
    "\n",
    "        if not contexto_detectado:\n",
    "            error_msg = \"No se detectó un '## Contexto' en el notebook.\"\n",
    "            logger.critical(error_msg)\n",
    "            raise RuntimeError(error_msg)\n",
    "\n",
    "        logger.info(\"La información del examen ha sido extraída correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado al procesar el archivo {examen_file}: {e}\"\n",
    "        logger.critical(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    return examen_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "import logging\n",
    "\n",
    "def procesa_respuestas_alumno(alumno_file, numero_ejercicios):\n",
    "    \"\"\"\n",
    "    Procesa un notebook de respuestas de un alumno para extraer las soluciones a los ejercicios.\n",
    "\n",
    "    Parameters:\n",
    "        alumno_file (str): Ruta del archivo del notebook de respuesta del alumno.\n",
    "        numero_ejercicios (int): Número esperado de ejercicios en el examen.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con el 'nombre_alumno' y una lista de 'ejercicios', donde cada ejercicio\n",
    "              tiene su 'enunciado' y 'solucion' (o un valor indicando que no fue respondido).\n",
    "    \"\"\"\n",
    "    # Extraer el nombre del alumno desde el nombre del archivo\n",
    "    nombre_alumno = os.path.splitext(os.path.basename(alumno_file))[0]\n",
    "\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger(f'procesa_respuestas_alumno_{nombre_alumno}')\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Iniciar el log indicando el inicio del procesamiento\n",
    "    logger.info(f\"Procesando respuestas del alumno {nombre_alumno}\")\n",
    "\n",
    "    respuestas_alumno = {\n",
    "        \"nombre_alumno\": nombre_alumno,\n",
    "        \"ejercicios\": []\n",
    "    }\n",
    "\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    ejercicio_info = {}\n",
    "\n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(alumno_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Archivo no encontrado para el alumno {nombre_alumno}: {alumno_file}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al leer el notebook para el alumno {nombre_alumno}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": f\"Error al leer el notebook: {str(e)}\"}\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Identificar un nuevo ejercicio basado en el enunciado\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    # Si estábamos esperando una solución y no se añadió código, marcamos como no respondido\n",
    "                    if se_espera_solucion:\n",
    "                        if not ejercicio_info.get(\"solucion\") or all(\n",
    "                            not line.strip() or line.strip().startswith(\"#\") \n",
    "                            for solution in ejercicio_info[\"solucion\"] \n",
    "                            for line in solution.splitlines()):\n",
    "                            ejercicio_info[\"solucion\"] = \"No respondido\"\n",
    "                        respuestas_alumno[\"ejercicios\"].append(ejercicio_info)\n",
    "\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "                    ejercicio_info = {\n",
    "                        \"enunciado\": cell_content.split(\"## Ejercicio\")[1].strip(),\n",
    "                        \"solucion\": []\n",
    "                    }\n",
    "\n",
    "            elif cell.cell_type == 'code' and se_espera_solucion:\n",
    "                # Procesar celdas de código para una solución\n",
    "                codigo = cell['source'].strip()\n",
    "                if codigo:\n",
    "                    ejercicio_info[\"solucion\"].append(codigo)\n",
    "\n",
    "        # Agregar la información del último ejercicio si no ha sido añadido\n",
    "        if se_espera_solucion:\n",
    "            if not ejercicio_info.get(\"solucion\") or all(\n",
    "                not line.strip() or line.strip().startswith(\"#\") \n",
    "                for solution in ejercicio_info[\"solucion\"] \n",
    "                for line in solution.splitlines()):\n",
    "                ejercicio_info[\"solucion\"] = \"No respondido\"\n",
    "            respuestas_alumno[\"ejercicios\"].append(ejercicio_info)\n",
    "\n",
    "        # Verificar que el número de ejercicios y soluciones coincida con el número esperado\n",
    "        num_ejercicios = len(respuestas_alumno[\"ejercicios\"])\n",
    "        num_soluciones = sum(1 for ejercicio in respuestas_alumno[\"ejercicios\"] if ejercicio[\"solucion\"] != \"No respondido\")\n",
    "\n",
    "        if num_ejercicios != numero_ejercicios:\n",
    "            error_msg = f\"El número de ejercicios en el notebook ({num_ejercicios}) no coincide con el número esperado ({numero_ejercicios}) para el alumno {nombre_alumno}.\"\n",
    "            logger.error(error_msg)\n",
    "            return {\"error\": error_msg}\n",
    "\n",
    "        if num_soluciones != numero_ejercicios:\n",
    "            error_msg = f\"El número de soluciones en el notebook ({num_soluciones}) no coincide con el número esperado de ejercicios ({numero_ejercicios}) para el alumno {nombre_alumno}.\"\n",
    "            logger.error(error_msg)\n",
    "            return {\"error\": error_msg}\n",
    "\n",
    "        logger.info(f\"Respuestas del alumno {nombre_alumno} procesadas correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado al procesar el archivo {alumno_file} para el alumno {nombre_alumno}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": f\"Error inesperado: {str(e)}\"}\n",
    "\n",
    "    return respuestas_alumno\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def comprueba_ejecucion(respuestas_alumno):\n",
    "    \"\"\"\n",
    "    Comprueba si las soluciones de los ejercicios se ejecutan sin errores y añade el estado\n",
    "    y el mensaje de error (si corresponde) al diccionario original de respuestas del alumno.\n",
    "\n",
    "    Parameters:\n",
    "        respuestas_alumno (dict): Diccionario que contiene las respuestas del alumno y sus soluciones.\n",
    "        \n",
    "    Returns:\n",
    "        dict: El mismo diccionario 'respuestas_alumno' con los campos 'estado' y 'mensaje_de_error'\n",
    "              añadidos a cada ejercicio.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la ejecución de la función.\n",
    "    \"\"\"\n",
    "    # Configurar un logger específico para el alumno\n",
    "    nombre_alumno = respuestas_alumno[\"nombre_alumno\"]\n",
    "    logger_detallado = logging.getLogger(f'detallado_{nombre_alumno}')\n",
    "\n",
    "    try:\n",
    "        for i, ejercicio in enumerate(respuestas_alumno[\"ejercicios\"], 1):\n",
    "            solucion = ejercicio[\"solucion\"]\n",
    "\n",
    "            if solucion == \"No respondido\":\n",
    "                ejercicio[\"estado\"] = \"No respondido\"\n",
    "                ejercicio[\"mensaje_de_error\"] = None\n",
    "            else:\n",
    "                # Redirigir la salida estándar a un objeto StringIO para capturarla\n",
    "                original_stdout = sys.stdout\n",
    "                sys.stdout = io.StringIO()\n",
    "\n",
    "                # Reemplazar plt.show con una función que no hace nada\n",
    "                original_show = plt.show\n",
    "                plt.show = lambda *args, **kwargs: None\n",
    "\n",
    "                try:\n",
    "                    # Crear un diccionario para almacenar el entorno de ejecución\n",
    "                    entorno_ejecucion = {}\n",
    "                    # Concatenar todos los bloques de código en una sola cadena\n",
    "                    codigo_completo = \"\\n\".join(solucion)\n",
    "                    # Ejecutar el código completo en el entorno de ejecución\n",
    "                    exec(codigo_completo, entorno_ejecucion)\n",
    "                    ejercicio[\"estado\"] = \"Correcto\"\n",
    "                    ejercicio[\"mensaje_de_error\"] = None\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Si hay un error en la ejecución, marcar el estado como \"Error\" y guardar el mensaje de error\n",
    "                    ejercicio[\"estado\"] = \"Error\"\n",
    "                    ejercicio[\"mensaje_de_error\"] = str(e)\n",
    "                    logger_detallado.error(f\"Error en la ejecución del Ejercicio {i} para {nombre_alumno}: {e}\")\n",
    "                finally:\n",
    "                    # Cerrar todas las figuras abiertas para evitar la acumulación de memoria\n",
    "                    plt.close('all')\n",
    "                    # Restaurar plt.show a su comportamiento original\n",
    "                    plt.show = original_show\n",
    "                    # Restaurar la salida estándar original\n",
    "                    sys.stdout = original_stdout\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en comprueba_ejecucion para el alumno {nombre_alumno}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    return respuestas_alumno\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from openai import OpenAI, OpenAIError, RateLimitError\n",
    "\n",
    "def evaluar_con_chatgpt(contexto, codigo, enunciado, criterios_texto, prompt_template):\n",
    "    \"\"\"\n",
    "    Evalúa el código de un ejercicio utilizando el modelo GPT-4 de OpenAI.\n",
    "\n",
    "    Parameters:\n",
    "        contexto (str): El contexto del examen.\n",
    "        codigo (str): El código del ejercicio a evaluar.\n",
    "        enunciado (str): El enunciado del ejercicio.\n",
    "        criterios_texto (str): Texto que contiene los criterios específicos para este ejercicio.\n",
    "        prompt_template (str): El template del prompt con marcadores para la descripción y el código.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con la puntuación y comentario del ejercicio.\n",
    "    \"\"\"\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger(f'evaluar_con_chatgpt')\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Insertar los valores en el prompt, incluyendo los criterios específicos para este ejercicio\n",
    "    prompt = prompt_template.format(contexto=contexto, enunciado=enunciado, codigo=codigo, criterios=criterios_texto)\n",
    "\n",
    "    try:\n",
    "        cliente = OpenAI()\n",
    "        response = cliente.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a programming teaching assistant evaluating student code.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    except RateLimitError as e:\n",
    "        error_msg = f\"Rate limit error: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except OpenAIError as e:\n",
    "        error_msg = f\"OpenAI API error: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except Exception as e:\n",
    "        error_msg = f\"General error: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    # Extraer la respuesta de ChatGPT\n",
    "    try:\n",
    "        evaluacion = response.choices[0].message.content\n",
    "    except (KeyError, IndexError) as e:\n",
    "        error_msg = f\"Error al procesar la respuesta de la API: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    return evaluacion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import io\n",
    "def evaluar_ejercicios(info_examen, resultados_alumno, prompt_file, criterios_file):\n",
    "    \"\"\"\n",
    "    Evalúa los ejercicios utilizando GPT-4 y devuelve las notas y comentarios para cada ejercicio.\n",
    "    \n",
    "    Parameters:\n",
    "        info_examen (dict): Diccionario con el contexto del examen y los enunciados de los ejercicios, obtenido de `extrae_informacion_examen`.\n",
    "        resultados_alumno (dict): Diccionario con las soluciones y estado de ejecución de los ejercicios, obtenido de `comprueba_ejecucion`.\n",
    "        prompt_file (str): Ruta del archivo de texto que contiene el prompt.\n",
    "        criterios_file (str): Ruta del archivo de texto que contiene los criterios.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con las evaluaciones de cada ejercicio.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la evaluación.\n",
    "    \"\"\"\n",
    "    # Crear un logger específico para la función\n",
    "    logger = logging.getLogger(f'evaluar_ejercicios')\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    evaluaciones = {}\n",
    "\n",
    "    try:\n",
    "        # Leer el prompt desde el archivo\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as file:\n",
    "            prompt_template = file.read()\n",
    "\n",
    "        # Cargar los criterios desde el archivo\n",
    "        criterios_info = cargar_criterios(criterios_file)\n",
    "\n",
    "        # Iterar sobre los ejercicios y sus resultados\n",
    "        for i, (ejercicio_info, resultado_alumno) in enumerate(zip(info_examen['ejercicios'], resultados_alumno['ejercicios']), start=1):\n",
    "            enunciado = ejercicio_info['enunciado']\n",
    "            codigo = resultado_alumno['solucion']\n",
    "            criterios_nombres = ejercicio_info.get('criterios', [])\n",
    "\n",
    "            try:\n",
    "                # Generar el texto de los criterios para el prompt\n",
    "                criterios_texto = \"\\n\\n\".join([\n",
    "                    f\"**{criterio.strip('@')}**\\nDescripción: {criterios_info[criterio.strip('@')]['descripcion']}\\nEjemplo: {criterios_info[criterio.strip('@')]['ejemplo']}\"\n",
    "                    for criterio in criterios_nombres if criterio.strip('@') in criterios_info\n",
    "                ])\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"Error en el formato de los criterios: {e}\")\n",
    "                raise RuntimeError(f\"Error en el formato de los criterios: {e}\")\n",
    "\n",
    "            # Redirigir la salida estándar a un objeto StringIO para capturarla\n",
    "            original_stdout = sys.stdout\n",
    "            sys.stdout = io.StringIO()\n",
    "\n",
    "            try:\n",
    "                if resultado_alumno['estado'] == \"No respondido\":\n",
    "                    logger.warning(f\"El ejercicio {i} no fue respondido por el alumno.\")\n",
    "                    evaluaciones[f'Ejercicio {i}'] = (\n",
    "                        \"**Puntuaciones**: [0]\\n\"\n",
    "                        \"**Comentarios**: [\\\"El ejercicio no fue respondido.\\\"]\\n\"\n",
    "                        \"**Comentario General**: [\\\"El alumno no proporcionó ninguna solución para este ejercicio.\\\"]\"\n",
    "                    )\n",
    "                elif resultado_alumno['estado'] == \"Error\":\n",
    "                    logger.warning(f\"El ejercicio {i} contiene un error en la ejecución.\")\n",
    "                    evaluaciones[f'Ejercicio {i}'] = (\n",
    "                        \"**Puntuaciones**: [0]\\n\"\n",
    "                        f\"**Comentarios**: [\\\"Error en la ejecución: {resultado_alumno['mensaje_de_error']}\\\"]\\n\"\n",
    "                        \"**Comentario General**: [\\\"El código presentado contiene errores que impiden su correcta ejecución.\\\"]\"\n",
    "                    )\n",
    "                else:\n",
    "                    resultado = evaluar_con_chatgpt(\n",
    "                        info_examen['contexto'], codigo, enunciado, criterios_texto, prompt_template\n",
    "                    )\n",
    "                    evaluaciones[f'Ejercicio {i}'] = resultado\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error al evaluar el ejercicio {i} para el alumno: {e}\"\n",
    "                logger.error(error_msg)\n",
    "                evaluaciones[f'Ejercicio {i}'] = (\n",
    "                    \"**Puntuaciones**: [0]\\n\"\n",
    "                    f\"**Comentarios**: [\\\"Error durante la evaluación: {str(e)}\\\"]\\n\"\n",
    "                    \"**Comentario General**: [\\\"Ocurrió un error inesperado durante la evaluación del código.\\\"]\"\n",
    "                )\n",
    "\n",
    "            finally:\n",
    "                # Restaurar la salida estándar original\n",
    "                sys.stdout = original_stdout\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en evaluar_ejercicios: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar y remover el handler del logger\n",
    "        log_stream.close()\n",
    "        logger.removeHandler(log_stream)\n",
    "\n",
    "    return evaluaciones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "def extraer_resultados(resultados, nombre_alumno):\n",
    "    \"\"\"\n",
    "    Extrae las puntuaciones, comentarios y comentarios generales de una estructura de resultados para todos los ejercicios.\n",
    "\n",
    "    Parámetros:\n",
    "    resultados (dict): Diccionario con las evaluaciones para todos los ejercicios.\n",
    "    nombre_alumno (str): Nombre del alumno.\n",
    "\n",
    "    Retorna:\n",
    "    dict: Un diccionario con los resultados estructurados para todos los ejercicios.\n",
    "    \"\"\"\n",
    "    # Crear un logger específico para este alumno\n",
    "    logger = logging.getLogger(f'extraer_resultados_{nombre_alumno}')\n",
    "    log_stream = logging.StreamHandler()\n",
    "    logger.addHandler(log_stream)\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar los resultados\n",
    "    resultados_detallados = {}\n",
    "\n",
    "    try:\n",
    "        # Recorrer cada ejercicio y extraer los resultados\n",
    "        for ejercicio, evaluacion in resultados.items():\n",
    "            try:\n",
    "                comentarios = evaluacion  # El texto completo del resultado\n",
    "\n",
    "                # Buscar el patrón para las puntuaciones usando una expresión regular\n",
    "                puntuaciones_pattern = re.search(r'\\*\\*Puntuaciones\\*\\*:\\s*(\\[[^\\]]*\\])', comentarios)\n",
    "                comentarios_pattern = re.search(r'\\*\\*Comentarios\\*\\*:\\s*(\\[.*?\\])', comentarios, re.DOTALL)\n",
    "                comentario_general_pattern = re.search(r'\\*\\*Comentario General\\*\\*:\\s*(\\[.*?\\])', comentarios, re.DOTALL)\n",
    "\n",
    "                if puntuaciones_pattern:\n",
    "                    puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "                else:\n",
    "                    puntuaciones = []\n",
    "                    logger.warning(f\"No se encontraron puntuaciones para el ejercicio {ejercicio} de {nombre_alumno}\")\n",
    "\n",
    "                if comentarios_pattern:\n",
    "                    comentarios_list = eval(comentarios_pattern.group(1))\n",
    "                else:\n",
    "                    comentarios_list = []\n",
    "                    logger.warning(f\"No se encontraron comentarios para el ejercicio {ejercicio} de {nombre_alumno}\")\n",
    "\n",
    "                if comentario_general_pattern:\n",
    "                    comentario_general = eval(comentario_general_pattern.group(1))[0]  # Extrae el primer elemento\n",
    "                else:\n",
    "                    comentario_general = \"No disponible\"\n",
    "                    logger.warning(f\"No se encontró comentario general para el ejercicio {ejercicio} de {nombre_alumno}\")\n",
    "\n",
    "                resultados_detallados[ejercicio] = {\n",
    "                    'puntuaciones': puntuaciones,\n",
    "                    'comentarios': comentarios_list,\n",
    "                    'comentario_general': comentario_general\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error procesando los resultados para '{nombre_alumno}' en el ejercicio '{ejercicio}': {e}\"\n",
    "                logger.error(error_msg)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar y remover el handler del logger\n",
    "        log_stream.close()\n",
    "        logger.removeHandler(log_stream)\n",
    "\n",
    "    return resultados_detallados\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def generar_informe_profesor(examen_info, resultados, nombre_examen, output_dir):\n",
    "    \"\"\"\n",
    "    Genera un informe en PDF para el profesor que resume las calificaciones, comentarios y errores \n",
    "    para cada alumno, junto con gráficos de notas.\n",
    "\n",
    "    Parameters:\n",
    "        examen_info (dict): Diccionario que contiene el contexto del examen y los enunciados de los ejercicios.\n",
    "        resultados (dict): Diccionario con los resultados de la evaluación para todos los alumnos.\n",
    "        nombre_examen (str): Nombre del examen.\n",
    "        output_dir (str): Directorio donde se guardará el informe en PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Crear el objeto PDF\n",
    "        pdf = FPDF()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        \n",
    "        # Agregar la portada\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 16)\n",
    "        pdf.cell(0, 10, f\"Informe Resumido de Evaluación - {nombre_examen}\", ln=True, align='C')\n",
    "        pdf.ln(20)\n",
    "        \n",
    "        # Inicializar las variables para las notas finales\n",
    "        notas_finales = []\n",
    "        notas_por_ejercicio = {f'Ejercicio {i+1}': [] for i in range(len(examen_info['ejercicios']))}\n",
    "\n",
    "        # Resumen para cada alumno\n",
    "        for alumno, resultado_alumno in resultados.items():\n",
    "            if alumno == 'criterios':\n",
    "                continue  # Saltar el diccionario de criterios, ya que no es un alumno\n",
    "            \n",
    "            pdf.add_page()\n",
    "            pdf.set_font(\"Arial\", \"B\", 14)\n",
    "            \n",
    "            # Calcular y añadir la nota final para el alumno al principio\n",
    "            notas_ejercicios = []\n",
    "            for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "                resultado_ejercicio = resultado_alumno.get(f\"Ejercicio {i}\", {})\n",
    "                puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "                if puntuaciones:\n",
    "                    nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                    notas_ejercicios.append(nota_media)\n",
    "            \n",
    "            if notas_ejercicios:\n",
    "                nota_final = sum(notas_ejercicios) / len(notas_ejercicios)\n",
    "                notas_finales.append(nota_final)\n",
    "                pdf.cell(0, 10, f\"Alumno: {alumno.split('.')[0]} - Nota Final: {nota_final:.2f}\", ln=True)\n",
    "            else:\n",
    "                pdf.cell(0, 10, f\"Alumno: {alumno.split('.')[0]}\", ln=True)\n",
    "            \n",
    "            pdf.ln(10)  # Añadir espacio para evitar superposiciones\n",
    "            \n",
    "            # Resumen de cada ejercicio\n",
    "            for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "                resultado_ejercicio = resultado_alumno.get(f\"Ejercicio {i}\", {})\n",
    "                puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "                comentarios = resultado_ejercicio.get('comentarios', [])\n",
    "                \n",
    "                if puntuaciones and comentarios:\n",
    "                    # Mostrar la puntuación y el comentario para cada criterio\n",
    "                    for j, (puntuacion, comentario) in enumerate(zip(puntuaciones, comentarios)):\n",
    "                        pdf.set_font(\"Arial\", \"\", 12)\n",
    "                        criterio_nombre = examen_info['ejercicios'][i-1]['criterios'][j].strip('@')\n",
    "                        pdf.cell(0, 10, f\"{criterio_nombre}: Puntuación - {puntuacion}\", ln=True)\n",
    "                        pdf.set_font(\"Arial\", \"I\", 12)\n",
    "                        pdf.multi_cell(0, 10, f\"Comentario - {comentario}\")\n",
    "                    \n",
    "                    # Mostrar la nota promedio del ejercicio\n",
    "                    nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                    notas_por_ejercicio[f'Ejercicio {i}'].append(nota_media)\n",
    "                    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                    pdf.cell(0, 10, f\"Nota del Ejercicio {i}: {nota_media:.2f}\", ln=True)\n",
    "                else:\n",
    "                    # Si no hay puntuaciones/comentarios, se considera un error\n",
    "                    pdf.set_font(\"Arial\", \"\", 12)\n",
    "                    pdf.cell(0, 10, \"Errores:\", ln=True)\n",
    "                    pdf.multi_cell(0, 10, f\"{resultado_ejercicio.get('comentarios', 'No disponible')}\")\n",
    "                \n",
    "                pdf.ln(5)\n",
    "            \n",
    "        # Gráfica del histograma de notas finales\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(notas_finales, bins=10, edgecolor='black')\n",
    "        plt.title('Histograma de Notas Finales')\n",
    "        plt.xlabel('Nota Final')\n",
    "        plt.ylabel('Número de Alumnos')\n",
    "        plt.tight_layout()\n",
    "        hist_path_finales = os.path.join(output_dir, 'histograma_notas_finales.png')\n",
    "        plt.savefig(hist_path_finales)\n",
    "        plt.close()\n",
    "\n",
    "        # Añadir el histograma de notas finales al PDF\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 14)\n",
    "        pdf.cell(0, 10, 'Histograma de Notas Finales', ln=True)\n",
    "        pdf.image(hist_path_finales, x=10, y=30, w=190)\n",
    "        \n",
    "        # Histograma de las notas y gráficas de criterios por ejercicio\n",
    "        graficos_a_borrar = [hist_path_finales]\n",
    "        for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "            notas = notas_por_ejercicio[f'Ejercicio {i}']\n",
    "            if notas:\n",
    "                # Histograma de las notas\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.hist(notas, bins=10, edgecolor='black')\n",
    "                plt.title(f'Histograma de Notas - Ejercicio {i}')\n",
    "                plt.xlabel('Nota')\n",
    "                plt.ylabel('Número de Alumnos')\n",
    "                plt.tight_layout()\n",
    "                img_path = os.path.join(output_dir, f'histograma_ejercicio_{i}.png')\n",
    "                plt.savefig(img_path)\n",
    "                plt.close()\n",
    "\n",
    "                graficos_a_borrar.append(img_path)\n",
    "\n",
    "                # Añadir el histograma al PDF\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", \"B\", 14)\n",
    "                pdf.cell(0, 10, f\"Histograma de Notas - Ejercicio {i}\", ln=True)\n",
    "                pdf.image(img_path, x=10, y=30, w=190)\n",
    "\n",
    "                # Gráfico de barras para las puntuaciones por criterio\n",
    "                criterios = [criterio.strip('@') for criterio in examen_info['ejercicios'][i-1]['criterios']]\n",
    "\n",
    "                # Calcular las puntuaciones por criterio manejando el caso de longitud desigual\n",
    "                puntuaciones_criterio = []\n",
    "                for j in range(len(criterios)):\n",
    "                    puntuaciones = [\n",
    "                        resultados[alumno][f'Ejercicio {i}']['puntuaciones'][j]\n",
    "                        for alumno in resultados\n",
    "                        if alumno != 'criterios' and f'Ejercicio {i}' in resultados[alumno] and\n",
    "                        len(resultados[alumno][f'Ejercicio {i}']['puntuaciones']) > j\n",
    "                    ]\n",
    "                    puntuaciones_criterio.append(np.mean(puntuaciones))\n",
    "\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.bar(criterios, puntuaciones_criterio, color='blue', edgecolor='black')\n",
    "                plt.title(f'Puntuaciones por Criterio - Ejercicio {i}')\n",
    "                plt.xlabel('Criterio')\n",
    "                plt.ylabel('Puntuación Promedio')\n",
    "                plt.xticks(rotation=45, ha='right')  # Girar los nombres de los criterios 45 grados\n",
    "                plt.tight_layout()\n",
    "                img_path_criterios = os.path.join(output_dir, f'puntuaciones_criterios_ejercicio_{i}.png')\n",
    "                plt.savefig(img_path_criterios)\n",
    "                plt.close()\n",
    "\n",
    "                graficos_a_borrar.append(img_path_criterios)\n",
    "\n",
    "                # Añadir gráfico de puntuaciones por criterio al PDF\n",
    "                pdf.add_page()\n",
    "                pdf.image(img_path_criterios, x=10, y=30, w=190)\n",
    "        \n",
    "        # Guardar el PDF en el directorio de salida\n",
    "        output_path = os.path.join(output_dir, f\"Informe_Profesor_{nombre_examen}.pdf\")\n",
    "        pdf.output(output_path)\n",
    "        logging.info(f\"Informe resumido generado en: {output_path}\")\n",
    "\n",
    "        # Eliminar los archivos de gráficas temporales\n",
    "        for grafico in graficos_a_borrar:\n",
    "            if os.path.exists(grafico):\n",
    "                os.remove(grafico)\n",
    "                logging.info(f\"Archivo gráfico temporal eliminado: {grafico}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el informe del profesor: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def generar_informe_pdf_alumnos(examen_info, resultados, nombre_examen, output_dir):\n",
    "    \"\"\"\n",
    "    Genera informes en PDF para todos los alumnos basados en la evaluación de sus ejercicios.\n",
    "\n",
    "    Parameters:\n",
    "        examen_info (dict): Diccionario que contiene el contexto del examen y los enunciados de los ejercicios.\n",
    "        resultados (dict): Diccionario con los resultados de la evaluación para todos los alumnos.\n",
    "        nombre_examen (str): Nombre del examen.\n",
    "        output_dir (str): Directorio donde se guardarán los informes en PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for alumno in resultados.keys():\n",
    "        if alumno == 'criterios':\n",
    "            continue\n",
    "\n",
    "        nombre_alumno = alumno.split('.')[0]\n",
    "\n",
    "        try:\n",
    "            # Crear el objeto PDF\n",
    "            pdf = FPDF()\n",
    "            pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "            # Agregar la portada\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(\"Arial\", \"B\", 16)\n",
    "            pdf.cell(0, 10, f\"Informe de Evaluación de {nombre_alumno}\", ln=True, align='C')\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            pdf.cell(0, 10, f\"Curso: Introducción a Python para Finanzas\", ln=True, align='C')\n",
    "            pdf.cell(0, 10, f\"Examen: {nombre_examen}\", ln=True, align='C')\n",
    "            pdf.cell(0, 10, f\"Fecha: {pd.Timestamp('now').strftime('%d/%m/%Y')}\", ln=True, align='C')\n",
    "            pdf.ln(20)\n",
    "\n",
    "            # Introducción\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            pdf.multi_cell(0, 10, examen_info['contexto'])\n",
    "            pdf.ln(10)\n",
    "\n",
    "            # Resumen de Evaluación Global\n",
    "            pdf.set_font(\"Arial\", \"B\", 14)\n",
    "            pdf.cell(0, 10, \"Resumen de Evaluación Global\", ln=True)\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "\n",
    "            # Calcular la puntuación total y la nota media\n",
    "            notas_ejercicios = []\n",
    "            for i in range(1, len(examen_info['ejercicios']) + 1):\n",
    "                resultado_ejercicio = resultados[alumno].get(f\"Ejercicio {i}\", {})\n",
    "                puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "                if puntuaciones:\n",
    "                    nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                    notas_ejercicios.append(nota_media)\n",
    "            \n",
    "            if notas_ejercicios:\n",
    "                nota_final = sum(notas_ejercicios) / len(notas_ejercicios)\n",
    "            else:\n",
    "                nota_final = 0\n",
    "\n",
    "            pdf.cell(0, 10, f\"Puntuación Total: {nota_final:.2f}\", ln=True)\n",
    "            pdf.ln(5)\n",
    "\n",
    "            # Comentarios generales según la nota final\n",
    "            if nota_final >= 9:\n",
    "                comentarios_generales = \"Excelente desempeño. Sigue así para mantener este nivel.\"\n",
    "            elif nota_final >= 7:\n",
    "                comentarios_generales = \"Buen desempeño, pero hay algunas áreas que podrían beneficiarse de más práctica y revisión.\"\n",
    "            elif nota_final >= 5:\n",
    "                comentarios_generales = \"Desempeño aceptable, pero es necesario trabajar más en ciertos aspectos.\"\n",
    "            elif nota_final > 0:\n",
    "                comentarios_generales = \"El desempeño es bajo. Se recomienda revisar los conceptos básicos y practicar más.\"\n",
    "            else:\n",
    "                comentarios_generales = \"El alumno no ha completado satisfactoriamente el examen. Es necesario un repaso completo de los temas cubiertos.\"\n",
    "\n",
    "            pdf.multi_cell(0, 10, comentarios_generales)\n",
    "            pdf.ln(10)\n",
    "\n",
    "            # Detalle de Evaluación por Ejercicio\n",
    "            for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "                pdf.add_page()  # Cada ejercicio comienza en una nueva página\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "\n",
    "                # Calcular la puntuación del ejercicio\n",
    "                resultado_ejercicio = resultados[alumno].get(f\"Ejercicio {i}\", {})\n",
    "                puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "                if puntuaciones:\n",
    "                    nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                else:\n",
    "                    nota_media = 0\n",
    "\n",
    "                # Título del ejercicio con la puntuación\n",
    "                pdf.cell(0, 10, f\"Ejercicio {i}: {ejercicio['enunciado'].splitlines()[0]}\", ln=False)\n",
    "                pdf.set_x(-50)  # Mueve el cursor para la puntuación hacia la derecha\n",
    "                pdf.cell(0, 10, f\"Puntuación: {nota_media:.2f}\", ln=True, align='R')\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "\n",
    "                # Enunciado completo\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(0, 10, \"1. Enunciado:\", ln=True)\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "                pdf.multi_cell(0, 10, ejercicio['enunciado'])\n",
    "                pdf.ln(5)\n",
    "\n",
    "                # Criterios evaluados\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(0, 10, \"2. Criterios Evaluados:\", ln=True)\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "                for j, criterio in enumerate(ejercicio['criterios']):\n",
    "                    criterio_nombre = criterio.strip('@')\n",
    "                    descripcion_criterio = resultados['criterios'][criterio_nombre]['descripcion']\n",
    "                    puntuacion_criterio = puntuaciones[j] if j < len(puntuaciones) else \"N/A\"\n",
    "                    pdf.multi_cell(0, 10, f\"- {criterio_nombre} ({puntuacion_criterio}): {descripcion_criterio}\")\n",
    "                pdf.ln(5)\n",
    "\n",
    "                if resultado_ejercicio:\n",
    "                    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                    pdf.cell(0, 10, \"3. Comentarios:\", ln=True)\n",
    "                    pdf.set_font(\"Arial\", \"\", 12)\n",
    "                    for comentario in resultado_ejercicio['comentarios']:\n",
    "                        pdf.multi_cell(0, 10, f\"- {comentario}\")\n",
    "                    pdf.ln(5)\n",
    "\n",
    "                    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                    pdf.cell(0, 10, \"4. Comentario General:\", ln=True)\n",
    "                    pdf.set_font(\"Arial\", \"\", 12)\n",
    "                    pdf.multi_cell(0, 10, resultado_ejercicio['comentario_general'])\n",
    "                    pdf.ln(10)\n",
    "                else:\n",
    "                    pdf.cell(0, 10, \"No se encontraron resultados para este ejercicio.\", ln=True)\n",
    "                    pdf.ln(10)\n",
    "\n",
    "            # Conclusión y Recomendaciones ajustadas al nivel del examen\n",
    "            pdf.set_font(\"Arial\", \"B\", 14)\n",
    "            pdf.cell(0, 10, \"Conclusión y Recomendaciones\", ln=True)\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            \n",
    "            if nota_final >= 9:\n",
    "                conclusion = \"Has demostrado un excelente dominio del material. Mantén este nivel de esfuerzo y sigue perfeccionando tus habilidades.\"\n",
    "            elif nota_final >= 7:\n",
    "                conclusion = \"Buen trabajo, pero hay algunas áreas que podrían beneficiarse de más práctica y revisión.\"\n",
    "            elif nota_final >= 5:\n",
    "                conclusion = \"El desempeño es aceptable, pero se recomienda revisar los temas en los que tuviste más dificultades para mejorar en futuras evaluaciones.\"\n",
    "            elif nota_final > 0:\n",
    "                conclusion = \"El rendimiento en este examen indica que es necesario un repaso más exhaustivo de los temas cubiertos. Considera buscar apoyo adicional para resolver dudas.\"\n",
    "            else:\n",
    "                conclusion = \"Es crucial que dediques tiempo a revisar todos los conceptos clave del curso. Considera buscar ayuda para entender mejor los temas.\"\n",
    "\n",
    "            pdf.multi_cell(0, 10, conclusion)\n",
    "            pdf.ln(10)\n",
    "\n",
    "            # Guardar el PDF en el directorio de salida\n",
    "            output_path = os.path.join(output_dir, f\"{nombre_alumno}_informe.pdf\")\n",
    "            pdf.output(output_path)\n",
    "            logging.info(f\"Informe generado para {nombre_alumno} en: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar el informe para {nombre_alumno}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generar_excel_resultados(resultados, examen_info, filename_agrupado='resultados_evaluacion_agrupado.xlsx', filename_detallado='resultados_evaluacion_detallado.xlsx'):\n",
    "    # Estructuras para almacenar los datos de ambos archivos Excel\n",
    "    filas_agrupado = []\n",
    "    filas_detallado = []\n",
    "\n",
    "    # Iterar sobre los resultados de cada alumno\n",
    "    for alumno, ejercicios in resultados.items():\n",
    "        if alumno == 'criterios':  # Ignorar la clave de criterios\n",
    "            continue\n",
    "        \n",
    "        fila_agrupada = {'Alumno': alumno}\n",
    "        fila_detallada = {'Alumno': alumno}\n",
    "        notas = []\n",
    "\n",
    "        # Iterar sobre los ejercicios del alumno\n",
    "        for i, (ejercicio, contenido) in enumerate(ejercicios.items(), start=1):\n",
    "            if ejercicio == 'NOTA FINAL':  # Saltar la nota final, ya que se calculará de nuevo\n",
    "                continue\n",
    "\n",
    "            # Verificar si el ejercicio contiene un error de ejecución\n",
    "            if contenido == {'puntuaciones': [0], 'comentarios': [\"Error en la ejecución: name 'volatility' is not defined\"], 'comentario_general': 'El código presentado contiene errores que impiden su correcta ejecución.'}:\n",
    "                # Si hay un error de ejecución, asignar 0 a todos los criterios\n",
    "                puntuaciones = [0] * len(examen_info['ejercicios'][i-1]['criterios'])\n",
    "                nota_media = 0\n",
    "            else:\n",
    "                # Calcular la nota media del ejercicio\n",
    "                puntuaciones = contenido['puntuaciones']\n",
    "                nota_media = np.mean(puntuaciones)\n",
    "            \n",
    "            fila_agrupada[ejercicio] = nota_media\n",
    "            notas.append(nota_media)\n",
    "            \n",
    "            # Añadir la nota final del ejercicio\n",
    "            fila_detallada[f'{ejercicio} - NOTA FINAL'] = nota_media\n",
    "            \n",
    "            # Iterar sobre los criterios evaluados en el ejercicio\n",
    "            criterios_evaluados = examen_info['ejercicios'][i-1]['criterios']  # i-1 porque los ejercicios están basados en índice\n",
    "            for criterio, puntuacion in zip(criterios_evaluados, puntuaciones):\n",
    "                criterio_nombre = criterio.strip('@@')  # Quitar los \"@@\" si existen\n",
    "                fila_detallada[f'{ejercicio} - {criterio_nombre}'] = puntuacion\n",
    "\n",
    "        # Calcular la nota final como la media de las notas de los ejercicios\n",
    "        nota_final = np.mean(notas)\n",
    "        fila_agrupada['NOTA FINAL'] = nota_final\n",
    "        fila_detallada['NOTA FINAL'] = nota_final\n",
    "\n",
    "        filas_agrupado.append(fila_agrupada)\n",
    "        filas_detallado.append(fila_detallada)\n",
    "\n",
    "    # Crear los DataFrames y guardarlos en Excel\n",
    "    df_agrupado = pd.DataFrame(filas_agrupado)\n",
    "    df_agrupado.to_excel(filename_agrupado, index=False)\n",
    "    print(f\"Archivo agrupado guardado en {filename_agrupado}\")\n",
    "\n",
    "    df_detallado = pd.DataFrame(filas_detallado)\n",
    "    df_detallado.to_excel(filename_detallado, index=False)\n",
    "    print(f\"Archivo detallado guardado en {filename_detallado}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_problemas(problemas, output_dir):\n",
    "    \"\"\"\n",
    "    Guarda la lista de problemas en un archivo de texto en el directorio de reports.\n",
    "\n",
    "    Parameters:\n",
    "        problemas (list): Lista de problemas encontrados durante la evaluación.\n",
    "        output_dir (str): Directorio donde se guardará el archivo de problemas.\n",
    "    \"\"\"\n",
    "    problemas_file = os.path.join(output_dir, 'problemas_entregas.txt')\n",
    "    \n",
    "    with open(problemas_file, 'w') as f:\n",
    "        for problema in problemas:\n",
    "            f.write(f\"{problema}\\n\")\n",
    "    \n",
    "    print(f\"Archivo de problemas guardado en: {problemas_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_y_evalua_notebook(fich, directorio_entregas, examen_info, criterios_file, prompt_file, dir_log):\n",
    "    \"\"\"\n",
    "    Procesa y evalúa un notebook individual.\n",
    "\n",
    "    Parameters:\n",
    "        fich (str): Nombre del archivo del notebook del alumno.\n",
    "        directorio_entregas (str): Ruta del directorio donde se encuentran los notebooks.\n",
    "        examen_info (dict): Información del examen.\n",
    "        criterios (dict): Criterios de evaluación.\n",
    "        prompt_file (str): Ruta del archivo de prompt.\n",
    "        dir_log (str): Ruta del directorio donde se guardarán los logs de error.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Nombre del alumno y resultados de la evaluación, o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    # Crear logger temporal para este notebook\n",
    "    logger_temporal, log_stream = obtener_logger_temporal()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Procesar respuestas del alumno\n",
    "        alumno_file = os.path.join(directorio_entregas, fich)\n",
    "        respuestas_alumno = procesa_respuestas_alumno(alumno_file, len(examen_info['ejercicios']))\n",
    "        \n",
    "        # Comprobar la ejecución de las respuestas\n",
    "        respuestas_evaluadas = comprueba_ejecucion(respuestas_alumno)\n",
    "        \n",
    "        # Evaluar las respuestas utilizando ChatGPT\n",
    "        evaluaciones = evaluar_ejercicios(examen_info, respuestas_evaluadas, prompt_file, criterios_file)\n",
    "        \n",
    "        # Extraer los resultados detallados de la evaluación\n",
    "        resultados_detallados = extraer_resultados(evaluaciones, fich)\n",
    "        \n",
    "        return fich, resultados_detallados\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Registrar el error en el log temporal\n",
    "        logger_temporal.error(f\"Error procesando el notebook {fich}: {e}\")\n",
    "        # Guardar el log detallado en caso de error\n",
    "        guardar_log_error(log_stream, fich, dir_log)\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Cerrar el log temporal\n",
    "        log_stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener un logger temporal\n",
    "\n",
    "def obtener_logger_temporal():\n",
    "    log_stream = StringIO()\n",
    "    handler = logging.StreamHandler(log_stream)\n",
    "    logger = logging.getLogger(f'logger_temporal_{id(log_stream)}')\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    return logger, log_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para guardar logs de error en un archivo separado\n",
    "def guardar_log_error(log_stream, nombre_notebook, dir_log):\n",
    "    error_log_file = os.path.join(dir_log, f'log_errores_{nombre_notebook}.log')\n",
    "    with open(error_log_file, 'w') as f:\n",
    "        f.write(log_stream.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "def configurar_logs(directorio_raiz):\n",
    "    dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "\n",
    "    if os.path.exists(dir_log):\n",
    "        # Si el directorio existe, eliminar los archivos de log antiguos\n",
    "        for log_file in ['log_critico.log', 'log_detallado.log']:\n",
    "            log_path = os.path.join(dir_log, log_file)\n",
    "            if os.path.exists(log_path):\n",
    "                os.remove(log_path)\n",
    "    else:\n",
    "        os.makedirs(dir_log)\n",
    "\n",
    "    # Configurar log crítico global (mostrado en consola y en archivo)\n",
    "    log_critico_path = os.path.join(dir_log, 'log_critico.log')\n",
    "    handler_critico_file = logging.FileHandler(log_critico_path)\n",
    "    handler_critico_console = logging.StreamHandler()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            handler_critico_file,  # Registrar en archivo\n",
    "            handler_critico_console  # Mostrar en consola\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Configurar log detallado global (solo en archivo)\n",
    "    log_detallado_path = os.path.join(dir_log, 'log_detallado.log')\n",
    "    logger_detallado = logging.getLogger('detallado')\n",
    "    handler_detallado = logging.FileHandler(log_detallado_path)\n",
    "    formatter_detallado = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler_detallado.setFormatter(formatter_detallado)\n",
    "    logger_detallado.addHandler(handler_detallado)\n",
    "    logger_detallado.setLevel(logging.DEBUG)\n",
    "\n",
    "    return dir_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(directorio_raiz, nombre_fich_examen, nombre_examen):\n",
    "    try:\n",
    "        # Configurar los logs\n",
    "        dir_log = configurar_logs(directorio_raiz)\n",
    "        \n",
    "        # Log de inicio del proceso\n",
    "        logging.info(\"Inicio del proceso de evaluación.\")\n",
    "        \n",
    "        # Configurar las rutas basadas en el directorio raíz\n",
    "        directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "        directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "        directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "        criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "        fichero_examen = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "        fichero_res = 'resultados_evaluacion.xlsx'\n",
    "        prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "\n",
    "        # Crear el directorio de reports si no existe\n",
    "        if not os.path.exists(directorio_reports):\n",
    "            os.makedirs(directorio_reports)\n",
    "\n",
    "        # Cargar criterios\n",
    "        if not os.path.exists(criterios_file):\n",
    "            raise FileNotFoundError(f\"El archivo de criterios {criterios_file} no existe.\")\n",
    "        criterios = cargar_criterios(criterios_file)\n",
    "\n",
    "        # Preprocesar el examen\n",
    "        if not os.path.exists(fichero_examen):\n",
    "            raise FileNotFoundError(f\"El archivo de examen {fichero_examen} no existe.\")\n",
    "        examen_info = extrae_informacion_examen(fichero_examen, criterios.keys())\n",
    "\n",
    "        # Verificar estructura del examen\n",
    "        verifica_estructura_examen(fichero_examen)\n",
    "\n",
    "        # Listar los notebooks entregados por los alumnos\n",
    "        if not os.path.exists(directorio_entregas):\n",
    "            raise FileNotFoundError(f\"El directorio de entregas {directorio_entregas} no existe.\")\n",
    "        alumnos, ficheros = listar_notebooks(directorio_entregas)\n",
    "        if not ficheros:\n",
    "            raise FileNotFoundError(\"No se encontraron notebooks de alumnos en el directorio de entregas.\")\n",
    "        \n",
    "        # Inicializar el diccionario de resultados\n",
    "        resultados = {}\n",
    "        resultados['criterios'] = criterios\n",
    "        problemas = []  # Lista para almacenar los problemas con cada notebook\n",
    "\n",
    "        # # Procesar y evaluar cada notebook en paralelo\n",
    "        # resultados_alumnos = Parallel(n_jobs=-1, verbose = 13)(\n",
    "        #     delayed(procesa_y_evalua_notebook)(fich, directorio_entregas, examen_info, criterios, prompt_file, dir_log) \n",
    "        #     for fich in tqdm(ficheros, desc=\"Procesando notebooks\")\n",
    "        # )\n",
    "        \n",
    "        # Procesar y evaluar cada notebook de forma secuencial\n",
    "        resultados_alumnos = []\n",
    "        for fich in tqdm(ficheros, desc=\"Procesando notebooks\"):\n",
    "            resultado = procesa_y_evalua_notebook(fich, directorio_entregas, examen_info, criterios_file, prompt_file, dir_log)\n",
    "            resultados_alumnos.append(resultado)\n",
    "\n",
    "        # Filtrar resultados exitosos y agregar al diccionario de resultados\n",
    "        procesados_exitosamente = []\n",
    "        for result in resultados_alumnos:\n",
    "            if result is not None:\n",
    "                alumno, res_extraido = result\n",
    "                resultados[alumno] = res_extraido\n",
    "                procesados_exitosamente.append(alumno)\n",
    "            else:\n",
    "                problemas.append(f\"Un notebook no se pudo procesar correctamente.\")\n",
    "\n",
    "        # Generar los informes en PDF para cada estudiante\n",
    "        generar_informe_pdf_alumnos(examen_info, resultados, nombre_examen, directorio_reports)\n",
    "        \n",
    "        # Comparar la lista de notebooks entregados con los informes generados\n",
    "        informes_generados = os.listdir(directorio_reports)\n",
    "        for alumno in alumnos:\n",
    "            informe_esperado = f\"{alumno.split('.')[0]}_informe.pdf\"\n",
    "            if informe_esperado not in informes_generados:\n",
    "                problemas.append(f\"{alumno}: No se generó un informe.\")\n",
    "\n",
    "        # Generar el informe para el profesor\n",
    "        generar_informe_profesor(examen_info, resultados, nombre_examen, directorio_reports)\n",
    "\n",
    "         # Generar el archivo Excel con los resultados de la evaluación\n",
    "      \n",
    "        # Ruta del archivo Excel con los resultados agrupados\n",
    "        filename_agrupado = os.path.join(directorio_reports, fichero_res)\n",
    "\n",
    "        # Ruta del archivo Excel con los resultados detallados\n",
    "        filename_detallado = os.path.join(directorio_reports, 'resultados_evaluacion_detallado.xlsx')\n",
    "\n",
    "        # Llamar a la función con las rutas especificadas\n",
    "        generar_excel_resultados(resultados, examen_info, filename_agrupado=filename_agrupado, filename_detallado=filename_detallado)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "        # Guardar el archivo con la lista de problemas\n",
    "        guardar_problemas(problemas, directorio_reports)\n",
    "\n",
    "        # Log de finalización exitosa\n",
    "        logging.info(\"Proceso completado con éxito.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en el proceso: {e}\"\n",
    "        logging.critical(error_msg)\n",
    "        print(error_msg, file=sys.stderr)\n",
    "\n",
    "# Ejemplo de ejecución\n",
    "# examen_file = \"/workspace/examenes/examen_finanzas_gpt.ipynb\"\n",
    "\n",
    "# main(\"/workspace\", examen_file, \"Examen comprobar funcioanmiento con errores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 06:56:09,470 - INFO - Inicio del proceso de evaluación.\n",
      "Criterios cargados correctamente. No se encontraron errores.\n",
      "2024-08-29 06:56:09,473 - INFO - Criterios cargados correctamente. No se encontraron errores.\n",
      "La información del examen ha sido extraída correctamente.\n",
      "La información del examen ha sido extraída correctamente.\n",
      "2024-08-29 06:56:09,479 - INFO - La información del examen ha sido extraída correctamente.\n",
      "Archivo del examen /workspace/examenes/examen_finanzas_gpt.ipynb leído correctamente.\n",
      "2024-08-29 06:56:09,483 - INFO - Archivo del examen /workspace/examenes/examen_finanzas_gpt.ipynb leído correctamente.\n",
      "EXAMEN Estructura Correcta: El notebook sigue la estructura esperada.\n",
      "2024-08-29 06:56:09,485 - INFO - EXAMEN Estructura Correcta: El notebook sigue la estructura esperada.\n",
      "Se encontraron 2 archivos .ipynb en el directorio /workspace/entregas.\n",
      "2024-08-29 06:56:09,488 - INFO - Se encontraron 2 archivos .ipynb en el directorio /workspace/entregas.\n",
      "Procesando notebooks:   0%|          | 0/2 [00:00<?, ?it/s]Procesando respuestas del alumno Cortés_Eva\n",
      "Procesando respuestas del alumno Cortés_Eva\n",
      "2024-08-29 06:56:09,490 - INFO - Procesando respuestas del alumno Cortés_Eva\n",
      "Respuestas del alumno Cortés_Eva procesadas correctamente.\n",
      "Respuestas del alumno Cortés_Eva procesadas correctamente.\n",
      "2024-08-29 06:56:09,494 - INFO - Respuestas del alumno Cortés_Eva procesadas correctamente.\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  3 of 3 completed\n",
      "Criterios cargados correctamente. No se encontraron errores.\n",
      "2024-08-29 06:56:09,747 - INFO - Criterios cargados correctamente. No se encontraron errores.\n",
      "2024-08-29 06:56:11,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:14,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:18,298 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:22,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:25,328 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Procesando notebooks:  50%|█████     | 1/2 [00:15<00:15, 15.84s/it]Procesando respuestas del alumno Álvarez_Carlos\n",
      "Procesando respuestas del alumno Álvarez_Carlos\n",
      "2024-08-29 06:56:25,330 - INFO - Procesando respuestas del alumno Álvarez_Carlos\n",
      "Respuestas del alumno Álvarez_Carlos procesadas correctamente.\n",
      "Respuestas del alumno Álvarez_Carlos procesadas correctamente.\n",
      "2024-08-29 06:56:25,335 - INFO - Respuestas del alumno Álvarez_Carlos procesadas correctamente.\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  3 of 3 completed\n",
      "Criterios cargados correctamente. No se encontraron errores.\n",
      "2024-08-29 06:56:25,590 - INFO - Criterios cargados correctamente. No se encontraron errores.\n",
      "2024-08-29 06:56:27,268 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:30,719 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:34,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:37,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-29 06:56:42,027 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Procesando notebooks: 100%|██████████| 2/2 [00:32<00:00, 16.27s/it]\n",
      "2024-08-29 06:56:42,037 - INFO - Informe generado para Cortés_Eva en: /workspace/reports/Cortés_Eva_informe.pdf\n",
      "2024-08-29 06:56:42,043 - INFO - Informe generado para Álvarez_Carlos en: /workspace/reports/Álvarez_Carlos_informe.pdf\n",
      "2024-08-29 06:56:44,053 - INFO - Informe resumido generado en: /workspace/reports/Informe_Profesor_Examen comprobar funcioanmiento con errores.pdf\n",
      "2024-08-29 06:56:44,056 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_notas_finales.png\n",
      "2024-08-29 06:56:44,059 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_ejercicio_1.png\n",
      "2024-08-29 06:56:44,061 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/puntuaciones_criterios_ejercicio_1.png\n",
      "2024-08-29 06:56:44,065 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_ejercicio_2.png\n",
      "2024-08-29 06:56:44,068 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/puntuaciones_criterios_ejercicio_2.png\n",
      "2024-08-29 06:56:44,071 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_ejercicio_3.png\n",
      "2024-08-29 06:56:44,074 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/puntuaciones_criterios_ejercicio_3.png\n",
      "2024-08-29 06:56:44,076 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_ejercicio_4.png\n",
      "2024-08-29 06:56:44,079 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/puntuaciones_criterios_ejercicio_4.png\n",
      "2024-08-29 06:56:44,082 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/histograma_ejercicio_5.png\n",
      "2024-08-29 06:56:44,085 - INFO - Archivo gráfico temporal eliminado: /workspace/reports/puntuaciones_criterios_ejercicio_5.png\n",
      "2024-08-29 06:56:44,110 - INFO - Proceso completado con éxito.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo agrupado guardado en /workspace/reports/resultados_evaluacion.xlsx\n",
      "Archivo detallado guardado en /workspace/reports/resultados_evaluacion_detallado.xlsx\n",
      "Archivo de problemas guardado en: /workspace/reports/problemas_entregas.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "examen_file = \"/workspace/examenes/examen_finanzas_gpt.ipynb\"\n",
    "\n",
    "main(\"/workspace\", examen_file, \"Examen comprobar funcioanmiento con errores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
