{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "import re\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "import openai\n",
    "from openai import RateLimitError, OpenAIError\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from openai import OpenAI\n",
    "from fpdf import FPDF\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave de API\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"API key is not set\")\n",
    "\n",
    "# Inicializar la API de OpenAI\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_notebooks(directory):\n",
    "    \"\"\"\n",
    "    Devuelve una lista con los nombres de todos los archivos con extensión '.ipynb' en un directorio dado,\n",
    "    excluyendo aquellos que se llamen 'solucion.ipynb'.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): La ruta al directorio donde buscar los archivos.\n",
    "        \n",
    "    Returns:\n",
    "        list: Una lista con los nombres de los archivos que cumplen los criterios.\n",
    "    \"\"\"\n",
    "    archivos = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.ipynb') and filename != 'solucion.ipynb':\n",
    "            archivos.append(filename)\n",
    "            \n",
    "    alumnos = [archivo.replace('.ipynb', '') for archivo in archivos]\n",
    "    return alumnos, archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrae_criterios(prompt_file):\n",
    "    \"\"\"\n",
    "    Extrae los nombres de los criterios de un archivo de plantilla de prompt.\n",
    "\n",
    "    Parámetros:\n",
    "    prompt_file (str): Ruta al archivo que contiene la plantilla de prompt con los criterios delimitados por '@@'.\n",
    "\n",
    "    Retorna:\n",
    "    list: Una lista de cadenas, donde cada cadena es el nombre de un criterio extraído de la plantilla de prompt.\n",
    "    \"\"\"\n",
    "    # Abrir y leer el archivo de plantilla de prompt\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as file:\n",
    "        prompt_template = file.read()\n",
    "\n",
    "    # Buscar los nombres de los criterios usando una expresión regular\n",
    "    criteria_pattern = re.compile(r'@@(.*?)@@')\n",
    "    criteria = criteria_pattern.findall(prompt_template)\n",
    "    \n",
    "    return criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesa_examen(file_path):\n",
    "    \"\"\"\n",
    "    Procesa un notebook Jupyter para extraer el contexto del examen y los enunciados de los ejercicios.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Ruta del archivo del notebook de solución.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un diccionario con las claves 'contexto_examen' y 'enunciados_ejercicios'.\n",
    "    \"\"\"\n",
    "    # Inicialización de variables\n",
    "    contexto_examen = \"\"\n",
    "    enunciados_ejercicios = []\n",
    "\n",
    "    # Leer el notebook\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_tmp = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook_tmp.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                cell_content = cell['source'].strip()\n",
    "                \n",
    "                if cell_content.startswith('Contexto'):\n",
    "                    # Asignar el contenido de la celda a la variable contexto_examen\n",
    "                    contexto_examen = cell_content\n",
    "                \n",
    "                elif cell_content.startswith('Ejercicio'):\n",
    "                    # Añadir el contenido de la celda a la lista enunciados_ejercicios\n",
    "                    enunciados_ejercicios.append(cell_content)\n",
    "    \n",
    "    return {\n",
    "        'contexto_examen': contexto_examen,\n",
    "        'enunciados_ejercicios': enunciados_ejercicios\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesa_respuesta_alumno(file_path):\n",
    "    \"\"\"\n",
    "    Procesa un notebook Jupyter para extraer el código de solución para cada ejercicio y el nombre del alumno.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Ruta del archivo del notebook de solución.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un diccionario con las claves 'codigo_ejercicios' y 'alumno'.\n",
    "    \"\"\"\n",
    "    # Inicialización de variables\n",
    "    codigo_ejercicios = []\n",
    "\n",
    "    # Leer el notebook\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_tmp = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook_tmp.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # Añadir el código de cada celda de código a codigo_ejercicios\n",
    "                codigo_ejercicios.append(cell['source'])\n",
    "\n",
    "    # Separar código en ejercicios utilizando el marcador '# Solucion ejercicio'\n",
    "    codigo_ejercicios = re.split(r'# Solucion ejercicio \\d+', '\\n'.join(codigo_ejercicios))\n",
    "    codigo_ejercicios = [code.strip() for code in codigo_ejercicios if code.strip()]  # Limpiar y filtrar vacíos\n",
    "\n",
    "    # Extraer el nombre del alumno del nombre del archivo\n",
    "    nombre_archivo = os.path.basename(file_path)  # Obtener el nombre del archivo con extensión\n",
    "    alumno, _ = os.path.splitext(nombre_archivo)  # Separar el nombre y la extensión\n",
    "    \n",
    "    return {\n",
    "        'codigo_ejercicios': codigo_ejercicios,\n",
    "        'alumno': alumno\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Esta función no se utiliza en el script principal, pero puede ser útil para pruebas #########\n",
    "\n",
    "def evaluar_ejecucion_ejercicios(datos_ejercicios):\n",
    "    \"\"\"\n",
    "    Evalúa la ejecución de los ejercicios en el diccionario de datos extraídos del notebook.\n",
    "    \n",
    "    Parameters:\n",
    "        datos_ejercicios (dict): Diccionario con 'contexto_examen', 'enunciados_ejercicios' y 'codigo_ejercicios'.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un diccionario con los datos originales y el estado de ejecución y los mensajes de error para cada ejercicio.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    codigo_ejercicios = datos_ejercicios['codigo_ejercicios']\n",
    "    \n",
    "    for idx, code in enumerate(codigo_ejercicios):\n",
    "        resultado = {\n",
    "            'id_ejercicio': idx + 1,\n",
    "            'ejecucion': False,\n",
    "            'mensaje_error': '',\n",
    "            'calificacion': 0  # Calificación inicial, puede ser actualizada después\n",
    "        }\n",
    "        try:\n",
    "            exec(code)\n",
    "            resultado['ejecucion'] = True\n",
    "            resultado['calificacion'] = 1  # Calificación inicial de 1 para ejecuciones exitosas\n",
    "        except Exception as e:\n",
    "            resultado['mensaje_error'] = str(e)\n",
    "        \n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    # Incluir los resultados en la estructura de datos original\n",
    "    datos_ejercicios_con_resultados = {\n",
    "        'contexto_examen': datos_ejercicios['contexto_examen'],\n",
    "        'enunciados_ejercicios': datos_ejercicios['enunciados_ejercicios'],\n",
    "        'codigo_ejercicios': datos_ejercicios['codigo_ejercicios'],\n",
    "        'resultados': resultados\n",
    "    }\n",
    "    \n",
    "    return datos_ejercicios_con_resultados\n",
    "\n",
    "# Ejemplo de uso\n",
    "# datos_ejercicios = preprocesa_notebook(\"ruta/a/tu/archivo.ipynb\")\n",
    "# datos_evaluados = evaluar_ejecucion_ejercicios(datos_ejercicios)\n",
    "# print(datos_evaluados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import re\n",
    "\n",
    "# def evaluar_con_chatgpt(contexto, codigo, descripcion, prompt_template):\n",
    "#     \"\"\"\n",
    "#     Evalúa el código de un ejercicio utilizando el modelo GPT-4 de OpenAI.\n",
    "\n",
    "#     Parameters:\n",
    "#         codigo (str): El código del ejercicio a evaluar.\n",
    "#         descripcion (str): Una descripción del ejercicio.\n",
    "#         prompt_template (str): El template del prompt con marcadores para la descripción y el código.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Un diccionario con la puntuación y comentario del ejercicio.\n",
    "#     \"\"\"\n",
    "#     # Insertar los valores en el prompt\n",
    "#     prompt = prompt_template.format(contexto=contexto, descripcion=descripcion, codigo=codigo)\n",
    "\n",
    "#     cliente = openai.OpenAI()  # Asegúrate de que este es el cliente correcto\n",
    "#     try:\n",
    "#         response = cliente.ChatCompletion.create(\n",
    "#             model=\"gpt-4o-mini\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a programming teaching assistant evaluating student code.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ]\n",
    "#         )\n",
    "#     except openai.error.OpenAIError as e:\n",
    "#         print(f\"Error en la llamada a la API de OpenAI: {e}\")\n",
    "#         return None\n",
    "\n",
    "#     # Verificar que la respuesta tiene el formato esperado\n",
    "#     try:\n",
    "#         evaluacion = response['choices'][0]['message']['content']\n",
    "#         # Extraer puntuaciones y comentarios\n",
    "#         puntuaciones_pattern = re.search(r'A\\.\\s\\*\\*Puntuaciones\\*\\*:\\s(\\[.*?\\])', evaluacion)\n",
    "#         comentarios_pattern = re.search(r'B\\.\\s\\*\\*Comentarios\\*\\*:\\s(.*?)C\\.\\s\\*\\*Comentario General\\*\\*:', evaluacion, re.DOTALL)\n",
    "        \n",
    "#         if puntuaciones_pattern:\n",
    "#             puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "#         else:\n",
    "#             puntuaciones = []\n",
    "\n",
    "#         if comentarios_pattern:\n",
    "#             comentarios_text = comentarios_pattern.group(1).strip()\n",
    "#             comentarios = re.findall(r'\"\\s*(.*?)\\s*\"', comentarios_text, re.DOTALL)\n",
    "#         else:\n",
    "#             comentarios = []\n",
    "\n",
    "#         # Verificar que las listas de puntuaciones y comentarios tienen la misma longitud\n",
    "#         if len(puntuaciones) != len(comentarios):\n",
    "#             print('FUNCIÓN EVALUAR CON CHATGPT')\t\n",
    "#             print(f\"Error: La longitud de las puntuaciones y los comentarios no coincide.\")\n",
    "#             print(f\"Puntuaciones: {puntuaciones}\")\n",
    "#             print(f\"Comentarios: {comentarios}\")\n",
    "#             print('\\n\\n')\n",
    "#             return None\n",
    "\n",
    "#         comentario_general_pattern = re.search(r'C\\.\\s\\*\\*Comentario General\\*\\*:\\s(.*)', evaluacion, re.DOTALL)\n",
    "#         if comentario_general_pattern:\n",
    "#             comentario_general = comentario_general_pattern.group(1).strip()\n",
    "#         else:\n",
    "#             comentario_general = \"\"\n",
    "\n",
    "#         return {\n",
    "#             'puntuaciones': puntuaciones,\n",
    "#             'comentarios': comentarios,\n",
    "#             'comentario_general': comentario_general\n",
    "#         }\n",
    "\n",
    "#     except (KeyError, IndexError) as e:\n",
    "#         print(f\"Error al procesar la respuesta de la API: {e}\")\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_con_chatgpt(contexto, codigo, descripcion, prompt_template, dir_log):\n",
    "    \"\"\"\n",
    "    Evalúa el código de un ejercicio utilizando el modelo GPT-4 de OpenAI.\n",
    "\n",
    "    Parameters:\n",
    "        contexto (str): El contexto del examen.\n",
    "        codigo (str): El código del ejercicio a evaluar.\n",
    "        descripcion (str): Una descripción del ejercicio.\n",
    "        prompt_template (str): El template del prompt con marcadores para la descripción y el código.\n",
    "        dir_log (str): El directorio donde se guardará el archivo de log.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con la puntuación y comentario del ejercicio.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    log_file = os.path.join(dir_log, 'resultados.log')\n",
    "    if not os.path.exists(dir_log):\n",
    "        os.makedirs(dir_log)\n",
    "    logging.basicConfig(filename=log_file, level=logging.WARNING, \n",
    "                        format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "    # Insertar los valores en el prompt\n",
    "    prompt = prompt_template.format(contexto=contexto, descripcion=descripcion, codigo=codigo)\n",
    "\n",
    "    try:\n",
    "        cliente = OpenAI()\n",
    "        response = cliente.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a programming teaching assistant evaluating student code.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "    except RateLimitError as e:\n",
    "        error_msg = f\"Rate limit error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except OpenAIError as e:\n",
    "        error_msg = f\"OpenAI API error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except Exception as e:\n",
    "        error_msg = f\"General error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    # Extraer la respuesta de ChatGPT\n",
    "    try:\n",
    "        evaluacion = response.choices[0].message.content\n",
    "    except (KeyError, IndexError) as e:\n",
    "        error_msg = f\"Error al procesar la respuesta de la API: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    return evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluar_con_chatgpt(contexto, codigo, descripcion, prompt_template):\n",
    "#     \"\"\"\n",
    "#     Evalúa el código de un ejercicio utilizando el modelo GPT-4 de OpenAI.\n",
    "\n",
    "#     Parameters:\n",
    "#         codigo (str): El código del ejercicio a evaluar.\n",
    "#         descripcion (str): Una descripción del ejercicio.\n",
    "#         prompt_template (str): El template del prompt con marcadores para la descripción y el código.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Un diccionario con la puntuación y comentario del ejercicio.\n",
    "#     \"\"\"\n",
    "#     # Insertar los valores en el prompt\n",
    "#     prompt = prompt_template.format(contexto=contexto, descripcion=descripcion, codigo=codigo)\n",
    "\n",
    "#     cliente = OpenAI()\n",
    "#     response = cliente.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a programming teaching assistant evaluating student code.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Extraer la respuesta de ChatGPT\n",
    "#     evaluacion = response.choices[0].message.content\n",
    "#     return evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_ejercicios(diccionario_enunciados, diccionario_resultados, dir_log, prompt_file='prompt.txt'):\n",
    "    \"\"\"\n",
    "    Evalúa los ejercicios utilizando GPT-4 y devuelve las notas y comentarios para cada ejercicio.\n",
    "    \n",
    "    Parameters:\n",
    "        diccionario_enunciados (dict): Diccionario con los enunciados de los ejercicios preprocesados.\n",
    "        diccionario_resultados (dict): Diccionario con los datos del notebook preprocesado (código de los ejercicios).\n",
    "        prompt_file (str): Ruta del archivo de texto que contiene el prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con las evaluaciones de cada ejercicio.\n",
    "    \"\"\"\n",
    "    # Leer el prompt desde el archivo y cerrar el archivo automáticamente\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as file:\n",
    "        prompt_template = file.read()\n",
    "        \n",
    "    evaluaciones = {}\n",
    "    \n",
    "    # Iterar sobre los enunciados y códigos de los ejercicios\n",
    "    for i, (enunciado, codigo) in enumerate(zip(diccionario_enunciados['enunciados_ejercicios'], diccionario_resultados['codigo_ejercicios']), start=1):\n",
    "        # Llamar a la función que evalúa con ChatGPT pasando el prompt template\n",
    "        resultado = evaluar_con_chatgpt(diccionario_enunciados['contexto_examen'], codigo, enunciado, prompt_template, dir_log)\n",
    "        evaluaciones[f'Ejercicio {i}'] = resultado\n",
    "    \n",
    "    return evaluaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extraer_resultados(resultado):\n",
    "#     \"\"\"\n",
    "#     Extrae las puntuaciones, comentarios y comentarios generales de un diccionario de resultados.\n",
    "\n",
    "#     Parámetros:\n",
    "#     resultado (dict): Diccionario que contiene los resultados en forma de texto.\n",
    "\n",
    "#     Retorna:\n",
    "#     dict: Un diccionario con las claves originales y sus correspondientes listas de puntuaciones, comentarios y comentarios generales.\n",
    "#     \"\"\"\n",
    "#     # Inicializar el diccionario para almacenar los resultados\n",
    "#     res = {}\n",
    "    \n",
    "#     # Recorrer cada elemento del diccionario 'resultado'\n",
    "#     for key, value in resultado.items():\n",
    "#         try:\n",
    "#             # Buscar el patrón para las puntuaciones usando una expresión regular\n",
    "#             puntuaciones_pattern = re.search(r'A\\.\\s\\*\\*Puntuaciones\\*\\*:\\s(\\[.*?\\])', value)\n",
    "#             # Buscar el patrón para los comentarios usando una expresión regular\n",
    "#             comentarios_pattern = re.search(r'B\\.\\s\\*\\*Comentarios\\*\\*:\\s(.*?)C\\.\\s\\*\\*Comentario General\\*\\*:', value, re.DOTALL)\n",
    "#             # Buscar el patrón para el comentario general usando una expresión regular\n",
    "#             comentario_general_pattern = re.search(r'C\\.\\s\\*\\*Comentario General\\*\\*:\\s(.*)', value, re.DOTALL)\n",
    "            \n",
    "#             # Si se encuentra el patrón de puntuaciones, evalúa la cadena como una lista\n",
    "#             if puntuaciones_pattern:\n",
    "#                 puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "#             else:\n",
    "#                 puntuaciones = []\n",
    "\n",
    "#             # Si se encuentra el patrón de comentarios, evalúa la cadena como una lista\n",
    "#             if comentarios_pattern:\n",
    "#                 comentarios_text = comentarios_pattern.group(1).strip()\n",
    "#                 # Encuentra todos los comentarios dentro del texto de comentarios\n",
    "#                 comentarios = re.findall(r'\"\\s*(.*?)\\s*\"', comentarios_text, re.DOTALL)\n",
    "#             else:\n",
    "#                 comentarios = []\n",
    "\n",
    "#             # Si se encuentra el patrón de comentario general, evalúa la cadena como una lista\n",
    "#             if comentario_general_pattern:\n",
    "#                 comentario_general = eval(comentario_general_pattern.group(1).strip())\n",
    "#             else:\n",
    "#                 comentario_general = []\n",
    "\n",
    "#             # Verificar que las listas de puntuaciones y comentarios tienen la misma longitud\n",
    "#             if len(puntuaciones) != len(comentarios):\n",
    "#                 print('\\n\\n')\n",
    "#                 print('FUNCIÓN EXTRAER RESULTADOS')\n",
    "#                 print('\\n')\n",
    "#                 print(f\"Error en los resultados para '{key}':\")\n",
    "#                 print('\\n')\n",
    "#                 print(f\"Puntuaciones: {puntuaciones}\")\n",
    "#                 print('\\n')\n",
    "#                 print(f\"Comentarios: {comentarios}\")\n",
    "#                 print('\\n')\n",
    "#                 raise ValueError(f\"La longitud de las puntuaciones ({len(puntuaciones)}) y los comentarios ({len(comentarios)}) no coinciden para '{key}'.\")\n",
    "\n",
    "#             # Añadir el resultado al diccionario 'res' para la clave actual\n",
    "#             res[key] = {\n",
    "#                 'puntuaciones': puntuaciones,\n",
    "#                 'comentarios': comentarios,\n",
    "#                 'comentario_general': comentario_general\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error procesando los resultados para '{key}': {e}\")\n",
    "\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_resultados(resultado, nombre_alumno, criterios, dir_log):\n",
    "    \"\"\"\n",
    "    Extrae las puntuaciones, comentarios y comentarios generales de un diccionario de resultados.\n",
    "\n",
    "    Parámetros:\n",
    "    resultado (dict): Diccionario que contiene los resultados en forma de texto.\n",
    "    nombre_alumno (str): Nombre del alumno.\n",
    "    ejercicio (str): Nombre del ejercicio.\n",
    "    criterios (list): Lista de criterios de evaluación.\n",
    "\n",
    "    Retorna:\n",
    "    dict: Un diccionario con las claves originales y sus correspondientes listas de puntuaciones, comentarios y comentarios generales.\n",
    "    \"\"\"\n",
    "    # Configurar el logger\n",
    "    log_file = os.path.join(dir_log, 'resultados.log')\n",
    "    if not os.path.exists(dir_log):\n",
    "        os.makedirs(dir_log)\n",
    "    logging.basicConfig(filename=log_file, level=logging.WARNING, \n",
    "                        format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "    # Inicializar el diccionario para almacenar los resultados\n",
    "    res = {}\n",
    "\n",
    "    # Recorrer cada elemento del diccionario 'resultado'\n",
    "    for key, value in resultado.items():\n",
    "        try:\n",
    "            # Buscar el patrón para las puntuaciones usando una expresión regular\n",
    "            puntuaciones_pattern = re.search(r'A\\.\\s\\*\\*Puntuaciones\\*\\*:\\s(\\[.*?\\])', value)\n",
    "            # Buscar el patrón para los comentarios usando una expresión regular\n",
    "            comentarios_pattern = re.search(r'B\\.\\s\\*\\*Comentarios\\*\\*:\\s(.*?)C\\.\\s\\*\\*Comentario General\\*\\*:', value, re.DOTALL)\n",
    "            # Buscar el patrón para el comentario general usando una expresión regular\n",
    "            comentario_general_pattern = re.search(r'C\\.\\s\\*\\*Comentario General\\*\\*:\\s(.*)', value, re.DOTALL)\n",
    "            \n",
    "            # Si se encuentra el patrón de puntuaciones, evalúa la cadena como una lista\n",
    "            if puntuaciones_pattern:\n",
    "                puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "            else:\n",
    "                puntuaciones = []\n",
    "\n",
    "            # Si se encuentra el patrón de comentarios, evalúa la cadena como una lista\n",
    "            if comentarios_pattern:\n",
    "                comentarios_text = comentarios_pattern.group(1).strip()\n",
    "                # Encuentra todos los comentarios dentro del texto de comentarios\n",
    "                comentarios = re.findall(r'\"\\s*(.*?)\\s*\"', comentarios_text, re.DOTALL)\n",
    "            else:\n",
    "                comentarios = []\n",
    "\n",
    "            # Si se encuentra el patrón de comentario general, evalúa la cadena como una lista\n",
    "            if comentario_general_pattern:\n",
    "                comentario_general = eval(comentario_general_pattern.group(1).strip())\n",
    "            else:\n",
    "                comentario_general = []\n",
    "\n",
    "            # Verificar que las listas de criterios y puntuaciones tienen la misma longitud\n",
    "            if len(criterios) != len(puntuaciones):\n",
    "                error_msg = f\"Error: La longitud de los criterios ({len(criterios)}) y las puntuaciones ({len(puntuaciones)}) no coinciden para '{nombre_alumno}' en el ejercicio '{key}'.\"\n",
    "                logging.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "            # Verificar que las listas de puntuaciones y comentarios tienen la misma longitud\n",
    "            if len(puntuaciones) != len(comentarios):\n",
    "                warning_msg = f\"Warning: La longitud de las puntuaciones y los comentarios no coincide para '{nombre_alumno}' en el ejercicio '{key}'.\"\n",
    "                logging.warning(warning_msg)\n",
    "                \n",
    "                if len(comentarios) < len(puntuaciones):\n",
    "                    # Añadir \"Sin comentarios\" hasta que las listas tengan la misma longitud\n",
    "                    while len(comentarios) < len(puntuaciones):\n",
    "                        comentarios.append(\"Sin comentarios\")\n",
    "                    warning_msg += f\" Se añadieron 'Sin comentarios' para equilibrar las listas.\"\n",
    "                elif len(comentarios) > len(puntuaciones):\n",
    "                    # Acortar la lista de comentarios\n",
    "                    comentarios = comentarios[:len(puntuaciones)]\n",
    "                    warning_msg += f\" Se acortaron los comentarios para equilibrar las listas.\"\n",
    "\n",
    "                logging.warning(warning_msg)\n",
    "\n",
    "            # Añadir el resultado al diccionario 'res' para la clave actual\n",
    "            res[key] = {\n",
    "                'puntuaciones': puntuaciones,\n",
    "                'comentarios': comentarios,\n",
    "                'comentario_general': comentario_general\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error procesando los resultados para '{nombre_alumno}' en el ejercicio '{ejercicio}': {e}\"\n",
    "            logging.error(error_msg)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extraer_resultados(resultado):\n",
    "#     \"\"\"\n",
    "#     Extrae las puntuaciones, comentarios y comentarios generales de un diccionario de resultados.\n",
    "\n",
    "#     Parámetros:\n",
    "#     resultado (dict): Diccionario que contiene los resultados en forma de texto.\n",
    "\n",
    "#     Retorna:\n",
    "#     dict: Un diccionario con las claves originales y sus correspondientes listas de puntuaciones, comentarios y comentarios generales.\n",
    "#     \"\"\"\n",
    "#     # Inicializar el diccionario para almacenar los resultados\n",
    "#     res = {}\n",
    "    \n",
    "#     # Recorrer cada elemento del diccionario 'resultado'\n",
    "#     for key, value in resultado.items():\n",
    "#         # Buscar el patrón para las puntuaciones usando una expresión regular\n",
    "#         puntuaciones_pattern = re.search(r'A\\.\\s\\*\\*Puntuaciones\\*\\*:\\s(\\[.*?\\])', value)\n",
    "#         # Buscar el patrón para los comentarios usando una expresión regular\n",
    "#         comentarios_pattern = re.search(r'B\\.\\s\\*\\*Comentarios\\*\\*:\\s(.*?)C\\.\\s\\*\\*Comentario General\\*\\*:', value, re.DOTALL)\n",
    "#         # Buscar el patrón para el comentario general usando una expresión regular\n",
    "#         comentario_general_pattern = re.search(r'C\\.\\s\\*\\*Comentario General\\*\\*:\\s(.*)', value, re.DOTALL)\n",
    "        \n",
    "#         # Si se encuentra el patrón de puntuaciones, evalúa la cadena como una lista\n",
    "#         if puntuaciones_pattern:\n",
    "#             puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "#         else:\n",
    "#             puntuaciones = []\n",
    "\n",
    "#         # Si se encuentra el patrón de comentarios, evalúa la cadena como una lista\n",
    "#         if comentarios_pattern:\n",
    "#             comentarios_text = comentarios_pattern.group(1).strip()\n",
    "#             # Encuentra todos los comentarios dentro del texto de comentarios\n",
    "#             comentarios = re.findall(r'\"\\s*(.*?)\\s*\"', comentarios_text, re.DOTALL)\n",
    "#         else:\n",
    "#             comentarios = []\n",
    "\n",
    "#         # Si se encuentra el patrón de comentario general, evalúa la cadena como una lista\n",
    "#         if comentario_general_pattern:\n",
    "#             comentario_general = eval(comentario_general_pattern.group(1).strip())\n",
    "#         else:\n",
    "#             comentario_general = []\n",
    "\n",
    "#         # Añadir el resultado al diccionario 'res' para la clave actual\n",
    "#         res[key] = {\n",
    "#             'puntuaciones': puntuaciones,\n",
    "#             'comentarios': comentarios,\n",
    "#             'comentario_general': comentario_general\n",
    "#         }\n",
    "\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(FPDF):\n",
    "    \"\"\"\n",
    "    Clase personalizada para crear reportes de evaluación de ejercicios en formato PDF.\n",
    "    \n",
    "    Atributos:\n",
    "    alumno (str): Nombre del alumno para el cual se está generando el reporte.\n",
    "\n",
    "    Métodos:\n",
    "    __init__(self, alumno): Inicializa la instancia de la clase PDF con el nombre del alumno.\n",
    "    header(self): Añade un encabezado a cada página del PDF con el nombre del alumno.\n",
    "    footer(self): Añade un pie de página a cada página del PDF con el número de página.\n",
    "    add_context(self, contexto_examen): Añade el contexto del examen al PDF.\n",
    "    add_enunciados(self, enunciados): Añade los enunciados de los ejercicios al PDF.\n",
    "    add_evaluacion(self, alumno, ejercicios, criterios): Añade la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alumno):\n",
    "        \"\"\"\n",
    "        Inicializa la instancia de la clase PDF con el nombre del alumno.\n",
    "        \n",
    "        Parámetros:\n",
    "        alumno (str): Nombre del alumno para el cual se está generando el reporte.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alumno = alumno\n",
    "    \n",
    "    def header(self):\n",
    "        \"\"\"\n",
    "        Añade un encabezado a cada página del PDF con el nombre del alumno.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, f'Reporte de Evaluación de Ejercicios de {self.alumno}', 0, 1, 'C')\n",
    "\n",
    "    def footer(self):\n",
    "        \"\"\"\n",
    "        Añade un pie de página a cada página del PDF con el número de página.\n",
    "        \"\"\"\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        self.cell(0, 10, f'Página {self.page_no()}', 0, 0, 'C')\n",
    "\n",
    "    def add_context(self, contexto_examen):\n",
    "        \"\"\"\n",
    "        Añade el contexto del examen al PDF.\n",
    "        \n",
    "        Parámetros:\n",
    "        contexto_examen (str): Contexto general del examen.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Contexto del Examen:', 0, 1)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, contexto_examen)\n",
    "        self.ln(10)\n",
    "\n",
    "    def add_enunciados(self, enunciados):\n",
    "        \"\"\"\n",
    "        Añade los enunciados de los ejercicios al PDF.\n",
    "        \n",
    "        Parámetros:\n",
    "        enunciados (list): Lista de enunciados de los ejercicios del examen.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Enunciados de los Ejercicios:', 0, 1)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        for i, enunciado in enumerate(enunciados, 1):\n",
    "            self.cell(0, 10, f'Ejercicio {i}:', 0, 1)\n",
    "            self.multi_cell(0, 10, enunciado)\n",
    "            self.ln(5)\n",
    "        self.ln(10)\n",
    "\n",
    "    def add_evaluacion(self, alumno, ejercicios, criterios):\n",
    "        \"\"\"\n",
    "        Añade la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "        \n",
    "        Parámetros:\n",
    "        alumno (str): Nombre del alumno.\n",
    "        ejercicios (dict): Diccionario que contiene las evaluaciones de los ejercicios del estudiante.\n",
    "        criterios (list): Lista de criterios de evaluación.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.cell(0, 10, f'Informe de {alumno}', 0, 1, 'C')\n",
    "        self.ln(10)\n",
    "   \n",
    "        for ejercicio, contenido in ejercicios.items():\n",
    "            # Título del ejercicio\n",
    "            self.set_font('Arial', 'B', 12)\n",
    "            self.cell(0, 10, ejercicio, 0, 1)\n",
    "            self.ln(5)\n",
    "            \n",
    "            # Puntuaciones y comentarios\n",
    "            for i, criterio in enumerate(criterios):\n",
    "                self.set_font('Arial', 'B', 12)\n",
    "                self.cell(0, 10, f'{criterio}:', 0, 1, 'L')\n",
    "                self.set_font('Arial', '', 12)\n",
    "                self.multi_cell(0, 10, f'Puntuación: {contenido[\"puntuaciones\"][i]}')\n",
    "                self.multi_cell(0, 10, f'Comentario: {contenido[\"comentarios\"][i]}')\n",
    "                self.ln(5)\n",
    "            \n",
    "            # Comentario general\n",
    "            self.set_font('Arial', 'B', 12)\n",
    "            self.cell(0, 10, 'Comentario General del Ejercicio:', 0, 1, 'L')\n",
    "            self.set_font('Arial', '', 12)\n",
    "            self.multi_cell(0, 10, contenido['comentario_general'][0])\n",
    "            self.ln(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el PDF\n",
    "def create_pdf(file_path, student_name, contexto_examen, enunciados, ejercicios, criterios):\n",
    "    \"\"\"\n",
    "    Crea un archivo PDF con la evaluación del estudiante.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): Ruta donde se guardará el archivo PDF.\n",
    "    student_name (str): Nombre del estudiante.\n",
    "    contexto_examen (str): Contexto general del examen.\n",
    "    enunciados (list): Lista de enunciados de los ejercicios del examen.\n",
    "    ejercicios (dict): Diccionario que contiene las evaluaciones de los ejercicios del estudiante. La estructura del diccionario es:\n",
    "        {\n",
    "            'Ejercicio 1': {\n",
    "                'puntuaciones': [listado de puntuaciones],\n",
    "                'comentarios': [listado de comentarios],\n",
    "                'comentario_general': [comentario general]\n",
    "            },\n",
    "            'Ejercicio 2': {...},\n",
    "            ...\n",
    "        }\n",
    "    criterios (list): Lista de criterios de evaluación.\n",
    "\n",
    "    Procedimiento:\n",
    "    1. Crea una instancia de la clase PDF con el nombre del estudiante.\n",
    "    2. Añade una página al PDF.\n",
    "    3. Agrega el contexto del examen al PDF.\n",
    "    4. Agrega los enunciados de los ejercicios al PDF.\n",
    "    5. Agrega la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "    6. Guarda el archivo PDF en la ruta especificada.\n",
    "\n",
    "    Retorna:\n",
    "    Ninguno\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = PDF(student_name)\n",
    "    pdf.add_page()\n",
    "    \n",
    "   \n",
    "    # Agregar contexto del examen\n",
    "    pdf.add_context(contexto_examen)\n",
    "    \n",
    "    # Agregar enunciados de los ejercicios (comentado porque no está en uso)\n",
    "    # pdf.add_enunciados(enunciados)\n",
    "    \n",
    "    # Agregar evaluación del estudiante\n",
    "    pdf.add_evaluacion(student_name, ejercicios, criterios)\n",
    "    \n",
    "    # Guardar el PDF con el nombre del estudiante\n",
    "    pdf.output(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_pdfs_para_estudiantes(examen_preprocesado, directorio_reports, resultados):\n",
    "    \"\"\"\n",
    "    Genera archivos PDF de evaluación para cada estudiante basado en los resultados del examen.\n",
    "\n",
    "    Parámetros:\n",
    "    examen_preprocesado (dict): Diccionario con los datos del examen preprocesado que incluye el contexto y los enunciados de los ejercicios.\n",
    "    directorio_reports (str): Ruta al directorio donde se guardarán los archivos PDF generados.\n",
    "    resultados (dict): Diccionario que contiene las evaluaciones de los estudiantes. La estructura del diccionario es:\n",
    "        {\n",
    "            'nombre_estudiante': {\n",
    "                'Ejercicio 1': {\n",
    "                    'puntuaciones': [listado de puntuaciones],\n",
    "                    'comentarios': [listado de comentarios],\n",
    "                    'comentario_general': [comentario general]\n",
    "                },\n",
    "                'Ejercicio 2': {...},\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "            'criterios': [listado de criterios]\n",
    "        }\n",
    "\n",
    "    Precondiciones:\n",
    "    - El archivo del examen debe existir en la ruta especificada.\n",
    "    - Las funciones `preprocesa_notebook` y `create_pdf` deben estar definidas previamente.\n",
    "\n",
    "    Procedimiento:\n",
    "    1. Crea el directorio para guardar los archivos PDF si no existe.\n",
    "    2. Para cada estudiante en los resultados, genera un archivo PDF con las evaluaciones y comentarios.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear directorio para guardar los PDFs\n",
    "    if not os.path.exists(directorio_reports):\n",
    "        os.makedirs(directorio_reports)\n",
    "    \n",
    "    # Generar el PDF para cada estudiante\n",
    "    for student_name, ejercicios in resultados.items():\n",
    "        if student_name != 'criterios':  # Ignorar la clave de criterios\n",
    "            file_path = os.path.join(directorio_reports, f'{student_name}.pdf')\n",
    "            try:\n",
    "                create_pdf(file_path, student_name, examen_preprocesado['contexto_examen'], examen_preprocesado['enunciados_ejercicios'], ejercicios, resultados['criterios'])\n",
    "            except IndexError as e:\n",
    "                print(f\"Error de índice al procesar {student_name}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_excel_resultados(resultados, filename='resultados_evaluacion.xlsx'):\n",
    "    filas = []\n",
    "\n",
    "    for alumno, ejercicios in resultados.items():\n",
    "        if alumno == 'criterios':  # Ignorar la clave de criterios\n",
    "            continue\n",
    "        fila = {'Alumno': alumno}\n",
    "        notas = []\n",
    "        for ejercicio, contenido in ejercicios.items():\n",
    "            media = np.mean(contenido['puntuaciones'])\n",
    "            fila[ejercicio] = media\n",
    "            notas.append(media)\n",
    "        \n",
    "        # Calcular la nota final como la media ponderada de las notas de los ejercicios\n",
    "        nota_final = np.mean(notas)\n",
    "        fila['NOTA FINAL'] = nota_final\n",
    "        \n",
    "        filas.append(fila)\n",
    "\n",
    "    df = pd.DataFrame(filas)\n",
    "    df.to_excel(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_notebook(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log):\n",
    "    \"\"\"\n",
    "    Procesa y evalúa un notebook de un alumno.\n",
    "    \n",
    "    Parameters:\n",
    "        fich (str): Nombre del archivo del notebook del alumno.\n",
    "        directorio_entregas (str): Ruta del directorio de entregas.\n",
    "        examen_procesado (dict): Datos preprocesados del examen.\n",
    "        prompt_file (str): Ruta del archivo de texto que contiene el prompt.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Nombre del alumno y resultado de la evaluación extraída.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        alumno, _ = os.path.splitext(fich)\n",
    "        nb_preprocesado = preprocesa_respuesta_alumno(os.path.join(directorio_entregas, fich))\n",
    "        res_eval_tmp = evaluar_ejercicios(examen_procesado, nb_preprocesado, dir_log, prompt_file=prompt_file)\n",
    "        res_extraido = extraer_resultados(res_eval_tmp, alumno, criterios, dir_log)\n",
    "        return alumno, res_extraido\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el notebook {fich}: {e}\", file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directorio_raiz, nombre_fich_examen):\n",
    "    \"\"\"\n",
    "    Función principal para evaluar los notebooks entregados por los estudiantes.\n",
    "    \n",
    "    Parameters:\n",
    "        directorio_raiz (str): Ruta del directorio raíz que contiene los subdirectorios y archivos necesarios.\n",
    "        nombre_fich_examen (str): Nombre del archivo del examen.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir las rutas basadas en el directorio raíz\n",
    "        directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "        dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "        prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "        directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "        directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "        fichero_res = 'resultados_evaluacion.xlsx'\n",
    "        fichero_examen = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "        # Preprocesar el examen\n",
    "        if not os.path.exists(fichero_examen):\n",
    "            raise FileNotFoundError(f\"El archivo de examen {fichero_examen} no existe.\")\n",
    "        examen_procesado = preprocesa_examen(fichero_examen)\n",
    "        \n",
    "        # Listar los notebooks entregados por los alumnos\n",
    "        if not os.path.exists(directorio_entregas):\n",
    "            raise FileNotFoundError(f\"El directorio de entregas {directorio_entregas} no existe.\")\n",
    "        alumnos, ficheros = listar_notebooks(directorio_entregas)\n",
    "        if not ficheros:\n",
    "            raise FileNotFoundError(\"No se encontraron notebooks de alumnos en el directorio de entregas.\")\n",
    "\n",
    "        # Extraer los criterios de evaluación del prompt\n",
    "        if not os.path.exists(prompt_file):\n",
    "            raise FileNotFoundError(f\"El archivo de prompt {prompt_file} no existe.\")\n",
    "        criterios = extrae_criterios(prompt_file)\n",
    "\n",
    "        # Inicializar el diccionario de resultados\n",
    "        resultados = {}\n",
    "        resultados['criterios'] = criterios\n",
    "        \n",
    "        \n",
    "        # #Prosecar los notebooks y evaluarlos uno por uno\n",
    "        # for fich in tqdm(ficheros, desc=\"Procesando notebooks\"):\n",
    "        #     alumno, resultados_alumnos = evaluar_notebook(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Procesar y evaluar cada notebook en paralelo\n",
    "        resultados_alumnos = Parallel(n_jobs=-1)(\n",
    "            delayed(evaluar_notebook)(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log) \n",
    "            for fich in tqdm(ficheros, desc=\"Procesando notebooks\")\n",
    "        )\n",
    "\n",
    "        # Filtrar resultados exitosos y agregar al diccionario de resultados\n",
    "        for result in resultados_alumnos:\n",
    "            if result is not None:\n",
    "                alumno, res_extraido = result\n",
    "                resultados[alumno] = res_extraido\n",
    "    \n",
    "        # Generar los informes en PDF para cada estudiante\n",
    "        generar_pdfs_para_estudiantes(examen_procesado, directorio_reports, resultados)\n",
    "        \n",
    "         # Generar el archivo Excel con los resultados de la evaluación\n",
    "        generar_excel_resultados(resultados, filename=os.path.join(directorio_reports, fichero_res))\n",
    "        \n",
    "        print(\"Proceso completado con éxito.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en el proceso: {e}\", file=sys.stderr)\n",
    "        \n",
    "\n",
    "# directorio_raiz = \"/workspace\"\n",
    "# main(directorio_raiz, \"examen.ipynb\")\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando notebooks: 100%|██████████| 3/3 [00:00<00:00, 4011.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "directorio_raiz = \"/workspace\"\n",
    "main(directorio_raiz, \"examen.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________\n",
    "\n",
    "### Pruebas para comparar los nuevos ficheros de prompts para ver si siguen la estrucrura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_prompt(file_path):\n",
    "    # Leer el contenido del archivo\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    # Definimos las secciones y su estructura requerida\n",
    "    seccion_1 = r\"\\*\\*Contexto Examen\\*\\*\\n\\{contexto\\}\\n\\n\\*\\*Descripción del Ejercicio:\\*\\*\\n\\{descripcion\\}\\n\\n\\*\\*Código del estudiante:\\*\\*\\n\\{codigo\\}\"\n",
    "    seccion_2 = r\"Instrucciones Generales para la Evaluación:\\n\\nEl modelo debe evaluar cada ejercicio utilizando los criterios proporcionados.\\nCada criterio debe ser evaluado en una escala de 0 a 10, a menos que el código del ejercicio genere errores al ejecutarse.\\nSi el código genera errores, todos los criterios de ese ejercicio deben ser evaluados en una escala de 0 a 5.\\nEs importante tener en cuenta que no todos los ejercicios necesitarán usar listas o bucles; en esos casos, los criterios correspondientes deben ser omitidos de la evaluación.\"\n",
    "    criterio_patron = r\"@@\\w+@@\\nDescripción: .+\\nEjemplo: .+\"\n",
    "    seccion_3 = (\n",
    "        r\"Devuelve tres listas solo con la lista proporcionada en formato y nada más:\\n\\n\"\n",
    "        r\"A. \\*\\*Puntuaciones\\*\\*: Una lista de puntuaciones \\(solo los números, de 0 a 10\\) correspondiente a cada criterio en el orden en que se presentan. Si el código da algún error al ejecutarse, la nota máxima para cada criterio será 5.\\n   - Formato: \\[0, 10, 7, \\.\\.\\.\\]\\n\"\n",
    "        r\"B. \\*\\*Comentarios\\*\\*: Una lista de comentarios correspondiente a cada criterio en el mismo orden.\\n   - Formato: \\[\\\"Comentario para exactitud\\\", \\\"Comentario para claridad\\\", \\.\\.\\.\\]\\n\"\n",
    "        r\"C. \\*\\*Comentario General\\*\\*: Un comentario que ofrezca una idea global sobre el ejercicio teniendo en cuenta los criterios definidos. Especifica claramente si el código genera algún error al ejecutarse.\\n   - Formato: \\[\\\"Comentario general sobre el ejercicio\\\"\\]\"\n",
    "    )\n",
    "\n",
    "    # Validar secciones\n",
    "    if not re.search(seccion_1, prompt):\n",
    "        return \"La primera sección no sigue la estructura requerida.\"\n",
    "\n",
    "    if not re.search(seccion_2, prompt):\n",
    "        return \"La segunda sección no sigue la estructura requerida.\"\n",
    "\n",
    "    # Validar al menos un criterio en la segunda sección\n",
    "    criterios = re.findall(criterio_patron, prompt)\n",
    "    if not criterios:\n",
    "        return \"No se encontraron criterios en la segunda sección.\"\n",
    "\n",
    "    # Validar la tercera sección\n",
    "    if not re.search(seccion_3, prompt):\n",
    "        return \"La tercera sección no sigue la estructura requerida.\"\n",
    "\n",
    "    return \"El prompt es válido.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validar_prompt(\"/workspace/prompt funciona copy.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
