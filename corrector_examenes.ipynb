{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO: PUEDE HABER DIFERENCIAS ENTRE EL CÓDIGO DEL .PY Y DEL NOTEBOOK. EL BUENEO ES EL DEL .PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "import re\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "import openai\n",
    "from openai import RateLimitError, OpenAIError\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from openai import OpenAI\n",
    "from fpdf import FPDF\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave de API\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"API key is not set\")\n",
    "\n",
    "# Inicializar la API de OpenAI\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurar_logger(log_file):\n",
    "    # Comprobar si el logger ya está configurado\n",
    "    if not logging.getLogger().hasHandlers():\n",
    "        # Configurar el logger una vez\n",
    "        logging.basicConfig(filename=log_file, level=logging.ERROR, \n",
    "                            format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_notebooks(directory, log_file):\n",
    "    \"\"\"\n",
    "    Devuelve una lista con los nombres de todos los archivos con extensión '.ipynb' en un directorio dado.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): La ruta al directorio donde buscar los archivos.\n",
    "        log_file (str): La ruta al archivo de log donde se registrarán los mensajes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos listas, la primera con los nombres de los alumnos (nombres de archivos sin extensión),\n",
    "               y la segunda con los nombres de los archivos completos que cumplen los criterios.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    archivos = []\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.ipynb'):\n",
    "                archivos.append(filename)\n",
    "        \n",
    "        alumnos = [archivo.replace('.ipynb', '') for archivo in archivos]\n",
    "        \n",
    "        if not archivos:\n",
    "            logging.warning(f\"No se encontraron archivos .ipynb en el directorio {directory}'.\")\n",
    "            print(f\"No se encontraron archivos .ipynb en el directorio {directory}'.\")\n",
    "        \n",
    "        return alumnos, archivos\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al listar archivos en el directorio {directory}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_criterios(criterios_file, log_file):\n",
    "    \"\"\"\n",
    "    Carga los criterios desde un archivo de texto y los almacena en un diccionario.\n",
    "\n",
    "    Parameters:\n",
    "        criterios_file (str): Ruta del archivo de texto que contiene los criterios.\n",
    "        log_file (str): Ruta del archivo donde se guardarán los logs de errores.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario donde las claves son los nombres de los criterios y los valores\n",
    "              son diccionarios con 'descripcion' y 'ejemplo'.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la carga de criterios.\n",
    "    \"\"\"\n",
    "    criterios = {}\n",
    "\n",
    "    # Configurar el logging para registrar solo errores\n",
    "    configurar_logger(log_file)\n",
    "    \n",
    "    try:\n",
    "        # Intentar abrir el archivo de criterios y leer su contenido\n",
    "        with open(criterios_file, 'r', encoding='utf-8') as file:\n",
    "            contenido = file.read()\n",
    "    except FileNotFoundError:\n",
    "        # Registrar un error si el archivo no se encuentra\n",
    "        error_msg = f\"Error en cargar_criterios: El archivo {criterios_file} no se encontró.\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except IOError as e:\n",
    "        # Registrar cualquier otro error de entrada/salida\n",
    "        error_msg = f\"Error en cargar_criterios: Error al leer el archivo {criterios_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Dividir el contenido en secciones usando '@@' como delimitador\n",
    "        secciones = contenido.split('@@')\n",
    "\n",
    "        # Validar que el contenido esté correctamente estructurado\n",
    "        if len(secciones) < 3 or len(secciones) % 2 == 0:\n",
    "            raise ValueError(f\"Error en cargar_criterios: Formato incorrecto en el archivo {criterios_file}. Verifique que cada criterio tenga nombre y detalles asociados.\")\n",
    "\n",
    "        # Iterar sobre las secciones para extraer los criterios y sus detalles\n",
    "        for i in range(1, len(secciones), 2):\n",
    "            nombre_criterio = secciones[i].strip()  # Extraer y limpiar el nombre del criterio\n",
    "            detalles = secciones[i + 1].strip()  # Extraer y limpiar los detalles del criterio\n",
    "            partes = detalles.split(\"Ejemplo:\")  # Dividir detalles en descripción y ejemplo\n",
    "            descripcion = partes[0].replace(\"Descripción:\", \"\").strip()  # Limpiar la descripción\n",
    "            ejemplo = partes[1].strip() if len(partes) > 1 else \"\"  # Limpiar el ejemplo si existe\n",
    "\n",
    "            # Almacenar el criterio en el diccionario\n",
    "            criterios[nombre_criterio] = {\"descripcion\": descripcion, \"ejemplo\": ejemplo}\n",
    "\n",
    "        # Si no se encontraron criterios válidos, lanzar una excepción\n",
    "        if not criterios:\n",
    "            raise ValueError(f\"Error en cargar_criterios: No se encontraron criterios válidos en el archivo {criterios_file}.\")\n",
    "\n",
    "    except IndexError:\n",
    "        # Registrar un error si el formato del archivo es incorrecto\n",
    "        error_msg = \"Error en cargar_criterios: Formato incorrecto en el archivo de criterios. Verifique la estructura del archivo.\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except ValueError as e:\n",
    "        # Registrar cualquier error relacionado con los criterios\n",
    "        logging.error(f\"Error en cargar_criterios: {e}\")\n",
    "        raise RuntimeError(f\"Error en cargar_criterios: {e}\")\n",
    "    except Exception as e:\n",
    "        # Registrar cualquier otro error inesperado\n",
    "        error_msg = f\"Error inesperado en cargar_criterios al procesar el archivo {criterios_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    # Log de resultado final\n",
    "    logging.info(\"Criterios cargados correctamente. No se encontraron errores.\")\n",
    "    \n",
    "    # Devolver el diccionario de criterios cargado correctamente\n",
    "    return criterios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_estructura_examen(examen_file, log_file):\n",
    "    \"\"\"\n",
    "    Verifica que un notebook Jupyter sigue la estructura esperada para un examen.\n",
    "\n",
    "    Parameters:\n",
    "        examen_file (str): Ruta del archivo del notebook de examen.\n",
    "        log_file (str): Ruta del archivo donde se guardarán los logs de errores.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si se detecta un error crítico en la estructura.\n",
    "    \"\"\"\n",
    "    errores = []\n",
    "    contexto_detectado = False\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    codigo_encontrado = False\n",
    "\n",
    "    # Configurar el logging para registrar solo errores\n",
    "    configurar_logger(log_file)\n",
    "    \n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(examen_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error en verifica_estructura_examen: El archivo {examen_file} no se encontró.\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error en verifica_estructura_examen: Error al leer el archivo {examen_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                # Si se esperaba una solución y no se encontró código, generar un error\n",
    "                if se_espera_solucion and not codigo_encontrado:\n",
    "                    error_msg = f\"Error en verifica_estructura_examen: La Solución del Ejercicio {ejercicio_num} no contiene código.\"\n",
    "                    errores.append(error_msg)\n",
    "                    logging.error(error_msg)\n",
    "                se_espera_solucion = False\n",
    "                codigo_encontrado = False\n",
    "\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Verificar Contexto\n",
    "                if cell_content.startswith(\"## Contexto\"):\n",
    "                    if contexto_detectado:\n",
    "                        error_msg = \"Error en verifica_estructura_examen: Más de un '## Contexto' detectado.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logging.error(error_msg)\n",
    "                    contexto_detectado = True\n",
    "\n",
    "                # Verificar Ejercicios y Criterios\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "\n",
    "                    if \"Criterios:\" not in cell_content:\n",
    "                        error_msg = f\"Error en verifica_estructura_examen: El Ejercicio {ejercicio_num} no contiene la sección 'Criterios'.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logging.error(error_msg)\n",
    "                    else:\n",
    "                        criterios = cell_content.split(\"Criterios:\")[-1].strip()\n",
    "                        criterios_list = criterios.split(\",\")\n",
    "\n",
    "                        # Verificar si cada criterio está delimitado correctamente y no está vacío\n",
    "                        for criterio in criterios_list:\n",
    "                            criterio = criterio.strip()\n",
    "                            if not (criterio.startswith(\"@@\") and criterio.endswith(\"@@\")) or len(criterio) <= 4:\n",
    "                                error_msg = f\"Error en verifica_estructura_examen: Criterios mal formateados o vacíos en el Ejercicio {ejercicio_num}\"\n",
    "                                errores.append(error_msg)\n",
    "                                logging.error(error_msg)\n",
    "                                break\n",
    "\n",
    "            elif cell.cell_type == 'code':\n",
    "                if se_espera_solucion:\n",
    "                    cell_content = cell['source'].strip()\n",
    "\n",
    "                    # Eliminar saltos de línea y espacios adicionales para una comparación más robusta\n",
    "                    normalized_content = \" \".join(cell_content.split())\n",
    "\n",
    "                    # Verificar si la celda comienza con \"## Solución ejercicio\" o \"## Solucion ejercicio\"\n",
    "                    if normalized_content.startswith(\"## Solución ejercicio\") or normalized_content.startswith(\"## Solucion ejercicio\"):\n",
    "                        try:\n",
    "                            sol_num = int(normalized_content.split(\"## Solución ejercicio\" if \"## Solución ejercicio\" in normalized_content else \"## Solucion ejercicio\")[1].strip().split()[0])\n",
    "                            if sol_num != ejercicio_num:\n",
    "                                error_msg = f\"Error en verifica_estructura_examen: La Solución ejercicio {sol_num} no corresponde al Ejercicio {ejercicio_num}.\"\n",
    "                                errores.append(error_msg)\n",
    "                                logging.error(error_msg)\n",
    "                            codigo_encontrado = False  # Reiniciar el indicador de código encontrado\n",
    "                        except (ValueError, IndexError):\n",
    "                            error_msg = f\"Error en verifica_estructura_examen: Formato de número incorrecto en la Solución del Ejercicio {ejercicio_num}.\"\n",
    "                            errores.append(error_msg)\n",
    "                            logging.error(error_msg)\n",
    "                    else:\n",
    "                        # Si la celda no comienza con \"## Solución ejercicio\" ni \"## Solucion ejercicio\", pero se espera una solución\n",
    "                        error_msg = f\"Error en verifica_estructura_examen: El Ejercicio {ejercicio_num} tiene una celda de código que no comienza con '## Solución ejercicio {ejercicio_num}' o '## Solucion ejercicio {ejercicio_num}'.\"\n",
    "                        errores.append(error_msg)\n",
    "                        logging.error(error_msg)\n",
    "                        se_espera_solucion = False\n",
    "\n",
    "                    # Verificar si la celda contiene código más allá de comentarios o está vacía\n",
    "                    lines = cell_content.split(\"\\n\")\n",
    "                    for line in lines:\n",
    "                        stripped_line = line.strip()\n",
    "                        if stripped_line and not stripped_line.startswith(\"#\"):\n",
    "                            codigo_encontrado = True\n",
    "                            break\n",
    "\n",
    "        # Verificar si la última solución esperada fue proporcionada y si tenía código\n",
    "        if se_espera_solucion and not codigo_encontrado:\n",
    "            error_msg = f\"Error en verifica_estructura_examen: La Solución del Ejercicio {ejercicio_num} no contiene código.\"\n",
    "            errores.append(error_msg)\n",
    "            logging.error(error_msg)\n",
    "\n",
    "        # Verificaciones finales\n",
    "        if not contexto_detectado:\n",
    "            error_msg = \"Error en verifica_estructura_examen: No se detectó un '## Contexto' en el notebook.\"\n",
    "            errores.append(error_msg)\n",
    "            logging.error(error_msg)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en verifica_estructura_examen al procesar el archivo {examen_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    # Log de resultado final\n",
    "    if errores:\n",
    "        logging.error(\"EXAMEN Estructura Incorrecta: Se encontraron los siguientes errores:\")\n",
    "        for error in errores:\n",
    "            logging.error(error)\n",
    "        raise RuntimeError(\"EXAMEN Se encontraron errores críticos en la estructura del notebook.\")\n",
    "    else:\n",
    "        logging.error(\"EXAMEN Estructura Correcta: El notebook sigue la estructura esperada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPROBACIÓN DE LAS DOS FUNCIONES cargar_criterios y verifica_estructura_examen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterios cargados correctamente. Continuando con la verificación del examen...\n",
      "La estructura del examen es correcta. Script completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directorio_raiz = \"/workspace\"\n",
    "directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "\n",
    "log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "nombre_fich_examen = 'examen_pruebas.ipynb'\n",
    "criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "fichero_res = 'resultados_evaluacion.xlsx'\n",
    "examen_file = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "\n",
    "# Borrar el archivo de log si ya existe para empezar con un archivo limpio\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Configurar el logging para registrar errores tanto en consola como en el archivo de log\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "try:\n",
    "    # Llamar a la función cargar_criterios\n",
    "    criterios = cargar_criterios(criterios_file, log_file)\n",
    "    print(\"Criterios cargados correctamente. Continuando con la verificación del examen...\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico al cargar los criterios: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error\n",
    "\n",
    "try:\n",
    "    # Llamar a la función verifica_estructura_examen\n",
    "    verifica_estructura_examen(examen_file, log_file)\n",
    "    print(\"La estructura del examen es correcta. Script completado con éxito.\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico al verificar la estructura del examen: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrae_informacion_examen(examen_file, log_file):\n",
    "    \"\"\"\n",
    "    Extrae la información del examen desde un notebook Jupyter.\n",
    "\n",
    "    Parameters:\n",
    "        examen_file (str): Ruta del archivo del notebook de examen.\n",
    "        log_file (str): Ruta del archivo donde se guardarán los logs de errores.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un diccionario con el 'contexto' del examen y una lista de 'ejercicios', \n",
    "              donde cada ejercicio tiene 'enunciado', 'criterios', y 'solucion'.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la extracción de información.\n",
    "    \"\"\"\n",
    "    examen_info = {\n",
    "        \"contexto\": \"\",\n",
    "        \"ejercicios\": []\n",
    "    }\n",
    "\n",
    "    contexto_detectado = False\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    solucion_detectada = False\n",
    "    ejercicio_info = {}\n",
    "\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(examen_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "        logging.error(f\"Notebook {examen_file} leído correctamente.\")\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error en extrae_informacion_examen: El archivo {examen_file} no se encontró.\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error en extrae_informacion_examen al leer el archivo {examen_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Extraer Contexto\n",
    "                if cell_content.startswith(\"## Contexto\"):\n",
    "                    examen_info[\"contexto\"] = cell_content.split(\"## Contexto:\")[-1].strip()\n",
    "                    contexto_detectado = True\n",
    "                    logging.error(\"Contexto del examen detectado y extraído correctamente.\")\n",
    "\n",
    "                # Extraer Enunciado y Criterios del Ejercicio\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    if ejercicio_num > 0:\n",
    "                        examen_info[\"ejercicios\"].append(ejercicio_info)\n",
    "                        logging.error(f\"Ejercicio {ejercicio_num} procesado y añadido a la lista.\")\n",
    "\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "                    solucion_detectada = False\n",
    "                    ejercicio_info = {\n",
    "                        \"enunciado\": cell_content.split(\"## Ejercicio\")[1].split(\"Criterios:\")[0].strip(),\n",
    "                        \"criterios\": [],\n",
    "                        \"solucion\": []\n",
    "                    }\n",
    "                    criterios_text = cell_content.split(\"Criterios:\")[-1].strip()\n",
    "                    criterios_list = [criterio.strip() for criterio in criterios_text.split(\",\")]\n",
    "                    ejercicio_info[\"criterios\"] = criterios_list\n",
    "                    logging.error(f\"Ejercicio {ejercicio_num} detectado con enunciado y criterios extraídos.\")\n",
    "\n",
    "            elif cell.cell_type == 'code' and se_espera_solucion:\n",
    "                # Extraer solución de las celdas de código\n",
    "                ejercicio_info[\"solucion\"].append(cell['source'].strip())\n",
    "                solucion_detectada = True\n",
    "                logging.error(f\"Solución del Ejercicio {ejercicio_num} extraída y añadida.\")\n",
    "\n",
    "        # Agregar la información del último ejercicio si no ha sido añadido\n",
    "        if ejercicio_num > 0 and ejercicio_info:\n",
    "            examen_info[\"ejercicios\"].append(ejercicio_info)\n",
    "            logging.error(f\"Ejercicio {ejercicio_num} procesado y añadido a la lista.\")\n",
    "\n",
    "        if not contexto_detectado:\n",
    "            error_msg = \"Error en extrae_informacion_examen: No se detectó un '## Contexto' en el notebook.\"\n",
    "            logging.error(error_msg)\n",
    "            raise RuntimeError(error_msg)\n",
    "\n",
    "        logging.error(\"La información del examen ha sido extraída correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en extrae_informacion_examen al procesar el archivo {examen_file}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    return examen_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_respuestas_alumno(alumno_file, numero_ejercicios, log_file):\n",
    "    \"\"\"\n",
    "    Procesa un notebook de respuestas de un alumno para extraer las soluciones a los ejercicios.\n",
    "\n",
    "    Parameters:\n",
    "        alumno_file (str): Ruta del archivo del notebook de respuesta del alumno.\n",
    "        numero_ejercicios (int): Número esperado de ejercicios en el examen.\n",
    "        log_file (str): Ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con el 'nombre_alumno' y una lista de 'ejercicios', donde cada ejercicio\n",
    "              tiene su 'enunciado' y 'solucion' (o un valor indicando que no fue respondido).\n",
    "    \"\"\"\n",
    "    # Extraer el nombre del alumno desde el nombre del archivo\n",
    "    nombre_alumno = os.path.splitext(os.path.basename(alumno_file))[0]\n",
    "\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    # Iniciar el log indicando el inicio del procesamiento\n",
    "    logging.error(f\"Procesando respuestas del alumno {nombre_alumno}\")\n",
    "\n",
    "    respuestas_alumno = {\n",
    "        \"nombre_alumno\": nombre_alumno,\n",
    "        \"ejercicios\": []\n",
    "    }\n",
    "\n",
    "    ejercicio_num = 0\n",
    "    se_espera_solucion = False\n",
    "    ejercicio_info = {}\n",
    "\n",
    "    try:\n",
    "        # Leer el notebook\n",
    "        with open(alumno_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "        logging.error(f\"Notebook {alumno_file} leído correctamente para el alumno {nombre_alumno}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error en procesa_respuestas_alumno: Archivo no encontrado para el alumno {nombre_alumno}: {alumno_file}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": \"Archivo no encontrado.\"}\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error en procesa_respuestas_alumno al leer el notebook para el alumno {nombre_alumno}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": f\"Error al leer el notebook: {str(e)}\"}\n",
    "\n",
    "    try:\n",
    "        # Procesar las celdas del notebook\n",
    "        for cell in notebook.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                cell_content = cell['source'].strip()\n",
    "\n",
    "                # Identificar un nuevo ejercicio basado en el enunciado\n",
    "                if cell_content.startswith(\"## Ejercicio\"):\n",
    "                    # Si estábamos esperando una solución y no se añadió código, marcamos como no respondido\n",
    "                    if se_espera_solucion:\n",
    "                        if not ejercicio_info.get(\"solucion\") or all(\n",
    "                            not line.strip() or line.strip().startswith(\"#\") \n",
    "                            for solution in ejercicio_info[\"solucion\"] \n",
    "                            for line in solution.splitlines()):\n",
    "                            ejercicio_info[\"solucion\"] = \"No respondido\"\n",
    "                        respuestas_alumno[\"ejercicios\"].append(ejercicio_info)\n",
    "                        logging.error(f\"Ejercicio {ejercicio_num} procesado para el alumno {nombre_alumno}.\")\n",
    "\n",
    "                    ejercicio_num += 1\n",
    "                    se_espera_solucion = True\n",
    "                    ejercicio_info = {\n",
    "                        \"enunciado\": cell_content.split(\"## Ejercicio\")[1].strip(),\n",
    "                        \"solucion\": []\n",
    "                    }\n",
    "                    logging.error(f\"Ejercicio {ejercicio_num} detectado para el alumno {nombre_alumno}.\")\n",
    "\n",
    "            elif cell.cell_type == 'code' and se_espera_solucion:\n",
    "                # Procesar celdas de código para una solución\n",
    "                codigo = cell['source'].strip()\n",
    "                if codigo:\n",
    "                    ejercicio_info[\"solucion\"].append(codigo)\n",
    "                    logging.error(f\"Solución del Ejercicio {ejercicio_num} extraída para el alumno {nombre_alumno}.\")\n",
    "\n",
    "        # Agregar la información del último ejercicio si no ha sido añadido\n",
    "        if se_espera_solucion:\n",
    "            if not ejercicio_info.get(\"solucion\") or all(\n",
    "                not line.strip() or line.strip().startswith(\"#\") \n",
    "                for solution in ejercicio_info[\"solucion\"] \n",
    "                for line in solution.splitlines()):\n",
    "                ejercicio_info[\"solucion\"] = \"No respondido\"\n",
    "            respuestas_alumno[\"ejercicios\"].append(ejercicio_info)\n",
    "            logging.error(f\"Ejercicio {ejercicio_num} procesado para el alumno {nombre_alumno}.\")\n",
    "\n",
    "        # Verificar que el número de ejercicios y soluciones coincida con el número esperado\n",
    "        num_ejercicios = len(respuestas_alumno[\"ejercicios\"])\n",
    "        num_soluciones = sum(1 for ejercicio in respuestas_alumno[\"ejercicios\"] if ejercicio[\"solucion\"] != \"No respondido\")\n",
    "\n",
    "        if num_ejercicios != numero_ejercicios:\n",
    "            error_msg = f\"Error en procesa_respuestas_alumno: El número de ejercicios en el notebook ({num_ejercicios}) no coincide con el número esperado ({numero_ejercicios}) para el alumno {nombre_alumno}.\"\n",
    "            logging.error(error_msg)\n",
    "            return {\"error\": error_msg}\n",
    "\n",
    "        if num_soluciones != numero_ejercicios:\n",
    "            error_msg = f\"Error en procesa_respuestas_alumno: El número de soluciones en el notebook ({num_soluciones}) no coincide con el número esperado de ejercicios ({numero_ejercicios}) para el alumno {nombre_alumno}.\"\n",
    "            logging.error(error_msg)\n",
    "            return {\"error\": error_msg}\n",
    "\n",
    "        logging.error(f\"Respuestas del alumno {nombre_alumno} procesadas correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en procesa_respuestas_alumno al procesar el archivo {alumno_file} para el alumno {nombre_alumno}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": f\"Error inesperado: {str(e)}\"}\n",
    "\n",
    "    return respuestas_alumno\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprueba_ejecucion(respuestas_alumno, log_file):\n",
    "    \"\"\"\n",
    "    Comprueba si las soluciones de los ejercicios se ejecutan sin errores y añade el estado\n",
    "    y el mensaje de error (si corresponde) al diccionario original de respuestas del alumno.\n",
    "\n",
    "    Parameters:\n",
    "        respuestas_alumno (dict): Diccionario que contiene las respuestas del alumno y sus soluciones.\n",
    "        log_file (str): Ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "        \n",
    "    Returns:\n",
    "        dict: El mismo diccionario 'respuestas_alumno' con los campos 'estado' y 'mensaje_de_error'\n",
    "              añadidos a cada ejercicio.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la ejecución de la función.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    nombre_alumno = respuestas_alumno[\"nombre_alumno\"]\n",
    "\n",
    "    try:\n",
    "        for i, ejercicio in enumerate(respuestas_alumno[\"ejercicios\"], 1):\n",
    "            solucion = ejercicio[\"solucion\"]\n",
    "\n",
    "            if solucion == \"No respondido\":\n",
    "                ejercicio[\"estado\"] = \"No respondido\"\n",
    "                ejercicio[\"mensaje_de_error\"] = None\n",
    "            else:\n",
    "                # Redirigir la salida estándar a un objeto StringIO para capturarla\n",
    "                original_stdout = sys.stdout\n",
    "                sys.stdout = io.StringIO()\n",
    "\n",
    "                try:\n",
    "                    # Crear un diccionario para almacenar el entorno de ejecución\n",
    "                    entorno_ejecucion = {}\n",
    "                    # Concatenar todos los bloques de código en una sola cadena\n",
    "                    codigo_completo = \"\\n\".join(solucion)\n",
    "                    # Ejecutar el código completo en el entorno de ejecución\n",
    "                    exec(codigo_completo, entorno_ejecucion)\n",
    "                    ejercicio[\"estado\"] = \"Correcto\"\n",
    "                    ejercicio[\"mensaje_de_error\"] = None\n",
    "                except Exception as e:\n",
    "                    # Si hay un error en la ejecución, marcar el estado como \"Error\" y guardar el mensaje de error\n",
    "                    ejercicio[\"estado\"] = \"Error\"\n",
    "                    ejercicio[\"mensaje_de_error\"] = str(e)\n",
    "                finally:\n",
    "                    # Restaurar la salida estándar original\n",
    "                    sys.stdout = original_stdout\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en comprueba_ejecucion para el alumno {nombre_alumno}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    return respuestas_alumno\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPROBAR LAS FUNCIONES cargar_criterios, verifica_estructura_examen, extrae_informacion_examen, procesa_respuestas_alumno, comprueba_ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterios cargados correctamente. Continuando con la verificación del examen...\n",
      "La estructura del examen es correcta. Continuando con la extracción de información...\n",
      "Información del examen extraída correctamente. Continuando con el procesamiento de respuestas del alumno...\n",
      "Respuestas del alumno procesadas correctamente. Continuando con la comprobación de ejecución...\n",
      "Ejecución de las respuestas del alumno verificada correctamente.\n",
      "Resumen de la evaluación:\n",
      "Nombre del alumno: ubeda_fernando\n",
      "Ejercicio 1: Estado - Error, Mensaje de error - list assignment index out of range\n",
      "Ejercicio 2: Estado - Error, Mensaje de error - cannot access local variable 'maximo' where it is not associated with a value\n",
      "Ejercicio 3: Estado - Error, Mensaje de error - name 'valor_total' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Configurar las rutas de los archivos y directorios\n",
    "directorio_raiz = \"/workspace\"\n",
    "directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "\n",
    "log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "nombre_fich_examen = 'examen_pruebas.ipynb'\n",
    "criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "fichero_res = 'resultados_evaluacion.xlsx'\n",
    "examen_file = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "# Crear el directorio de logs si no existe\n",
    "if not os.path.exists(dir_log):\n",
    "    os.makedirs(dir_log)\n",
    "\n",
    "# Borrar el archivo de log si ya existe para empezar con un archivo limpio\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "\n",
    "# Configurar el logging para registrar errores tanto en consola como en el archivo de log\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "try:\n",
    "    # 1. Cargar criterios\n",
    "    criterios = cargar_criterios(criterios_file, log_file)\n",
    "    print(\"Criterios cargados correctamente. Continuando con la verificación del examen...\")\n",
    "\n",
    "    # 2. Verificar la estructura del examen\n",
    "    verifica_estructura_examen(examen_file, log_file)\n",
    "    print(\"La estructura del examen es correcta. Continuando con la extracción de información...\")\n",
    "\n",
    "    # 3. Extraer la información del examen\n",
    "    examen_info = extrae_informacion_examen(examen_file, log_file)\n",
    "    print(\"Información del examen extraída correctamente. Continuando con el procesamiento de respuestas del alumno...\")\n",
    "\n",
    "    # 4. Procesar respuestas de un alumno\n",
    "    nombre_fich_alumno = 'ubeda_fernando.ipynb'\n",
    "    alumno_file = os.path.join(directorio_entregas, nombre_fich_alumno)\n",
    "    respuestas_alumno = procesa_respuestas_alumno(alumno_file, len(examen_info['ejercicios']), log_file)\n",
    "    print(\"Respuestas del alumno procesadas correctamente. Continuando con la comprobación de ejecución...\")\n",
    "\n",
    "    # 5. Comprobar la ejecución de las respuestas del alumno\n",
    "    respuestas_evaluadas = comprueba_ejecucion(respuestas_alumno, log_file)\n",
    "    print(\"Ejecución de las respuestas del alumno verificada correctamente.\")\n",
    "\n",
    "    # 6. Imprimir un resumen del resultado\n",
    "    print(\"Resumen de la evaluación:\")\n",
    "    print(f\"Nombre del alumno: {respuestas_evaluadas['nombre_alumno']}\")\n",
    "    for i, ejercicio in enumerate(respuestas_evaluadas['ejercicios'], 1):\n",
    "        estado = ejercicio.get('estado', 'No definido')\n",
    "        mensaje_error = ejercicio.get('mensaje_de_error', 'Ninguno')\n",
    "        print(f\"Ejercicio {i}: Estado - {estado}, Mensaje de error - {mensaje_error}\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nombre_alumno': 'ubeda_fernando',\n",
       " 'ejercicios': [{'enunciado': '1: Datos de Ingresos y Gastos.\\n\\nDefine una lista llamada ingresos que contenga los ingresos mensuales de una empresa durante 6 meses. Por ejemplo: [5000, 5500, 6000, 6500, 7000, 7500].\\n\\nDefine una lista llamada gastos que contenga los gastos mensuales correspondientes a esos mismos 6 meses. Por ejemplo: [3000, 3200, 3500, 3700, 3800, 3900].\\n\\nCalcula el saldo mensual (ingresos menos gastos) y almacena los saldos en una lista llamada saldos.\\n\\nImprime la lista de saldos y el saldo total acumulado de los 6 meses.',\n",
       "   'solucion': ['## Solucion ejercicio 1\\n\\ningresos = [5000, 5500, 6000, 6500, 7000, 7500]\\ngastos = [3000, 3200, 3500, 3700, 3800, 3900]\\nsaldos = []\\n\\n# Escribe aqui tu codigo\\nsaldos[0] = ingresos[0] - gastos[0]\\nsaldos[1] = ingresos[1] - gastos[1]\\nsaldos[2] = ingresos[2] - gastos[2]\\nsaldos[3] = ingresos[3] - gastos[3]\\nsaldos[4] = ingresos[4] - gastos[4]\\n\\nprint(\"Saldos mensuales:\", saldos)\\nprint(\"Saldo total acumulado:\", sum(saldos))'],\n",
       "   'estado': 'Error',\n",
       "   'mensaje_de_error': 'list assignment index out of range'},\n",
       "  {'enunciado': \"2: Análisis de Ingresos\\n\\nUsa un diccionario llamado ingresos_dict para almacenar los ingresos mensuales, donde las claves son los nombres de los meses (por ejemplo, 'Enero', 'Febrero', etc.) y los valores son los ingresos correspondientes.\\n\\nEscribe una función llamada mes_con_mayor_ingreso() que devuelva el mes con el ingreso más alto.\\n\\nEscribe una función llamada ingreso_promedio() que calcule y devuelva el ingreso promedio mensual.\\n\\nImprime el mes con el ingreso más alto y el ingreso promedio.\",\n",
       "   'solucion': ['## Solucion ejercicio 2\\n# Funcion para calcular mes con mayores ingresos\\ndef mes_con_mayor_ingreso(dict):\\n\\n    for mes, ingreso in dict.items():\\n        if ingreso > maximo:\\n            maximo = ingreso\\n            retorno = mes\\n\\n    return retorno\\n\\n# Funcion para calcular promedio ingresos mensual\\ndef ingreso_promedio(dict):\\n    suma = 0\\n    for value in dict.values():\\n        suma += value\\n\\n    promedio = suma / len(dict)\\n    return promedio\\n\\n\\ningresos_dict = {\\n    \\'Enero\\': 5000,\\n    \\'Febrero\\': 5500,\\n    \\'Marzo\\': 6000,\\n    \\'Abril\\': 6500,\\n    \\'Mayo\\': 7000,\\n    \\'Junio\\': 7500\\n}\\n\\nprint(\"Mes con mayor ingreso:\", mes_con_mayor_ingreso(ingresos_dict))\\nprint(\"Ingreso promedio:\", ingreso_promedio(ingresos_dict))'],\n",
       "   'estado': 'Error',\n",
       "   'mensaje_de_error': \"cannot access local variable 'maximo' where it is not associated with a value\"},\n",
       "  {'enunciado': '3: Análisis de Inversión en Activos\\n\\nImagina que tienes una cartera de inversión con diferentes activos como acciones, bonos, bienes raíces y criptomonedas. Necesitas escribir un programa en Python que:\\n\\n- Utilice datos predefinidos para las inversiones en cada activo.\\n- Calcule el valor tota \\n- l de la inversión.\\n- Determine la proporción de la inversión en cada activo respecto al total.\\n- Verifique si alguna inversión supera un umbral específico y muestra un mensaje si es así.',\n",
       "   'solucion': ['# Solucion ejercicio 3\\n\\n# Umbral para advertencia de alta inversión en un solo activo\\numbral_inversion = 5000.0  # Puedes cambiar este valor según sea necesario\\n\\n# Activos y sus valores predefinidos\\nactivos = [\"Acciones\", \"Bonos\", \"Bienes Raíces\", \"Criptomonedas\"]\\ninversiones = [7000.0, 2000.0, 8000.0, 3000.0]  # Valores de inversión predefinidos para cada activo\\n\\n# Calcular el valor total de la inversión\\nfor valor in inversiones:\\n    valor_total += valor\\n\\n# Mostrar el valor total de la inversión con 2 decimales\\nprint(f\"Valor total de la inversión: ${valor_total:.2f}\")\\n\\n# Verificar si alguna inversión supera el umbral\\nprint(\"\\\\nRevisión de inversiones individuales:\")\\nif inversiones[0] > umbral_inversion:\\n    print(f\"La inversion en {activos[0]} es superior a tu umbral de {umbral_inversion:.2f}\")\\nif inversiones[1] > umbral_inversion:\\n    print(f\"La inversion en {activos[1]} es superior a tu umbral de {umbral_inversion:.2f}\")\\nif inversiones[2] > umbral_inversion:\\n    print(f\"La inversion en {activos[2]} es superior a tu umbral de {umbral_inversion:.2f}\")\\nif inversiones[3] > umbral_inversion:\\n    print(f\"La inversion en {activos[3]} es superior a tu umbral de {umbral_inversion:.2f}\")\\n\\n\\n# Información adicional: porcentaje de cada activo sobre el valor total de la inversión\\nprint(\"\\\\nProporción de inversión por activo:\")\\nporcentaje_0 = (inversiones[0]/umbral_inversion)*100\\nprint(f\"{activos[0]} tiene un {porcentaje_0}% de inversión\")\\nporcentaje_2 = (inversiones[1]/umbral_inversion)*100\\nprint(f\"{activos[1]} tiene un {porcentaje_2}% de inversión\")\\nporcentaje_2 = (inversiones[2]/umbral_inversion)*100\\nprint(f\"{activos[2]} tiene un {porcentaje_2}% de inversión\")\\nporcentaje_3 = (inversiones[3]/umbral_inversion)*100\\nprint(f\"{activos[3]} tiene un {porcentaje_3}% de inversión\")'],\n",
       "   'estado': 'Error',\n",
       "   'mensaje_de_error': \"name 'valor_total' is not defined\"}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas_alumno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_con_chatgpt(contexto, codigo, enunciado, criterios_texto, prompt_template, log_file):\n",
    "    \"\"\"\n",
    "    Evalúa el código de un ejercicio utilizando el modelo GPT-4 de OpenAI.\n",
    "\n",
    "    Parameters:\n",
    "        contexto (str): El contexto del examen.\n",
    "        codigo (str): El código del ejercicio a evaluar.\n",
    "        enunciado (str): El enunciado del ejercicio.\n",
    "        criterios_texto (str): Texto que contiene los criterios específicos para este ejercicio.\n",
    "        prompt_template (str): El template del prompt con marcadores para la descripción y el código.\n",
    "        log_file (str): La ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con la puntuación y comentario del ejercicio.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    # Insertar los valores en el prompt, incluyendo los criterios específicos para este ejercicio\n",
    "    prompt = prompt_template.format(contexto=contexto, enunciado=enunciado, codigo=codigo, criterios=criterios_texto)\n",
    "\n",
    "    try:\n",
    "        cliente = OpenAI()\n",
    "        response = cliente.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a programming teaching assistant evaluating student code.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "    except RateLimitError as e:\n",
    "        error_msg = f\"Rate limit error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except OpenAIError as e:\n",
    "        error_msg = f\"OpenAI API error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    except Exception as e:\n",
    "        error_msg = f\"General error: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    # Extraer la respuesta de ChatGPT\n",
    "    try:\n",
    "        evaluacion = response.choices[0].message.content\n",
    "    except (KeyError, IndexError) as e:\n",
    "        error_msg = f\"Error al procesar la respuesta de la API: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "\n",
    "    return evaluacion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_ejercicios(info_examen, resultados_alumno, prompt_file, criterios_file, log_file):\n",
    "    \"\"\"\n",
    "    Evalúa los ejercicios utilizando GPT-4 y devuelve las notas y comentarios para cada ejercicio.\n",
    "    \n",
    "    Parameters:\n",
    "        info_examen (dict): Diccionario con el contexto del examen y los enunciados de los ejercicios, obtenido de `extrae_informacion_examen`.\n",
    "        resultados_alumno (dict): Diccionario con las soluciones y estado de ejecución de los ejercicios, obtenido de `comprueba_ejecucion`.\n",
    "        prompt_file (str): Ruta del archivo de texto que contiene el prompt.\n",
    "        criterios_file (str): Ruta del archivo de texto que contiene los criterios.\n",
    "        log_file (str): Ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con las evaluaciones de cada ejercicio.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Si ocurre un error crítico durante la evaluación.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    evaluaciones = {}\n",
    "\n",
    "    try:\n",
    "        # Leer el prompt desde el archivo\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as file:\n",
    "            prompt_template = file.read()\n",
    "\n",
    "        # Cargar los criterios desde el archivo\n",
    "        criterios_info = cargar_criterios(criterios_file, log_file)\n",
    "\n",
    "        # Iterar sobre los ejercicios y sus resultados\n",
    "        for i, (ejercicio_info, resultado_alumno) in enumerate(zip(info_examen['ejercicios'], resultados_alumno['ejercicios']), start=1):\n",
    "            enunciado = ejercicio_info['enunciado']\n",
    "            codigo = resultado_alumno['solucion']\n",
    "            criterios_nombres = ejercicio_info.get('criterios', [])\n",
    "\n",
    "            # Generar el texto de los criterios para el prompt\n",
    "            criterios_texto = \"\\n\\n\".join([\n",
    "                f\"**{criterio.strip('@')}**\\nDescripción: {criterios_info[criterio.strip('@')]['descripcion']}\\nEjemplo: {criterios_info[criterio.strip('@')]['ejemplo']}\"\n",
    "                for criterio in criterios_nombres if criterio.strip('@') in criterios_info\n",
    "            ])\n",
    "\n",
    "            # Redirigir la salida estándar a un objeto StringIO para capturarla\n",
    "            original_stdout = sys.stdout\n",
    "            sys.stdout = io.StringIO()\n",
    "\n",
    "            try:\n",
    "                # Si el ejercicio no fue respondido, se añade una evaluación específica\n",
    "                if resultado_alumno['estado'] == \"No respondido\":\n",
    "                    evaluaciones[f'Ejercicio {i}'] = (\n",
    "                        \"**Puntuaciones**: [0]\\n\"\n",
    "                        \"**Comentarios**: [\\\"El ejercicio no fue respondido.\\\"]\\n\"\n",
    "                        \"**Comentario General**: [\\\"El alumno no proporcionó ninguna solución para este ejercicio.\\\"]\"\n",
    "                    )\n",
    "                elif resultado_alumno['estado'] == \"Error\":\n",
    "                    evaluaciones[f'Ejercicio {i}'] = (\n",
    "                        \"**Puntuaciones**: [0]\\n\"\n",
    "                        f\"**Comentarios**: [\\\"Error en la ejecución: {resultado_alumno['mensaje_de_error']}\\\"]\\n\"\n",
    "                        \"**Comentario General**: [\\\"El código presentado contiene errores que impiden su correcta ejecución.\\\"]\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Llamar a la función que evalúa con ChatGPT pasando el prompt template y los criterios específicos\n",
    "                    resultado = evaluar_con_chatgpt(\n",
    "                        info_examen['contexto'], codigo, enunciado, criterios_texto, prompt_template, log_file\n",
    "                    )\n",
    "                    evaluaciones[f'Ejercicio {i}'] = resultado\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error al evaluar el ejercicio {i} para el alumno: {e}\"\n",
    "                logging.error(error_msg)\n",
    "                evaluaciones[f'Ejercicio {i}'] = (\n",
    "                    \"**Puntuaciones**: [0]\\n\"\n",
    "                    f\"**Comentarios**: [\\\"Error durante la evaluación: {str(e)}\\\"]\\n\"\n",
    "                    \"**Comentario General**: [\\\"Ocurrió un error inesperado durante la evaluación del código.\\\"]\"\n",
    "                )\n",
    "\n",
    "            finally:\n",
    "                # Restaurar la salida estándar original\n",
    "                sys.stdout = original_stdout\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error inesperado en evaluar_ejercicios: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "    return evaluaciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "def extraer_resultados(resultados, nombre_alumno, log_file):\n",
    "    \"\"\"\n",
    "    Extrae las puntuaciones, comentarios y comentarios generales de una estructura de resultados para todos los ejercicios.\n",
    "\n",
    "    Parámetros:\n",
    "    resultados (dict): Diccionario con las evaluaciones para todos los ejercicios.\n",
    "    nombre_alumno (str): Nombre del alumno.\n",
    "    log_file (str): Ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "\n",
    "    Retorna:\n",
    "    dict: Un diccionario con los resultados estructurados para todos los ejercicios.\n",
    "    \"\"\"\n",
    "    # Configurar el logging\n",
    "    configurar_logger(log_file)\n",
    "\n",
    "    # Inicializar el diccionario para almacenar los resultados\n",
    "    resultados_detallados = {}\n",
    "    errores = False\n",
    "\n",
    "    # Recorrer cada ejercicio y extraer los resultados\n",
    "    for ejercicio, evaluacion in resultados.items():\n",
    "        try:\n",
    "            comentarios = evaluacion  # El texto completo del resultado\n",
    "            \n",
    "            # Buscar el patrón para las puntuaciones usando una expresión regular\n",
    "            puntuaciones_pattern = re.search(r'\\*\\*Puntuaciones\\*\\*:\\s*(\\[[^\\]]*\\])', comentarios)\n",
    "            # Buscar el patrón para los comentarios usando una expresión regular\n",
    "            comentarios_pattern = re.search(r'\\*\\*Comentarios\\*\\*:\\s*(\\[.*?\\])', comentarios, re.DOTALL)\n",
    "            # Buscar el patrón para el comentario general usando una expresión regular\n",
    "            comentario_general_pattern = re.search(r'\\*\\*Comentario General\\*\\*:\\s*(\\[.*?\\])', comentarios, re.DOTALL)\n",
    "            \n",
    "            # Si se encuentra el patrón de puntuaciones, evalúa la cadena como una lista\n",
    "            if puntuaciones_pattern:\n",
    "                puntuaciones = eval(puntuaciones_pattern.group(1))\n",
    "            else:\n",
    "                puntuaciones = []\n",
    "\n",
    "            # Si se encuentra el patrón de comentarios, evalúa la cadena como una lista\n",
    "            if comentarios_pattern:\n",
    "                comentarios_list = eval(comentarios_pattern.group(1))\n",
    "            else:\n",
    "                comentarios_list = []\n",
    "\n",
    "            # Si se encuentra el patrón de comentario general, evalúa la cadena como una lista\n",
    "            if comentario_general_pattern:\n",
    "                comentario_general = eval(comentario_general_pattern.group(1))[0]  # Extrae el primer elemento\n",
    "            else:\n",
    "                comentario_general = \"No disponible\"\n",
    "\n",
    "            # Añadir el resultado al diccionario 'resultados_detallados' para el ejercicio actual\n",
    "            resultados_detallados[ejercicio] = {\n",
    "                'puntuaciones': puntuaciones,\n",
    "                'comentarios': comentarios_list,\n",
    "                'comentario_general': comentario_general\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error procesando los resultados para '{nombre_alumno}' en el ejercicio '{ejercicio}': {e}\"\n",
    "            logging.error(error_msg)\n",
    "            errores = True\n",
    "\n",
    "    if not errores:\n",
    "        success_msg = f\"Todos los resultados de '{nombre_alumno}' fueron procesados correctamente.\"\n",
    "        logging.info(success_msg)\n",
    "        print(success_msg)\n",
    "\n",
    "    return resultados_detallados\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPROBAR TODAS LAS FUNCIONES CON UN NOTEBOOK ENTREGADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterios cargados correctamente. Continuando con la verificación del examen...\n",
      "La estructura del examen es correcta. Continuando con la extracción de información...\n",
      "Información del examen extraída correctamente. Continuando con el procesamiento de respuestas del alumno...\n",
      "Respuestas del alumno procesadas correctamente. Continuando con la comprobación de ejecución...\n",
      "Ejecución de las respuestas del alumno verificada correctamente.\n",
      "Evaluación de las respuestas del alumno completada correctamente.\n",
      "Todos los resultados de 'santos_alfonso.ipynb' fueron procesados correctamente.\n",
      "Resumen de la evaluación detallada:\n",
      "Ejercicio 1: Puntuaciones - [10, 10, 5, 8, 9], Comentarios - ['El código se ejecuta sin errores y produce la salida correcta.', 'Se cumplen todas las instrucciones del ejercicio perfectamente.', 'Faltan comentarios que expliquen distintas partes del código, lo que dificultaría la comprensión para un lector.', 'El código es claro y estructurado, aunque el uso de un bucle for es un poco más sencillo de entender. Podría beneficiarse de nombres más descriptivos para las variables para mejorar la legibilidad general.', 'El código es eficiente en su ejecución y utiliza adecuadamente las listas y las funciones para minimizar la complejidad.'], Comentario General - El estudiante ha proporcionado una solución correcta y ejecutable al ejercicio, cumpliendo con los requisitos establecidos. Sin embargo, la falta de comentarios explicativos puede dificultar la comprensión por parte de otros usuarios.\n",
      "Ejercicio 2: Puntuaciones - [10, 10, 7, 8, 10, 10], Comentarios - ['El código se ejecuta correctamente', 'Cumple con todos los requisitos del ejercicio', 'Los comentarios existentes son mínimos, se sugiere añadir más explicaciones sobre las funciones', \"El código es mayormente claro, pero el uso de 'dict' como nombre de parámetro podría causar confusión\", 'Las funciones están bien definidas y mejoran la modularidad del código', 'El código es eficiente y utiliza correctamente estructuras de datos y bucles.'], Comentario General - El código cumple con los requisitos del ejercicio y se ejecuta sin errores. Presenta algunas oportunidades de mejora en los comentarios y en la claridad del uso de nombres de variables.\n",
      "Ejercicio 3: Puntuaciones - [10, 10, 8, 9, 9, 10], Comentarios - ['El código se ejecuta sin errores y cumple con todos los requisitos del ejercicio.', 'El uso de nombres descriptivos para las variables facilita la comprensión del código.', 'Los comentarios son útiles pero podrían incluir más detalles sobre ciertas secciones, como el cálculo de la diversificación.', 'El uso de bucles es adecuado para iterar sobre listas y realizar los cálculos necesarios.', 'La eficiencia es buena, utilizando adecuadamente la función sum() y list comprehensions.', 'Se ha utilizado un bucle for de manera efectiva para evaluar cada inversión.'], Comentario General - El código es funcional y cumple con todos los requerimientos del ejercicio. No genera errores y presenta una buena legibilidad y estructura.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Configurar las rutas de los archivos y directorios\n",
    "directorio_raiz = \"/workspace\"\n",
    "directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "\n",
    "log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "nombre_fich_examen = 'examen_pruebas.ipynb'\n",
    "criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "fichero_res = 'resultados_evaluacion.xlsx'\n",
    "examen_file = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "# Crear el directorio de logs si no existe\n",
    "if not os.path.exists(dir_log):\n",
    "    os.makedirs(dir_log)\n",
    "\n",
    "# Borrar el archivo de log si ya existe para empezar con un archivo limpio\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Configurar el logging para registrar errores tanto en consola como en el archivo de log\n",
    "logging.basicConfig(filename=log_file, level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Registrar un mensaje de log\n",
    "logging.error(\"Logging configurado correctamente. Comenzando la ejecución del script.\")\n",
    "\n",
    "try:\n",
    "    # 1. Cargar criterios\n",
    "    criterios = cargar_criterios(criterios_file, log_file)\n",
    "    print(\"Criterios cargados correctamente. Continuando con la verificación del examen...\")\n",
    "\n",
    "    # 2. Verificar la estructura del examen\n",
    "    verifica_estructura_examen(examen_file, log_file)\n",
    "    print(\"La estructura del examen es correcta. Continuando con la extracción de información...\")\n",
    "\n",
    "    # 3. Extraer la información del examen\n",
    "    examen_info = extrae_informacion_examen(examen_file, log_file)\n",
    "    print(\"Información del examen extraída correctamente. Continuando con el procesamiento de respuestas del alumno...\")\n",
    "\n",
    "    # 4. Procesar respuestas de un alumno\n",
    "    nombre_fich_alumno = 'santos_alfonso.ipynb'\n",
    "    alumno_file = os.path.join(directorio_entregas, nombre_fich_alumno)\n",
    "    respuestas_alumno = procesa_respuestas_alumno(alumno_file, len(examen_info['ejercicios']), log_file)\n",
    "    print(\"Respuestas del alumno procesadas correctamente. Continuando con la comprobación de ejecución...\")\n",
    "\n",
    "    # 5. Comprobar la ejecución de las respuestas del alumno\n",
    "    respuestas_evaluadas = comprueba_ejecucion(respuestas_alumno, log_file)\n",
    "    print(\"Ejecución de las respuestas del alumno verificada correctamente.\")\n",
    "\n",
    "    # 6. Evaluar las respuestas utilizando ChatGPT\n",
    "    evaluaciones = evaluar_ejercicios(examen_info, respuestas_evaluadas, prompt_file, criterios_file, log_file)\n",
    "    print(\"Evaluación de las respuestas del alumno completada correctamente.\")\n",
    "\n",
    "    # 7. Extraer los resultados detallados de la evaluación\n",
    "    resultados_detallados = extraer_resultados(evaluaciones, nombre_fich_alumno, log_file)\n",
    "\n",
    "    # 8. Imprimir un resumen del resultado de la evaluación detallada\n",
    "    print(\"Resumen de la evaluación detallada:\")\n",
    "    for ejercicio, resultado in resultados_detallados.items():\n",
    "        estado = resultado.get('estado', 'No definido')\n",
    "        if estado == 'Error':\n",
    "            mensaje_error = resultado.get('mensaje_de_error', 'Ninguno')\n",
    "            print(f\"{ejercicio}: Estado - Error, Mensaje de error - {mensaje_error}\")\n",
    "        else:\n",
    "            puntuaciones = resultado.get('puntuaciones', 'N/A')\n",
    "            comentarios = resultado.get('comentarios', 'N/A')\n",
    "            comentario_general = resultado.get('comentario_general', 'N/A')\n",
    "            print(f\"{ejercicio}: Puntuaciones - {puntuaciones}, Comentarios - {comentarios}, Comentario General - {comentario_general}\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado: {e}\")\n",
    "    logging.error(f\"Error inesperado: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobar todas las funciones con todos los ficheros en el directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterios cargados correctamente. Continuando con la verificación del examen...\n",
      "La estructura del examen es correcta. Continuando con la extracción de información...\n",
      "Información del examen extraída correctamente. Listo para procesar respuestas de los alumnos.\n",
      "\n",
      "Evaluando al alumno: santos_alfonso\n",
      "Respuestas del alumno santos_alfonso procesadas correctamente. Continuando con la comprobación de ejecución...\n",
      "Ejecución de las respuestas del alumno santos_alfonso verificada correctamente.\n",
      "Evaluación de las respuestas del alumno santos_alfonso completada correctamente.\n",
      "Todos los resultados de 'santos_alfonso' fueron procesados correctamente.\n",
      "Resumen de la evaluación detallada para santos_alfonso:\n",
      "Ejercicio 1: Puntuaciones - [10, 10, 7, 9, 10], Comentarios - ['El código se ejecuta correctamente y produce la salida esperada.', 'Todo el ejercicio se ha cumplido de acuerdo a las instrucciones.', 'Los comentarios son mínimos; sería beneficioso incluir más para mejorar la comprensión del código.', 'El código es claro y fácil de seguir, aunque podría beneficiarse de nombres de variables que reflejen mejor su contenido.', 'El uso de estructuras de datos y el algoritmo son eficientes, logrando el objetivo sin complejidad innecesaria.'], Comentario General - El código cumple con los requisitos del ejercicio, se ejecuta sin errores y realiza los cálculos correctos. Sin embargo, la claridad de los comentarios podría mejorarse para facilitar la comprensión del código.\n",
      "Ejercicio 2: Puntuaciones - [10, 10, 5, 8, 10, 10, 10, 10], Comentarios - ['El código se ejecuta correctamente sin errores y produce la salida esperada para ambas funciones.', 'Cumple con todas las instrucciones del ejercicio, y las funciones realizan las tareas requeridas.', 'Aunque hay un comentario inicial, sería beneficioso explicar brevemente la lógica de cada función y su propósito.', \"Los nombres de las funciones y variables son mayormente claros, aunque 'dict' como nombre de parámetro podría ser más descriptivo.\", 'La lógica es eficiente, utilizando correctamente los métodos de diccionarios y listas.', 'Las funciones están bien definidas y evitan la repetición de código, manteniendo una buena modularidad.', \"El uso de `float('-inf')` es apropiado para el cálculo del mes con el ingreso más alto.\"], Comentario General - El código cumple con los requisitos del ejercicio y se ejecuta sin errores. Sin embargo, se recomienda mejorar los comentarios para aumentar la claridad y la comprensión del código.\n",
      "Ejercicio 3: Puntuaciones - [10, 10, 8, 9, 10, 10], Comentarios - ['El código se ejecuta sin errores y produce la salida esperada.', 'Cumple con todas las especificaciones del ejercicio.', 'Los comentarios son útiles, pero se podrían añadir más detalles sobre el cálculo de diversificación.', 'El código es claro y fácil de entender, con nombres de variables descriptivos.', 'La solución es eficiente y utiliza correctamente las estructuras de datos.', 'Se utilizan bucles de manera adecuada para iterar sobre las listas.'], Comentario General - El ejercicio se ha resuelto correctamente, el código es funcional y cumple con todos los requisitos planteados en el enunciado.\n",
      "\n",
      "Evaluando al alumno: ubeda_fernando\n",
      "Respuestas del alumno ubeda_fernando procesadas correctamente. Continuando con la comprobación de ejecución...\n",
      "Ejecución de las respuestas del alumno ubeda_fernando verificada correctamente.\n",
      "Evaluación de las respuestas del alumno ubeda_fernando completada correctamente.\n",
      "Todos los resultados de 'ubeda_fernando' fueron procesados correctamente.\n",
      "Resumen de la evaluación detallada para ubeda_fernando:\n",
      "Ejercicio 1: Puntuaciones - [0], Comentarios - ['Error en la ejecución: list assignment index out of range'], Comentario General - El código presentado contiene errores que impiden su correcta ejecución.\n",
      "Ejercicio 2: Puntuaciones - [0], Comentarios - [\"Error en la ejecución: cannot access local variable 'maximo' where it is not associated with a value\"], Comentario General - El código presentado contiene errores que impiden su correcta ejecución.\n",
      "Ejercicio 3: Puntuaciones - [0], Comentarios - [\"Error en la ejecución: name 'valor_total' is not defined\"], Comentario General - El código presentado contiene errores que impiden su correcta ejecución.\n",
      "\n",
      "Evaluando al alumno: ventura_pedro\n",
      "Respuestas del alumno ventura_pedro procesadas correctamente. Continuando con la comprobación de ejecución...\n",
      "Ejecución de las respuestas del alumno ventura_pedro verificada correctamente.\n",
      "Evaluación de las respuestas del alumno ventura_pedro completada correctamente.\n",
      "Todos los resultados de 'ventura_pedro' fueron procesados correctamente.\n",
      "Resumen de la evaluación detallada para ventura_pedro:\n",
      "Ejercicio 1: Puntuaciones - [5, 8, 5, 7, 6], Comentarios - ['El código se ejecuta correctamente, pero la forma en que se calculan los saldos no utiliza bucles, lo que resta eficiencia y simplicidad.', 'El código es mayormente claro, aunque se podría mejorar la claridad al utilizar un bucle o comprensión de listas.', 'Hay un comentario que indica donde se debe escribir el código, pero falta explicar el proceso que se sigue para calcular los saldos.', 'Los nombres de las variables son adecuados y descriptivos, pero la estructura se puede simplificar.', 'El uso de append es innecesario aquí y podría ser optimizado usando una comprensión de listas para calcular los saldos.'], Comentario General - El código genera errores en la puntuación máxima porque, aunque se ejecuta correctamente, no cumple con el uso de bucles como se indica en el enunciado del ejercicio. La solución podría ser más eficiente y clara utilizando iteración.\n",
      "Ejercicio 2: Puntuaciones - [10, 10, 8, 9, 10, 10], Comentarios - ['El código se ejecuta correctamente y devuelve los resultados esperados.', 'Se han seguido todas las instrucciones y se han calculado correctamente los ingresos y promedios.', 'Los comentarios son útiles, pero podrían ser más detallados en la explicación de algunas funciones.', 'El uso de nombres para las variables y funciones es claro y descriptivo, lo que facilita la comprensión del código.', 'El código es muy eficiente, no hay redundancias ni operaciones innecesarias.', 'Las funciones están bien definidas y estructuradas, y cumplen con su propósito de mejorar la modularidad.'], Comentario General - El código del estudiante cumple con los requisitos del ejercicio y no genera errores al ejecutarse. Se puede mejorar en términos de comentarios, pero en general está bien estructurado y es eficiente.\n",
      "Ejercicio 3: Puntuaciones - [8, 7, 6, 8, 5, 8], Comentarios - ['El código se ejecuta sin errores y calcula correctamente el valor total de las inversiones.', 'Se cumple con la mayoría de las instrucciones, pero falta el cálculo de la diversificación de la cartera.', 'Los comentarios son escasos y no explican las secciones cruciales del código, se recomienda agregar más.', \"Los nombres de las variables son descriptivos, pero algunos nombres como 'inversiones' podrían ser más claros, como 'valores_inversion'.\", 'El cálculo de proporciones utiliza incorrectamente el umbral en lugar del valor total en los porcentajes, lo que afecta la precisión.', 'El uso de un bucle para comprobar inversiones superiores al umbral es adecuado y se aplica de manera eficiente.'], Comentario General - El código presenta un buen intento de cumplir con los requisitos del ejercicio, pero hay errores en el cálculo de proporciones y falta la implementación de la diversificación. Se recomienda mejorar los comentarios y corregir la lógica del cálculo de proporciones.\n",
      "Evaluación completada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Configurar las rutas de los archivos y directorios\n",
    "directorio_raiz = \"/workspace\"\n",
    "directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "\n",
    "log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "nombre_fich_examen = 'examen_pruebas.ipynb'\n",
    "criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "fichero_res = 'resultados_evaluacion.xlsx'\n",
    "examen_file = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "# Crear el directorio de logs si no existe\n",
    "if not os.path.exists(dir_log):\n",
    "    os.makedirs(dir_log)\n",
    "\n",
    "# Borrar el archivo de log si ya existe para empezar con un archivo limpio\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Bloque de preparación: Se ejecuta una sola vez\n",
    "try:\n",
    "    # 1. Cargar criterios\n",
    "    criterios = cargar_criterios(criterios_file, log_file)\n",
    "    print(\"Criterios cargados correctamente. Continuando con la verificación del examen...\")\n",
    "\n",
    "    # 2. Verificar la estructura del examen\n",
    "    verifica_estructura_examen(examen_file, log_file)\n",
    "    print(\"La estructura del examen es correcta. Continuando con la extracción de información...\")\n",
    "\n",
    "    # 3. Extraer la información del examen\n",
    "    examen_info = extrae_informacion_examen(examen_file, log_file)\n",
    "    print(\"Información del examen extraída correctamente. Listo para procesar respuestas de los alumnos.\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico en la preparación: {e}\")\n",
    "    logging.error(f\"Error crítico en la preparación: {e}\")\n",
    "    sys.exit(1)  # Termina el script con un código de error si falla la preparación\n",
    "\n",
    "# Obtener la lista de alumnos y archivos de notebooks\n",
    "try:\n",
    "    alumnos, archivos_notebooks = listar_notebooks(directorio_entregas, log_file)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error crítico al listar los notebooks: {e}\")\n",
    "    sys.exit(1)  # Termina el script si no se pueden listar los notebooks\n",
    "\n",
    "# Bucle para evaluar a cada alumno\n",
    "for nombre_fich_alumno, archivo_notebook in zip(alumnos, archivos_notebooks):\n",
    "    try:\n",
    "        alumno_file = os.path.join(directorio_entregas, archivo_notebook)\n",
    "        print(f\"\\nEvaluando al alumno: {nombre_fich_alumno}\")\n",
    "\n",
    "        # 4. Procesar respuestas de un alumno\n",
    "\n",
    "        respuestas_alumno = procesa_respuestas_alumno(alumno_file, len(examen_info['ejercicios']), log_file)\n",
    "        print(f\"Respuestas del alumno {nombre_fich_alumno} procesadas correctamente. Continuando con la comprobación de ejecución...\")\n",
    "\n",
    "        # 5. Comprobar la ejecución de las respuestas del alumno\n",
    "        respuestas_evaluadas = comprueba_ejecucion(respuestas_alumno, log_file)\n",
    "        print(f\"Ejecución de las respuestas del alumno {nombre_fich_alumno} verificada correctamente.\")\n",
    "\n",
    "        # 6. Evaluar las respuestas utilizando ChatGPT\n",
    "        evaluaciones = evaluar_ejercicios(examen_info, respuestas_evaluadas, prompt_file, criterios_file, log_file)\n",
    "        print(f\"Evaluación de las respuestas del alumno {nombre_fich_alumno} completada correctamente.\")\n",
    "\n",
    "        # 7. Extraer los resultados detallados de la evaluación\n",
    "        resultados_detallados = extraer_resultados(evaluaciones, nombre_fich_alumno, log_file)\n",
    "\n",
    "        # 8. Imprimir un resumen del resultado de la evaluación detallada\n",
    "        print(f\"Resumen de la evaluación detallada para {nombre_fich_alumno}:\")\n",
    "        for ejercicio, resultado in resultados_detallados.items():\n",
    "            print(f\"{ejercicio}: Puntuaciones - {resultado['puntuaciones']}, Comentarios - {resultado['comentarios']}, Comentario General - {resultado['comentario_general']}\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error al evaluar al alumno {nombre_fich_alumno}: {e}\")\n",
    "        logging.error(f\"Error al evaluar al alumno {nombre_fich_alumno}: {e}\")\n",
    "        continue  # Salta al siguiente alumno si hay un error crítico\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado al evaluar al alumno {nombre_fich_alumno}: {e}\")\n",
    "        logging.error(f\"Error inesperado al evaluar al alumno {nombre_fich_alumno}: {e}\")\n",
    "        continue  # Salta al siguiente alumno si hay un error inesperado\n",
    "\n",
    "print(\"Evaluación completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import os\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def main(directorio_raiz, nombre_fich_examen):\n",
    "#     \"\"\"\n",
    "#     Función principal para evaluar los notebooks entregados por los estudiantes.\n",
    "    \n",
    "#     Parameters:\n",
    "#         directorio_raiz (str): Ruta del directorio raíz que contiene los subdirectorios y archivos necesarios.\n",
    "#         nombre_fich_examen (str): Nombre del archivo del examen.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Configurar las rutas basadas en el directorio raíz\n",
    "#         directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "#         dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "#         prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "#         directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "#         directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "#         criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "#         fichero_res = 'resultados_evaluacion.xlsx'\n",
    "#         fichero_examen = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "#         titulo_examen = \"Python Core para Finanzas\"\n",
    "\n",
    "#         # Crear el directorio de logs si no existe\n",
    "#         if not os.path.exists(dir_log):\n",
    "#             os.makedirs(dir_log)\n",
    "\n",
    "#         log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "        \n",
    "#         # Configurar el logging\n",
    "#         logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#         logging.error(\"Inicio del proceso de evaluación.\")\n",
    "\n",
    "#         # Cargar criterios\n",
    "#         if not os.path.exists(criterios_file):\n",
    "#             raise FileNotFoundError(f\"El archivo de criterios {criterios_file} no existe.\")\n",
    "#         criterios = cargar_criterios(criterios_file, log_file)\n",
    "\n",
    "#         # Preprocesar el examen\n",
    "#         if not os.path.exists(fichero_examen):\n",
    "#             raise FileNotFoundError(f\"El archivo de examen {fichero_examen} no existe.\")\n",
    "#         examen_info = extrae_informacion_examen(fichero_examen, log_file)\n",
    "\n",
    "#         # Listar los notebooks entregados por los alumnos\n",
    "#         if not os.path.exists(directorio_entregas):\n",
    "#             raise FileNotFoundError(f\"El directorio de entregas {directorio_entregas} no existe.\")\n",
    "#         alumnos, ficheros = listar_notebooks(directorio_entregas, log_file)\n",
    "#         if not ficheros:\n",
    "#             raise FileNotFoundError(\"No se encontraron notebooks de alumnos en el directorio de entregas.\")\n",
    "        \n",
    "#         # Inicializar el diccionario de resultados\n",
    "#         resultados = {}\n",
    "#         resultados['criterios'] = criterios\n",
    "\n",
    "#         # Procesar y evaluar cada notebook en paralelo\n",
    "#         resultados_alumnos = Parallel(n_jobs=-1)(\n",
    "#             delayed(procesa_y_evalua_notebook)(fich, directorio_entregas, examen_info, criterios, prompt_file, log_file) \n",
    "#             for fich in tqdm(ficheros, desc=\"Procesando notebooks\")\n",
    "#         )\n",
    "\n",
    "#         # Filtrar resultados exitosos y agregar al diccionario de resultados\n",
    "#         for result in resultados_alumnos:\n",
    "#             if result is not None:\n",
    "#                 alumno, res_extraido = result\n",
    "#                 resultados[alumno] = res_extraido\n",
    "        \n",
    "#         # # Generar los informes en PDF para cada estudiante\n",
    "#         # generar_pdfs_para_estudiantes(examen_info, directorio_reports, resultados)\n",
    "        \n",
    "#         # # Generar el archivo Excel con los resultados de la evaluación\n",
    "#         # generar_excel_resultados(resultados, filename=os.path.join(directorio_reports, fichero_res))\n",
    "        \n",
    "#         print(\"Proceso completado con éxito.\")\n",
    "#         logging.error(\"Proceso completado con éxito.\")\n",
    "    \n",
    "#         return resultados, examen_info\n",
    "\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         error_msg = f\"Error en el proceso: {e}\"\n",
    "#         logging.error(error_msg)\n",
    "#         print(error_msg, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUA TODOS LOS FICHEROS Y GENERA LOS INFORMES DEL PROFESOR Y ALUMNOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def generar_informe_profesor(examen_info, resultados, nombre_examen, output_dir):\n",
    "    \"\"\"\n",
    "    Genera un informe en PDF para el profesor que resume las calificaciones, comentarios y errores \n",
    "    para cada alumno, junto con gráficos de notas.\n",
    "\n",
    "    Parameters:\n",
    "        examen_info (dict): Diccionario que contiene el contexto del examen y los enunciados de los ejercicios.\n",
    "        resultados (dict): Diccionario con los resultados de la evaluación para todos los alumnos.\n",
    "        nombre_examen (str): Nombre del examen.\n",
    "        output_dir (str): Directorio donde se guardará el informe en PDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear el objeto PDF\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    \n",
    "    # Agregar la portada\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 16)\n",
    "    pdf.cell(0, 10, f\"Informe Resumido de Evaluación - {nombre_examen}\", ln=True, align='C')\n",
    "    pdf.ln(20)\n",
    "    \n",
    "    # Inicializar las variables para las notas finales\n",
    "    notas_finales = []\n",
    "    notas_por_ejercicio = {f'Ejercicio {i+1}': [] for i in range(len(examen_info['ejercicios']))}\n",
    "\n",
    "    # Resumen para cada alumno\n",
    "    for alumno, resultado_alumno in resultados.items():\n",
    "        if alumno == 'criterios':\n",
    "            continue  # Saltar el diccionario de criterios, ya que no es un alumno\n",
    "        \n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 14)\n",
    "        \n",
    "        # Calcular y añadir la nota final para el alumno al principio\n",
    "        notas_ejercicios = []\n",
    "        for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "            resultado_ejercicio = resultado_alumno.get(f\"Ejercicio {i}\", {})\n",
    "            puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "            if puntuaciones:\n",
    "                nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                notas_ejercicios.append(nota_media)\n",
    "        \n",
    "        if notas_ejercicios:\n",
    "            nota_final = sum(notas_ejercicios) / len(notas_ejercicios)\n",
    "            notas_finales.append(nota_final)\n",
    "            pdf.cell(0, 10, f\"Alumno: {alumno.split('.')[0]} - Nota Final: {nota_final:.2f}\", ln=True)\n",
    "        else:\n",
    "            pdf.cell(0, 10, f\"Alumno: {alumno.split('.')[0]}\", ln=True)\n",
    "        \n",
    "        pdf.ln(10)  # Añadir espacio para evitar superposiciones\n",
    "        \n",
    "        # Resumen de cada ejercicio\n",
    "        for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "            resultado_ejercicio = resultado_alumno.get(f\"Ejercicio {i}\", {})\n",
    "            puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "            comentarios = resultado_ejercicio.get('comentarios', [])\n",
    "            \n",
    "            if puntuaciones and comentarios:\n",
    "                # Mostrar la puntuación y el comentario para cada criterio\n",
    "                for j, (puntuacion, comentario) in enumerate(zip(puntuaciones, comentarios)):\n",
    "                    pdf.set_font(\"Arial\", \"\", 12)\n",
    "                    criterio_nombre = examen_info['ejercicios'][i-1]['criterios'][j].strip('@')\n",
    "                    pdf.cell(0, 10, f\"{criterio_nombre}: Puntuación - {puntuacion}\", ln=True)\n",
    "                    pdf.set_font(\"Arial\", \"I\", 12)\n",
    "                    pdf.multi_cell(0, 10, f\"Comentario - {comentario}\")\n",
    "                \n",
    "                # Mostrar la nota promedio del ejercicio\n",
    "                nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                notas_por_ejercicio[f'Ejercicio {i}'].append(nota_media)\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(0, 10, f\"Nota del Ejercicio {i}: {nota_media:.2f}\", ln=True)\n",
    "            else:\n",
    "                # Si no hay puntuaciones/comentarios, se considera un error\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "                pdf.cell(0, 10, \"Errores:\", ln=True)\n",
    "                pdf.multi_cell(0, 10, f\"{resultado_ejercicio.get('comentarios', 'No disponible')}\")\n",
    "            \n",
    "            pdf.ln(5)\n",
    "        \n",
    "    # Gráfica del histograma de notas finales\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(notas_finales, bins=10, edgecolor='black')\n",
    "    plt.title('Histograma de Notas Finales')\n",
    "    plt.xlabel('Nota Final')\n",
    "    plt.ylabel('Número de Alumnos')\n",
    "    plt.tight_layout()\n",
    "    hist_path_finales = os.path.join(output_dir, 'histograma_notas_finales.png')\n",
    "    plt.savefig(hist_path_finales)\n",
    "    plt.close()\n",
    "\n",
    "    # Añadir el histograma de notas finales al PDF\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 14)\n",
    "    pdf.cell(0, 10, 'Histograma de Notas Finales', ln=True)\n",
    "    pdf.image(hist_path_finales, x=10, y=30, w=190)\n",
    "    \n",
    "    # Histograma de las notas y gráficas de criterios por ejercicio\n",
    "    for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "        notas = notas_por_ejercicio[f'Ejercicio {i}']\n",
    "        if notas:\n",
    "            # Histograma de las notas\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.hist(notas, bins=10, edgecolor='black')\n",
    "            plt.title(f'Histograma de Notas - Ejercicio {i}')\n",
    "            plt.xlabel('Nota')\n",
    "            plt.ylabel('Número de Alumnos')\n",
    "            plt.tight_layout()\n",
    "            img_path = os.path.join(output_dir, f'histograma_ejercicio_{i}.png')\n",
    "            plt.savefig(img_path)\n",
    "            plt.close()\n",
    "\n",
    "            # Añadir el histograma al PDF\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(\"Arial\", \"B\", 14)\n",
    "            pdf.cell(0, 10, f\"Histograma de Notas - Ejercicio {i}\", ln=True)\n",
    "            pdf.image(img_path, x=10, y=30, w=190)\n",
    "\n",
    "            # Gráfico de barras para las puntuaciones por criterio\n",
    "            criterios = [criterio.strip('@') for criterio in examen_info['ejercicios'][i-1]['criterios']]\n",
    "\n",
    "            # Calcular las puntuaciones por criterio manejando el caso de longitud desigual\n",
    "            puntuaciones_criterio = []\n",
    "            for j in range(len(criterios)):\n",
    "                puntuaciones = [\n",
    "                    resultados[alumno][f'Ejercicio {i}']['puntuaciones'][j]\n",
    "                    for alumno in resultados\n",
    "                    if alumno != 'criterios' and f'Ejercicio {i}' in resultados[alumno] and\n",
    "                    len(resultados[alumno][f'Ejercicio {i}']['puntuaciones']) > j\n",
    "                ]\n",
    "                puntuaciones_criterio.append(np.mean(puntuaciones))\n",
    "\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(criterios, puntuaciones_criterio, color='blue', edgecolor='black')\n",
    "            plt.title(f'Puntuaciones por Criterio - Ejercicio {i}')\n",
    "            plt.xlabel('Criterio')\n",
    "            plt.ylabel('Puntuación Promedio')\n",
    "            plt.xticks(rotation=45, ha='right')  # Girar los nombres de los criterios 45 grados\n",
    "            plt.tight_layout()\n",
    "            img_path_criterios = os.path.join(output_dir, f'puntuaciones_criterios_ejercicio_{i}.png')\n",
    "            plt.savefig(img_path_criterios)\n",
    "            plt.close()\n",
    "\n",
    "            # Añadir gráfico de puntuaciones por criterio al PDF\n",
    "            pdf.add_page()\n",
    "            pdf.image(img_path_criterios, x=10, y=30, w=190)\n",
    "    \n",
    "    # Guardar el PDF en el directorio de salida\n",
    "    output_path = os.path.join(output_dir, f\"Informe_Profesor_{nombre_examen}.pdf\")\n",
    "    pdf.output(output_path)\n",
    "    print(f\"Informe resumido generado en: {output_path}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# nombre_examen = \"Python Core para Finanzas\"\n",
    "# generar_informe_profesor(examen_info, resultados, nombre_examen, \"/path/to/output/directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def generar_informe_pdf_alumnos(examen_info, resultados, nombre_examen, output_dir):\n",
    "    \"\"\"\n",
    "    Genera informes en PDF para todos los alumnos basados en la evaluación de sus ejercicios.\n",
    "\n",
    "    Parameters:\n",
    "        examen_info (dict): Diccionario que contiene el contexto del examen y los enunciados de los ejercicios.\n",
    "        resultados (dict): Diccionario con los resultados de la evaluación para todos los alumnos.\n",
    "        nombre_examen (str): Nombre del examen.\n",
    "        output_dir (str): Directorio donde se guardarán los informes en PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for alumno in resultados.keys():\n",
    "        if alumno == 'criterios':\n",
    "            continue\n",
    "\n",
    "        nombre_alumno = alumno.split('.')[0]\n",
    "\n",
    "        # Crear el objeto PDF\n",
    "        pdf = FPDF()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "        # Agregar la portada\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 16)\n",
    "        pdf.cell(0, 10, f\"Informe de Evaluación de {nombre_alumno}\", ln=True, align='C')\n",
    "        pdf.ln(10)\n",
    "        pdf.set_font(\"Arial\", \"\", 12)\n",
    "        pdf.cell(0, 10, f\"Curso: Introducción a Python para Finanzas\", ln=True, align='C')\n",
    "        pdf.cell(0, 10, f\"Examen: {nombre_examen}\", ln=True, align='C')\n",
    "        pdf.cell(0, 10, f\"Fecha: {pd.Timestamp('now').strftime('%d/%m/%Y')}\", ln=True, align='C')\n",
    "        pdf.ln(20)\n",
    "\n",
    "        # Introducción\n",
    "        pdf.set_font(\"Arial\", \"\", 12)\n",
    "        pdf.multi_cell(0, 10, examen_info['contexto'])\n",
    "        pdf.ln(10)\n",
    "\n",
    "        # Resumen de Evaluación Global\n",
    "        pdf.set_font(\"Arial\", \"B\", 14)\n",
    "        pdf.cell(0, 10, \"Resumen de Evaluación Global\", ln=True)\n",
    "        pdf.set_font(\"Arial\", \"\", 12)\n",
    "\n",
    "        # Calcular la puntuación total y la nota media\n",
    "        notas_ejercicios = []\n",
    "        for i in range(1, len(examen_info['ejercicios']) + 1):\n",
    "            resultado_ejercicio = resultados[alumno].get(f\"Ejercicio {i}\", {})\n",
    "            puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "            if puntuaciones:\n",
    "                nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "                notas_ejercicios.append(nota_media)\n",
    "        \n",
    "        if notas_ejercicios:\n",
    "            nota_final = sum(notas_ejercicios) / len(notas_ejercicios)\n",
    "        else:\n",
    "            nota_final = 0\n",
    "\n",
    "        pdf.cell(0, 10, f\"Puntuación Total: {nota_final:.2f}\", ln=True)\n",
    "        pdf.ln(5)\n",
    "\n",
    "        # Comentarios generales según la nota final\n",
    "        if nota_final >= 9:\n",
    "            comentarios_generales = \"Excelente desempeño. Sigue así para mantener este nivel.\"\n",
    "        elif nota_final >= 7:\n",
    "            comentarios_generales = \"Buen desempeño, pero hay algunas áreas que podrían beneficiarse de más práctica y revisión.\"\n",
    "        elif nota_final >= 5:\n",
    "            comentarios_generales = \"Desempeño aceptable, pero es necesario trabajar más en ciertos aspectos.\"\n",
    "        elif nota_final > 0:\n",
    "            comentarios_generales = \"El desempeño es bajo. Se recomienda revisar los conceptos básicos y practicar más.\"\n",
    "        else:\n",
    "            comentarios_generales = \"El alumno no ha completado satisfactoriamente el examen. Es necesario un repaso completo de los temas cubiertos.\"\n",
    "\n",
    "        pdf.multi_cell(0, 10, comentarios_generales)\n",
    "        pdf.ln(10)\n",
    "\n",
    "        # Detalle de Evaluación por Ejercicio\n",
    "        for i, ejercicio in enumerate(examen_info['ejercicios'], 1):\n",
    "            pdf.add_page()  # Cada ejercicio comienza en una nueva página\n",
    "            pdf.set_font(\"Arial\", \"B\", 12)\n",
    "\n",
    "            # Calcular la puntuación del ejercicio\n",
    "            resultado_ejercicio = resultados[alumno].get(f\"Ejercicio {i}\", {})\n",
    "            puntuaciones = resultado_ejercicio.get('puntuaciones', [])\n",
    "            if puntuaciones:\n",
    "                nota_media = sum(puntuaciones) / len(puntuaciones)\n",
    "            else:\n",
    "                nota_media = 0\n",
    "\n",
    "            # Título del ejercicio con la puntuación\n",
    "            pdf.cell(0, 10, f\"Ejercicio {i}: {ejercicio['enunciado'].splitlines()[0]}\", ln=False)\n",
    "            pdf.set_x(-50)  # Mueve el cursor para la puntuación hacia la derecha\n",
    "            pdf.cell(0, 10, f\"Puntuación: {nota_media:.2f}\", ln=True, align='R')\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "\n",
    "            # Enunciado completo\n",
    "            pdf.set_font(\"Arial\", \"B\", 12)\n",
    "            pdf.cell(0, 10, \"1. Enunciado:\", ln=True)\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            pdf.multi_cell(0, 10, ejercicio['enunciado'])\n",
    "            pdf.ln(5)\n",
    "\n",
    "            # Criterios evaluados\n",
    "            pdf.set_font(\"Arial\", \"B\", 12)\n",
    "            pdf.cell(0, 10, \"2. Criterios Evaluados:\", ln=True)\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            for criterio in ejercicio['criterios']:\n",
    "                criterio_nombre = criterio.strip('@')\n",
    "                descripcion_criterio = resultados['criterios'][criterio_nombre]['descripcion']\n",
    "                pdf.multi_cell(0, 10, f\"- {criterio_nombre}: {descripcion_criterio}\")\n",
    "            pdf.ln(5)\n",
    "\n",
    "            if resultado_ejercicio:\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(0, 10, \"3. Comentarios:\", ln=True)\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "                for comentario in resultado_ejercicio['comentarios']:\n",
    "                    pdf.multi_cell(0, 10, f\"- {comentario}\")\n",
    "                pdf.ln(5)\n",
    "\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(0, 10, \"4. Comentario General:\", ln=True)\n",
    "                pdf.set_font(\"Arial\", \"\", 12)\n",
    "                pdf.multi_cell(0, 10, resultado_ejercicio['comentario_general'])\n",
    "                pdf.ln(10)\n",
    "            else:\n",
    "                pdf.cell(0, 10, \"No se encontraron resultados para este ejercicio.\", ln=True)\n",
    "                pdf.ln(10)\n",
    "\n",
    "        # Conclusión y Recomendaciones ajustadas al nivel del examen\n",
    "        pdf.set_font(\"Arial\", \"B\", 14)\n",
    "        pdf.cell(0, 10, \"Conclusión y Recomendaciones\", ln=True)\n",
    "        pdf.set_font(\"Arial\", \"\", 12)\n",
    "        \n",
    "        if nota_final >= 9:\n",
    "            conclusion = \"Has demostrado un excelente dominio del material. Mantén este nivel de esfuerzo y sigue perfeccionando tus habilidades.\"\n",
    "        elif nota_final >= 7:\n",
    "            conclusion = \"Buen trabajo, pero hay algunas áreas que podrían beneficiarse de más práctica y revisión.\"\n",
    "        elif nota_final >= 5:\n",
    "            conclusion = \"El desempeño es aceptable, pero se recomienda revisar los temas en los que tuviste más dificultades para mejorar en futuras evaluaciones.\"\n",
    "        elif nota_final > 0:\n",
    "            conclusion = \"El rendimiento en este examen indica que es necesario un repaso más exhaustivo de los temas cubiertos. Considera buscar apoyo adicional para resolver dudas.\"\n",
    "        else:\n",
    "            conclusion = \"Es crucial que dediques tiempo a revisar todos los conceptos clave del curso. Considera buscar ayuda para entender mejor los temas.\"\n",
    "\n",
    "        pdf.multi_cell(0, 10, conclusion)\n",
    "        pdf.ln(10)\n",
    "\n",
    "        # Guardar el PDF en el directorio de salida\n",
    "        output_path = os.path.join(output_dir, f\"{nombre_alumno}_informe.pdf\")\n",
    "        pdf.output(output_path)\n",
    "        print(f\"Informe generado para {nombre_alumno} en: {output_path}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# nombre_examen = \"Python Core para Finanzas\"\n",
    "# generar_informe_pdf_alumnos(examen_info, resultados, nombre_examen, \"/path/to/output/directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_problemas(problemas, output_dir):\n",
    "    \"\"\"\n",
    "    Guarda la lista de problemas en un archivo de texto en el directorio de reports.\n",
    "\n",
    "    Parameters:\n",
    "        problemas (list): Lista de problemas encontrados durante la evaluación.\n",
    "        output_dir (str): Directorio donde se guardará el archivo de problemas.\n",
    "    \"\"\"\n",
    "    problemas_file = os.path.join(output_dir, 'problemas_entregas.txt')\n",
    "    \n",
    "    with open(problemas_file, 'w') as f:\n",
    "        for problema in problemas:\n",
    "            f.write(f\"{problema}\\n\")\n",
    "    \n",
    "    print(f\"Archivo de problemas guardado en: {problemas_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_y_evalua_notebook(fich, directorio_entregas, examen_info, criterios, prompt_file, log_file):\n",
    "    \"\"\"\n",
    "    Procesa y evalúa un notebook individual.\n",
    "    \n",
    "    Parameters:\n",
    "        fich (str): Nombre del archivo del notebook del alumno.\n",
    "        directorio_entregas (str): Ruta del directorio donde se encuentran los notebooks.\n",
    "        examen_info (dict): Información del examen.\n",
    "        criterios (dict): Criterios de evaluación.\n",
    "        prompt_file (str): Ruta del archivo de prompt.\n",
    "        log_file (str): Ruta del archivo de log donde se guardarán los mensajes de error.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Nombre del alumno y resultados de la evaluación.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Procesar respuestas del alumno\n",
    "        alumno_file = os.path.join(directorio_entregas, fich)\n",
    "        respuestas_alumno = procesa_respuestas_alumno(alumno_file, len(examen_info['ejercicios']), log_file)\n",
    "        \n",
    "        # Comprobar la ejecución de las respuestas\n",
    "        respuestas_evaluadas = comprueba_ejecucion(respuestas_alumno, log_file)\n",
    "        \n",
    "        # Evaluar las respuestas utilizando ChatGPT\n",
    "        evaluaciones = evaluar_ejercicios(examen_info, respuestas_evaluadas, prompt_file, criterios_file, log_file)\n",
    "        \n",
    "        # Extraer los resultados detallados de la evaluación\n",
    "        resultados_detallados = extraer_resultados(evaluaciones, fich, log_file)\n",
    "        \n",
    "        return fich, resultados_detallados\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error procesando el notebook {fich}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando notebooks: 100%|██████████| 3/3 [00:00<00:00, 4498.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los resultados de 'santos_alfonso.ipynb' fueron procesados correctamente.\n",
      "Todos los resultados de 'ventura_pedro.ipynb' fueron procesados correctamente.\n",
      "Informe generado para santos_alfonso en: /workspace/reports/santos_alfonso_informe.pdf\n",
      "Informe generado para ventura_pedro en: /workspace/reports/ventura_pedro_informe.pdf\n",
      "Informe resumido generado en: /workspace/reports/Informe_Profesor_Python Core para Finanzas.pdf\n",
      "Archivo de problemas guardado en: /workspace/reports/problemas_entregas.txt\n",
      "Proceso completado con éxito.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando notebooks: 100%|██████████| 3/3 [00:00<00:00, 4572.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los resultados de 'ventura_pedro.ipynb' fueron procesados correctamente.\n",
      "Todos los resultados de 'santos_alfonso.ipynb' fueron procesados correctamente.\n",
      "Informe generado para santos_alfonso en: /workspace/reports/santos_alfonso_informe.pdf\n",
      "Informe generado para ventura_pedro en: /workspace/reports/ventura_pedro_informe.pdf\n",
      "Informe resumido generado en: /workspace/reports/Informe_Profesor_Python Core para Finanzas.pdf\n",
      "Archivo de problemas guardado en: /workspace/reports/problemas_entregas.txt\n",
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main(directorio_raiz, nombre_fich_examen, nombre_examen):\n",
    "    try:\n",
    "        # Configurar las rutas basadas en el directorio raíz\n",
    "        directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "        dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "        prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "        directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "        directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "        criterios_file = os.path.join(directorio_raiz, 'criterios.txt')\n",
    "        fichero_res = 'resultados_evaluacion.xlsx'\n",
    "        fichero_examen = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "        # Crear el directorio de logs si no existe\n",
    "        if not os.path.exists(dir_log):\n",
    "            os.makedirs(dir_log)\n",
    "\n",
    "        log_file = os.path.join(dir_log, 'evaluacion.log')\n",
    "        \n",
    "        # Configurar el logging\n",
    "        logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logging.error(\"Inicio del proceso de evaluación.\")\n",
    "\n",
    "        # Cargar criterios\n",
    "        if not os.path.exists(criterios_file):\n",
    "            raise FileNotFoundError(f\"El archivo de criterios {criterios_file} no existe.\")\n",
    "        criterios = cargar_criterios(criterios_file, log_file)\n",
    "\n",
    "        # Preprocesar el examen\n",
    "        if not os.path.exists(fichero_examen):\n",
    "            raise FileNotFoundError(f\"El archivo de examen {fichero_examen} no existe.\")\n",
    "        examen_info = extrae_informacion_examen(fichero_examen, log_file)\n",
    "\n",
    "        # Listar los notebooks entregados por los alumnos\n",
    "        if not os.path.exists(directorio_entregas):\n",
    "            raise FileNotFoundError(f\"El directorio de entregas {directorio_entregas} no existe.\")\n",
    "        alumnos, ficheros = listar_notebooks(directorio_entregas, log_file)\n",
    "        if not ficheros:\n",
    "            raise FileNotFoundError(\"No se encontraron notebooks de alumnos en el directorio de entregas.\")\n",
    "        \n",
    "        # Inicializar el diccionario de resultados\n",
    "        resultados = {}\n",
    "        resultados['criterios'] = criterios\n",
    "        problemas = []  # Lista para almacenar los problemas con cada notebook\n",
    "\n",
    "        # Procesar y evaluar cada notebook en paralelo\n",
    "        resultados_alumnos = Parallel(n_jobs=-1)(\n",
    "            delayed(procesa_y_evalua_notebook)(fich, directorio_entregas, examen_info, criterios, prompt_file, log_file) \n",
    "            for fich in tqdm(ficheros, desc=\"Procesando notebooks\")\n",
    "        )\n",
    "\n",
    "        # Filtrar resultados exitosos y agregar al diccionario de resultados\n",
    "        procesados_exitosamente = []\n",
    "        for result in resultados_alumnos:\n",
    "            if result is not None:\n",
    "                alumno, res_extraido = result\n",
    "                resultados[alumno] = res_extraido\n",
    "                procesados_exitosamente.append(alumno)\n",
    "            else:\n",
    "                problemas.append(f\"Un notebook no se pudo procesar correctamente.\")  # Mejor usar el nombre del archivo si es posible.\n",
    "\n",
    "        # Generar los informes en PDF para cada estudiante\n",
    "        generar_informe_pdf_alumnos(examen_info, resultados, nombre_examen, directorio_reports)\n",
    "        \n",
    "        # Comparar la lista de notebooks entregados con los informes generados\n",
    "        informes_generados = os.listdir(directorio_reports)\n",
    "        for alumno in alumnos:\n",
    "            informe_esperado = f\"{alumno.split('.')[0]}_informe.pdf\"\n",
    "            if informe_esperado not in informes_generados:\n",
    "                problemas.append(f\"{alumno}: No se generó un informe.\")\n",
    "\n",
    "        # Generar el informe para el profesor\n",
    "        generar_informe_profesor(examen_info, resultados, nombre_examen, directorio_reports)\n",
    "\n",
    "        # Guardar el archivo con la lista de problemas\n",
    "        guardar_problemas(problemas, directorio_reports)\n",
    "\n",
    "        print(\"Proceso completado con éxito.\")\n",
    "        logging.error(\"Proceso completado con éxito.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error en el proceso: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg, file=sys.stderr)\n",
    "        \n",
    "        \n",
    "main(\"/workspace\", \"examen.ipynb\", \"Python Core para Finanzas\")\n",
    "\n",
    "        \n",
    "        \n",
    "main(\"/workspace\", \"examen.ipynb\", \"Python Core para Finanzas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando notebooks: 100%|██████████| 3/3 [00:00<00:00, 4390.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los resultados de 'ubeda_fernando.ipynb' fueron procesados correctamente.\n",
      "Todos los resultados de 'ventura_pedro.ipynb' fueron procesados correctamente.\n",
      "Todos los resultados de 'santos_alfonso.ipynb' fueron procesados correctamente.\n",
      "Informe resumido generado en: /workspace/reports/Informe_Profesor_Python Core para Finanzas.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error en el proceso: [Errno 13] Permission denied: 'Python Core para Finanzas'\n"
     ]
    }
   ],
   "source": [
    "main(\"/workspace\", \"examen.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe generado para santos_alfonso en: /workspace/reports/santos_alfonso_informe.pdf\n",
      "Informe generado para ubeda_fernando en: /workspace/reports/ubeda_fernando_informe.pdf\n",
      "Informe generado para ventura_pedro en: /workspace/reports/ventura_pedro_informe.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "nombre_examen = \"Python Core para Finanzas\"\n",
    "generar_informe_pdf_alumnos(examen_info, results, nombre_examen, directorio_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_examen = \"Python Core para Finanzas\"\n",
    "generar_informe_profesor(examen_info, results, titulo_examen, directorio_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ejercicio 1': {'puntuaciones': [0],\n",
       "  'comentarios': ['Error en la ejecución: list assignment index out of range'],\n",
       "  'comentario_general': 'El código presentado contiene errores que impiden su correcta ejecución.'},\n",
       " 'Ejercicio 2': {'puntuaciones': [0],\n",
       "  'comentarios': [\"Error en la ejecución: cannot access local variable 'maximo' where it is not associated with a value\"],\n",
       "  'comentario_general': 'El código presentado contiene errores que impiden su correcta ejecución.'},\n",
       " 'Ejercicio 3': {'puntuaciones': [0],\n",
       "  'comentarios': [\"Error en la ejecución: name 'valor_total' is not defined\"],\n",
       "  'comentario_general': 'El código presentado contiene errores que impiden su correcta ejecución.'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['ubeda_fernando.ipynb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe generado para ubeda_fernando en: /workspace/reports/ubeda_fernando_informe.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "nombre_alumno = \"ubeda_fernando\"\n",
    "nombre_examen = \"Primer Examen de Programación\"\n",
    "generar_informe_pdf(nombre_alumno, examen_info, results['ubeda_fernando.ipynb'], nombre_examen, \"/workspace/reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(FPDF):\n",
    "    \"\"\"\n",
    "    Clase personalizada para crear reportes de evaluación de ejercicios en formato PDF.\n",
    "    \n",
    "    Atributos:\n",
    "    alumno (str): Nombre del alumno para el cual se está generando el reporte.\n",
    "\n",
    "    Métodos:\n",
    "    __init__(self, alumno): Inicializa la instancia de la clase PDF con el nombre del alumno.\n",
    "    header(self): Añade un encabezado a cada página del PDF con el nombre del alumno.\n",
    "    footer(self): Añade un pie de página a cada página del PDF con el número de página.\n",
    "    add_context(self, contexto_examen): Añade el contexto del examen al PDF.\n",
    "    add_enunciados(self, enunciados): Añade los enunciados de los ejercicios al PDF.\n",
    "    add_evaluacion(self, alumno, ejercicios, criterios): Añade la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alumno):\n",
    "        \"\"\"\n",
    "        Inicializa la instancia de la clase PDF con el nombre del alumno.\n",
    "        \n",
    "        Parámetros:\n",
    "        alumno (str): Nombre del alumno para el cual se está generando el reporte.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alumno = alumno\n",
    "    \n",
    "    def header(self):\n",
    "        \"\"\"\n",
    "        Añade un encabezado a cada página del PDF con el nombre del alumno.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, f'Reporte de Evaluación de Ejercicios de {self.alumno}', 0, 1, 'C')\n",
    "\n",
    "    def footer(self):\n",
    "        \"\"\"\n",
    "        Añade un pie de página a cada página del PDF con el número de página.\n",
    "        \"\"\"\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        self.cell(0, 10, f'Página {self.page_no()}', 0, 0, 'C')\n",
    "\n",
    "    def add_context(self, contexto_examen):\n",
    "        \"\"\"\n",
    "        Añade el contexto del examen al PDF.\n",
    "        \n",
    "        Parámetros:\n",
    "        contexto_examen (str): Contexto general del examen.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Contexto del Examen:', 0, 1)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, contexto_examen)\n",
    "        self.ln(10)\n",
    "\n",
    "    def add_enunciados(self, enunciados):\n",
    "        \"\"\"\n",
    "        Añade los enunciados de los ejercicios al PDF.\n",
    "        \n",
    "        Parámetros:\n",
    "        enunciados (list): Lista de enunciados de los ejercicios del examen.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Enunciados de los Ejercicios:', 0, 1)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        for i, enunciado in enumerate(enunciados, 1):\n",
    "            self.cell(0, 10, f'Ejercicio {i}:', 0, 1)\n",
    "            self.multi_cell(0, 10, enunciado)\n",
    "            self.ln(5)\n",
    "        self.ln(10)\n",
    "\n",
    "    def add_evaluacion(self, alumno, ejercicios, criterios):\n",
    "        \"\"\"\n",
    "        Añade la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "        \n",
    "        Parámetros:\n",
    "        alumno (str): Nombre del alumno.\n",
    "        ejercicios (dict): Diccionario que contiene las evaluaciones de los ejercicios del estudiante.\n",
    "        criterios (list): Lista de criterios de evaluación.\n",
    "        \"\"\"\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.cell(0, 10, f'Informe de {alumno}', 0, 1, 'C')\n",
    "        self.ln(10)\n",
    "   \n",
    "        for ejercicio, contenido in ejercicios.items():\n",
    "            # Título del ejercicio\n",
    "            self.set_font('Arial', 'B', 12)\n",
    "            self.cell(0, 10, ejercicio, 0, 1)\n",
    "            self.ln(5)\n",
    "            \n",
    "            # Puntuaciones y comentarios\n",
    "            for i, criterio in enumerate(criterios):\n",
    "                self.set_font('Arial', 'B', 12)\n",
    "                self.cell(0, 10, f'{criterio}:', 0, 1, 'L')\n",
    "                self.set_font('Arial', '', 12)\n",
    "                self.multi_cell(0, 10, f'Puntuación: {contenido[\"puntuaciones\"][i]}')\n",
    "                self.multi_cell(0, 10, f'Comentario: {contenido[\"comentarios\"][i]}')\n",
    "                self.ln(5)\n",
    "            \n",
    "            # Comentario general\n",
    "            self.set_font('Arial', 'B', 12)\n",
    "            self.cell(0, 10, 'Comentario General del Ejercicio:', 0, 1, 'L')\n",
    "            self.set_font('Arial', '', 12)\n",
    "            self.multi_cell(0, 10, contenido['comentario_general'][0])\n",
    "            self.ln(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el PDF\n",
    "def create_pdf(file_path, student_name, contexto_examen, enunciados, ejercicios, criterios):\n",
    "    \"\"\"\n",
    "    Crea un archivo PDF con la evaluación del estudiante.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): Ruta donde se guardará el archivo PDF.\n",
    "    student_name (str): Nombre del estudiante.\n",
    "    contexto_examen (str): Contexto general del examen.\n",
    "    enunciados (list): Lista de enunciados de los ejercicios del examen.\n",
    "    ejercicios (dict): Diccionario que contiene las evaluaciones de los ejercicios del estudiante. La estructura del diccionario es:\n",
    "        {\n",
    "            'Ejercicio 1': {\n",
    "                'puntuaciones': [listado de puntuaciones],\n",
    "                'comentarios': [listado de comentarios],\n",
    "                'comentario_general': [comentario general]\n",
    "            },\n",
    "            'Ejercicio 2': {...},\n",
    "            ...\n",
    "        }\n",
    "    criterios (list): Lista de criterios de evaluación.\n",
    "\n",
    "    Procedimiento:\n",
    "    1. Crea una instancia de la clase PDF con el nombre del estudiante.\n",
    "    2. Añade una página al PDF.\n",
    "    3. Agrega el contexto del examen al PDF.\n",
    "    4. Agrega los enunciados de los ejercicios al PDF.\n",
    "    5. Agrega la evaluación del estudiante al PDF, incluyendo puntuaciones y comentarios para cada criterio.\n",
    "    6. Guarda el archivo PDF en la ruta especificada.\n",
    "\n",
    "    Retorna:\n",
    "    Ninguno\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = PDF(student_name)\n",
    "    pdf.add_page()\n",
    "    \n",
    "   \n",
    "    # Agregar contexto del examen\n",
    "    pdf.add_context(contexto_examen)\n",
    "    \n",
    "    # Agregar enunciados de los ejercicios (comentado porque no está en uso)\n",
    "    # pdf.add_enunciados(enunciados)\n",
    "    \n",
    "    # Agregar evaluación del estudiante\n",
    "    pdf.add_evaluacion(student_name, ejercicios, criterios)\n",
    "    \n",
    "    # Guardar el PDF con el nombre del estudiante\n",
    "    pdf.output(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_pdfs_para_estudiantes(examen_preprocesado, directorio_reports, resultados):\n",
    "    \"\"\"\n",
    "    Genera archivos PDF de evaluación para cada estudiante basado en los resultados del examen.\n",
    "\n",
    "    Parámetros:\n",
    "    examen_preprocesado (dict): Diccionario con los datos del examen preprocesado que incluye el contexto y los enunciados de los ejercicios.\n",
    "    directorio_reports (str): Ruta al directorio donde se guardarán los archivos PDF generados.\n",
    "    resultados (dict): Diccionario que contiene las evaluaciones de los estudiantes. La estructura del diccionario es:\n",
    "        {\n",
    "            'nombre_estudiante': {\n",
    "                'Ejercicio 1': {\n",
    "                    'puntuaciones': [listado de puntuaciones],\n",
    "                    'comentarios': [listado de comentarios],\n",
    "                    'comentario_general': [comentario general]\n",
    "                },\n",
    "                'Ejercicio 2': {...},\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "            'criterios': [listado de criterios]\n",
    "        }\n",
    "\n",
    "    Precondiciones:\n",
    "    - El archivo del examen debe existir en la ruta especificada.\n",
    "    - Las funciones `preprocesa_notebook` y `create_pdf` deben estar definidas previamente.\n",
    "\n",
    "    Procedimiento:\n",
    "    1. Crea el directorio para guardar los archivos PDF si no existe.\n",
    "    2. Para cada estudiante en los resultados, genera un archivo PDF con las evaluaciones y comentarios.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear directorio para guardar los PDFs\n",
    "    if not os.path.exists(directorio_reports):\n",
    "        os.makedirs(directorio_reports)\n",
    "    \n",
    "    # Generar el PDF para cada estudiante\n",
    "    for student_name, ejercicios in resultados.items():\n",
    "        if student_name != 'criterios':  # Ignorar la clave de criterios\n",
    "            file_path = os.path.join(directorio_reports, f'{student_name}.pdf')\n",
    "            try:\n",
    "                create_pdf(file_path, student_name, examen_preprocesado['contexto_examen'], examen_preprocesado['enunciados_ejercicios'], ejercicios, resultados['criterios'])\n",
    "            except IndexError as e:\n",
    "                print(f\"Error de índice al procesar {student_name}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_excel_resultados(resultados, filename='resultados_evaluacion.xlsx'):\n",
    "    filas = []\n",
    "\n",
    "    for alumno, ejercicios in resultados.items():\n",
    "        if alumno == 'criterios':  # Ignorar la clave de criterios\n",
    "            continue\n",
    "        fila = {'Alumno': alumno}\n",
    "        notas = []\n",
    "        for ejercicio, contenido in ejercicios.items():\n",
    "            media = np.mean(contenido['puntuaciones'])\n",
    "            fila[ejercicio] = media\n",
    "            notas.append(media)\n",
    "        \n",
    "        # Calcular la nota final como la media ponderada de las notas de los ejercicios\n",
    "        nota_final = np.mean(notas)\n",
    "        fila['NOTA FINAL'] = nota_final\n",
    "        \n",
    "        filas.append(fila)\n",
    "\n",
    "    df = pd.DataFrame(filas)\n",
    "    df.to_excel(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_notebook(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log):\n",
    "    \"\"\"\n",
    "    Procesa y evalúa un notebook de un alumno.\n",
    "    \n",
    "    Parameters:\n",
    "        fich (str): Nombre del archivo del notebook del alumno.\n",
    "        directorio_entregas (str): Ruta del directorio de entregas.\n",
    "        examen_procesado (dict): Datos preprocesados del examen.\n",
    "        prompt_file (str): Ruta del archivo de texto que contiene el prompt.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Nombre del alumno y resultado de la evaluación extraída.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        alumno, _ = os.path.splitext(fich)\n",
    "        nb_preprocesado = preprocesa_respuesta_alumno(os.path.join(directorio_entregas, fich))\n",
    "        res_eval_tmp = evaluar_ejercicios(examen_procesado, nb_preprocesado, dir_log, prompt_file=prompt_file)\n",
    "        res_extraido = extraer_resultados(res_eval_tmp, alumno, criterios, dir_log)\n",
    "        return alumno, res_extraido\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el notebook {fich}: {e}\", file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directorio_raiz, nombre_fich_examen):\n",
    "    \"\"\"\n",
    "    Función principal para evaluar los notebooks entregados por los estudiantes.\n",
    "    \n",
    "    Parameters:\n",
    "        directorio_raiz (str): Ruta del directorio raíz que contiene los subdirectorios y archivos necesarios.\n",
    "        nombre_fich_examen (str): Nombre del archivo del examen.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir las rutas basadas en el directorio raíz\n",
    "        directorio_entregas = os.path.join(directorio_raiz, \"entregas\")\n",
    "        dir_log = os.path.join(directorio_raiz, \"logs\")\n",
    "        prompt_file = os.path.join(directorio_raiz, 'prompt.txt')\n",
    "        directorio_examen = os.path.join(directorio_raiz, \"examenes\")\n",
    "        directorio_reports = os.path.join(directorio_raiz, \"reports\")\n",
    "        fichero_res = 'resultados_evaluacion.xlsx'\n",
    "        fichero_examen = os.path.join(directorio_examen, nombre_fich_examen)\n",
    "\n",
    "        # Preprocesar el examen\n",
    "        if not os.path.exists(fichero_examen):\n",
    "            raise FileNotFoundError(f\"El archivo de examen {fichero_examen} no existe.\")\n",
    "        examen_procesado = preprocesa_examen(fichero_examen) #Extare contexto y enunciados del examen que tienen que estar en celdas markdown\n",
    "        \n",
    "        # Listar los notebooks entregados por los alumnos\n",
    "        if not os.path.exists(directorio_entregas):\n",
    "            raise FileNotFoundError(f\"El directorio de entregas {directorio_entregas} no existe.\")\n",
    "        alumnos, ficheros = listar_notebooks(directorio_entregas)\n",
    "        if not ficheros:\n",
    "            raise FileNotFoundError(\"No se encontraron notebooks de alumnos en el directorio de entregas.\")\n",
    "\n",
    "        # Extraer los criterios de evaluación del prompt\n",
    "        if not os.path.exists(prompt_file):\n",
    "            raise FileNotFoundError(f\"El archivo de prompt {prompt_file} no existe.\")\n",
    "        criterios = extrae_criterios(prompt_file)\n",
    "\n",
    "        # Inicializar el diccionario de resultados\n",
    "        resultados = {}\n",
    "        resultados['criterios'] = criterios\n",
    "        \n",
    "        \n",
    "        # #Prosecar los notebooks y evaluarlos uno por uno\n",
    "        # for fich in tqdm(ficheros, desc=\"Procesando notebooks\"):\n",
    "        #     alumno, resultados_alumnos = evaluar_notebook(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Procesar y evaluar cada notebook en paralelo\n",
    "        resultados_alumnos = Parallel(n_jobs=-1)(\n",
    "            delayed(evaluar_notebook)(fich, directorio_entregas, examen_procesado, prompt_file, criterios, dir_log) \n",
    "            for fich in tqdm(ficheros, desc=\"Procesando notebooks\")\n",
    "        )\n",
    "\n",
    "        # Filtrar resultados exitosos y agregar al diccionario de resultados\n",
    "        for result in resultados_alumnos:\n",
    "            if result is not None:\n",
    "                alumno, res_extraido = result\n",
    "                resultados[alumno] = res_extraido\n",
    "    \n",
    "        # Generar los informes en PDF para cada estudiante\n",
    "        generar_pdfs_para_estudiantes(examen_procesado, directorio_reports, resultados)\n",
    "        \n",
    "         # Generar el archivo Excel con los resultados de la evaluación\n",
    "        generar_excel_resultados(resultados, filename=os.path.join(directorio_reports, fichero_res))\n",
    "        \n",
    "        print(\"Proceso completado con éxito.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en el proceso: {e}\", file=sys.stderr)\n",
    "        \n",
    "\n",
    "# directorio_raiz = \"/workspace\"\n",
    "# main(directorio_raiz, \"examen.ipynb\")\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando notebooks: 100%|██████████| 3/3 [00:00<00:00, 4011.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "directorio_raiz = \"/workspace\"\n",
    "main(directorio_raiz, \"examen.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________\n",
    "\n",
    "### Pruebas para comparar los nuevos ficheros de prompts para ver si siguen la estrucrura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_prompt(file_path):\n",
    "    # Leer el contenido del archivo\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    # Definimos las secciones y su estructura requerida\n",
    "    seccion_1 = r\"\\*\\*Contexto Examen\\*\\*\\n\\{contexto\\}\\n\\n\\*\\*Descripción del Ejercicio:\\*\\*\\n\\{descripcion\\}\\n\\n\\*\\*Código del estudiante:\\*\\*\\n\\{codigo\\}\"\n",
    "    seccion_2 = r\"Instrucciones Generales para la Evaluación:\\n\\nEl modelo debe evaluar cada ejercicio utilizando los criterios proporcionados.\\nCada criterio debe ser evaluado en una escala de 0 a 10, a menos que el código del ejercicio genere errores al ejecutarse.\\nSi el código genera errores, todos los criterios de ese ejercicio deben ser evaluados en una escala de 0 a 5.\\nEs importante tener en cuenta que no todos los ejercicios necesitarán usar listas o bucles; en esos casos, los criterios correspondientes deben ser omitidos de la evaluación.\"\n",
    "    criterio_patron = r\"@@\\w+@@\\nDescripción: .+\\nEjemplo: .+\"\n",
    "    seccion_3 = (\n",
    "        r\"Devuelve tres listas solo con la lista proporcionada en formato y nada más:\\n\\n\"\n",
    "        r\"A. \\*\\*Puntuaciones\\*\\*: Una lista de puntuaciones \\(solo los números, de 0 a 10\\) correspondiente a cada criterio en el orden en que se presentan. Si el código da algún error al ejecutarse, la nota máxima para cada criterio será 5.\\n   - Formato: \\[0, 10, 7, \\.\\.\\.\\]\\n\"\n",
    "        r\"B. \\*\\*Comentarios\\*\\*: Una lista de comentarios correspondiente a cada criterio en el mismo orden.\\n   - Formato: \\[\\\"Comentario para exactitud\\\", \\\"Comentario para claridad\\\", \\.\\.\\.\\]\\n\"\n",
    "        r\"C. \\*\\*Comentario General\\*\\*: Un comentario que ofrezca una idea global sobre el ejercicio teniendo en cuenta los criterios definidos. Especifica claramente si el código genera algún error al ejecutarse.\\n   - Formato: \\[\\\"Comentario general sobre el ejercicio\\\"\\]\"\n",
    "    )\n",
    "\n",
    "    # Validar secciones\n",
    "    if not re.search(seccion_1, prompt):\n",
    "        return \"La primera sección no sigue la estructura requerida.\"\n",
    "\n",
    "    if not re.search(seccion_2, prompt):\n",
    "        return \"La segunda sección no sigue la estructura requerida.\"\n",
    "\n",
    "    # Validar al menos un criterio en la segunda sección\n",
    "    criterios = re.findall(criterio_patron, prompt)\n",
    "    if not criterios:\n",
    "        return \"No se encontraron criterios en la segunda sección.\"\n",
    "\n",
    "    # Validar la tercera sección\n",
    "    if not re.search(seccion_3, prompt):\n",
    "        return \"La tercera sección no sigue la estructura requerida.\"\n",
    "\n",
    "    return \"El prompt es válido.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validar_prompt(\"/workspace/prompt funciona copy.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
